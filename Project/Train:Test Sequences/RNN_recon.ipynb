{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617b23e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33900/3533163934.py:12: DtypeWarning: Columns (2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(csv_file, sep=',', header=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"ACE2_train_data.csv\")\n",
    "\n",
    "class ACE2Dataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file, sep=',', header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {'id': self.data.iloc[idx, 0],\n",
    "                  'junction_aa': self.data.iloc[idx, 1],\n",
    "                  'consensus_count': self.data.iloc[idx, 2],\n",
    "                  'Label': self.data.iloc[idx, 3],\n",
    "                  'Distance': self.data.iloc[idx, 4]}\n",
    "\n",
    "        return sample\n",
    "\n",
    "ace2_dataset = ACE2Dataset('ACE2_train_data.csv')\n",
    "ace2_dataloader = DataLoader(ace2_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d0aa06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>consensus_count</th>\n",
       "      <th>Label</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>406886.000000</td>\n",
       "      <td>406886.000000</td>\n",
       "      <td>406886.000000</td>\n",
       "      <td>406886.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>323195.466440</td>\n",
       "      <td>1.443908</td>\n",
       "      <td>0.500027</td>\n",
       "      <td>9.017398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>189813.180032</td>\n",
       "      <td>1.423285</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>1.167593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>154232.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>325632.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>483459.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>663081.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0  consensus_count          Label       Distance\n",
       "count  406886.000000    406886.000000  406886.000000  406886.000000\n",
       "mean   323195.466440         1.443908       0.500027       9.017398\n",
       "std    189813.180032         1.423285       0.500001       1.167593\n",
       "min         0.000000         1.000000       0.000000       1.000000\n",
       "25%    154232.250000         1.000000       0.000000       8.000000\n",
       "50%    325632.500000         1.000000       1.000000       9.000000\n",
       "75%    483459.500000         2.000000       1.000000      10.000000\n",
       "max    663081.000000       310.000000       1.000000      12.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a238a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# read in the first few lines of the file\n",
    "with open('ACE2_train_data.csv', 'r') as f:\n",
    "    first_lines = [f.readline() for _ in range(10)]\n",
    "\n",
    "# use csv.Sniffer to determine the delimiter\n",
    "dialect = csv.Sniffer().sniff('\\n'.join(first_lines))\n",
    "\n",
    "print(dialect.delimiter)\n",
    "\n",
    "# use the dialect object to read in the file\n",
    "#with open('ACE2_train_data.csv', 'r') as f:\n",
    "#    reader = csv.reader(f, dialect=dialect)\n",
    "#    for row in reader:\n",
    "#        # do something with the row data\n",
    "#        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f1735b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect.delimiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba890194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KNEQFNCYGPINAYGFQRTGGEDW'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645ad150",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [K, N, A, G, F, N, C, Y, N, P, L, E, T, Y, G, ...\n",
       "1         [K, N, E, Q, F, N, C, Y, G, P, I, N, A, Y, G, ...\n",
       "2         [K, N, Q, K, F, N, C, Y, V, P, L, F, H, Y, G, ...\n",
       "3         [K, N, Q, G, F, N, C, Y, N, P, L, V, N, Y, G, ...\n",
       "4         [K, N, R, G, F, N, C, Y, K, P, L, P, G, Y, G, ...\n",
       "                                ...                        \n",
       "406881    [K, N, K, G, F, N, C, Y, I, P, I, E, D, Y, G, ...\n",
       "406882    [K, N, E, G, F, N, C, Y, N, P, I, T, E, Y, G, ...\n",
       "406883    [K, N, G, K, F, N, C, Y, H, P, I, V, R, Y, G, ...\n",
       "406884    [K, N, G, Q, F, N, C, Y, I, P, I, A, G, Y, G, ...\n",
       "406885    [K, N, R, G, F, N, C, Y, T, P, I, F, K, Y, G, ...\n",
       "Name: junction_aa, Length: 406886, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, 1].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4ed07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         KNAGFNCYNPLETYGFWRTGGVDW\n",
       "1         KNEQFNCYGPINAYGFQRTGGEDW\n",
       "2         KNQKFNCYVPLFHYGFWPTVGVGF\n",
       "3         KNQGFNCYNPLVNYGFYRTNGRSF\n",
       "4         KNRGFNCYKPLPGYGFQRTDGINW\n",
       "                    ...           \n",
       "406881    KNKGFNCYIPIEDYGFQRTSGRSY\n",
       "406882    KNEGFNCYNPITEYGFWTTSGLDW\n",
       "406883    KNGKFNCYHPIVRYGFHPTVGRGY\n",
       "406884    KNGQFNCYIPIAGYGFLPTLGVSY\n",
       "406885    KNRGFNCYTPIFKYGFFTTWGRNY\n",
       "Name: junction_aa, Length: 406886, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ffb5b63",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvec_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(sentences, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sentences), epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 491\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_vocab\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file:\n\u001b[1;32m    584\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m LineSentence(corpus_file)\n\u001b[0;32m--> 586\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scan_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m raw words and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:570\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    565\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: at sentence #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, processed \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words, keeping \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    567\u001b[0m         sentence_no, total_words, \u001b[38;5;28mlen\u001b[39m(vocab),\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[0;32m--> 570\u001b[0m     \u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    571\u001b[0m total_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vocab) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_vocab_size:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "vec_size = 10\n",
    "win_size = 2\n",
    "epochs = 100\n",
    "\n",
    "sentences = data.iloc[:, 1].apply(list)\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=vec_size, window=win_size, min_count=1)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6c59900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Word2Vec_vs10_ws2_e100.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f4dafb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAA']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = 'AAA'\n",
    "A.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f53e0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "min_count = 1\n",
    "docs = data.iloc[:, 1]\n",
    "\n",
    "documents = [TaggedDocument(words=list(data.iloc[i, 1]), tags=[str(i)]) for i in range(len(data))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2747b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=10, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b4c950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('doc2vec_vs10_ws2_mc1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0a7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "494fbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "counts = Counter(chain(*data['junction_aa'].apply(list)))\n",
    "vocabulary = set([k for k, v in counts.items()])\n",
    "aa_to_id = {'<pad>':0, **{k:i for k, i in zip(vocabulary, range(1, len(vocabulary)))}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612a1688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'L': 1, 'F': 2, 'S': 3, 'W': 4, 'I': 5, 'T': 6, 'A': 7, 'C': 8, 'D': 9, 'V': 10, 'R': 11, 'Y': 12, 'G': 13, 'P': 14, 'E': 15, 'K': 16, 'Q': 17, 'M': 18, 'N': 19}\n",
      "{'L', 'F', 'S', 'W', 'I', 'T', 'A', 'C', 'D', 'V', 'R', 'Y', 'G', 'P', 'E', 'K', 'Q', 'M', 'N', 'H'}\n",
      "Counter({'G': 1495581, 'Y': 1013927, 'F': 1009186, 'N': 973928, 'T': 624723, 'K': 616561, 'P': 587453, 'R': 527679, 'C': 461412, 'W': 357738, 'L': 341333, 'Q': 304621, 'I': 286410, 'S': 278442, 'D': 200912, 'V': 199320, 'A': 179698, 'E': 162208, 'H': 90227, 'M': 53905})\n"
     ]
    }
   ],
   "source": [
    "print(aa_to_id)\n",
    "print(vocabulary)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af4bbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a2c56908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class AminoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_X, dataset_y, aa_to_id, DEVICE):\n",
    "        self.dataset_X = dataset_X.reset_index(drop=True)\n",
    "        self.dataset_y = dataset_y.reset_index(drop=True)\n",
    "        self.aa_to_id = aa_to_id\n",
    "        self.length = dataset_y.shape[0]\n",
    "        self.device = DEVICE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = list(self.dataset_X.iloc[idx])\n",
    "        ids = torch.LongTensor([self.aa_to_id[token] for token in tokens if token in self.aa_to_id])\n",
    "        y = self.dataset_y[idx]\n",
    "        \n",
    "        return ids, y\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        ids, y = list(zip(*batch))\n",
    "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
    "        y = torch.LongTensor(y)\n",
    "        \n",
    "        \n",
    "        return padded_ids, y\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9103c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class OneHotAminoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_X, dataset_y, aa_to_id, DEVICE):\n",
    "        self.dataset_X = dataset_X.reset_index(drop=True)\n",
    "        self.dataset_y = dataset_y.reset_index(drop=True)\n",
    "        self.aa_to_id = aa_to_id\n",
    "        self.length = dataset_y.shape[0]\n",
    "        self.device = DEVICE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = list(self.dataset_X.iloc[idx])\n",
    "        ids = torch.LongTensor([self.aa_to_id[token] for token in tokens if token in self.aa_to_id])\n",
    "        ids = F.one_hot(ids, num_classes=20)\n",
    "        y = self.dataset_y[idx]\n",
    "        \n",
    "        return ids, y\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        ids, y = list(zip(*batch))\n",
    "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
    "        y = torch.LongTensor(y)\n",
    "        \n",
    "        \n",
    "        return padded_ids, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "85356c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.info of 344999    0\n",
       "138794    1\n",
       "245214    0\n",
       "228730    0\n",
       "277360    1\n",
       "         ..\n",
       "155350    1\n",
       "126599    1\n",
       "229625    0\n",
       "84155     0\n",
       "388718    1\n",
       "Name: Label, Length: 366197, dtype: int64>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data['junction_aa'], data['Label'], test_size=0.1)\n",
    "\n",
    "train_dataset = OneHotAminoDataset(train_X, train_y, aa_to_id, device)\n",
    "test_dataset = OneHotAminoDataset(test_X, test_y, aa_to_id, device)\n",
    "\n",
    "train_y.info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "44a7f6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'N', 'E', 'K', 'F', 'N', 'C', 'Y', 'Q', 'P', 'L', 'V', 'T', 'Y', 'G', 'F', 'W', 'R', 'T', 'R', 'G', 'K', 'S', 'F']\n",
      "tensor([16, 19, 15, 16,  2, 19,  8, 12, 17, 14,  1, 10,  6, 12, 13,  2,  4, 11,\n",
      "         6, 11, 13, 16,  3,  2])\n"
     ]
    }
   ],
   "source": [
    "tokens = list(train_X.reset_index(drop=True).iloc[1])\n",
    "print(tokens)\n",
    "ids = torch.LongTensor([aa_to_id[token] for token in tokens if token in aa_to_id])\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c2e891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "test_sampler = SequentialSampler(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3dd301f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a502857",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=64, sampler=train_sampler, collate_fn=train_dataset.collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=64, sampler=test_sampler, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac04f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[16, 19,  7,  ..., 12,  0,  0],\n",
      "        [16, 19, 11,  ..., 10,  9,  2],\n",
      "        [16, 19, 15,  ..., 16,  9,  4],\n",
      "        ...,\n",
      "        [16, 19,  7,  ..., 13,  2,  0],\n",
      "        [16, 19, 16,  ..., 13, 13, 12],\n",
      "        [16, 19, 15,  ..., 18,  3,  4]]), tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]))\n",
      "2\n",
      "torch.Size([64, 24])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "print(batch)\n",
    "print(len(batch))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b5165b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "868ea9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_with_learned_embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding2hidden = nn.Linear(embedding_dim, 10)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.hidden2out = nn.Linear(10, 1)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        mean_emb = torch.mean(embedded, dim=1)\n",
    "        hidden = self.embedding2hidden(mean_emb)\n",
    "        hidden = self.act1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.hidden2out(hidden)\n",
    "        proba = self.act2(out)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c0af884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Linear_with_learned_embedding(vocab_size=len(vocabulary)+1, embedding_dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "aefaa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    print('Training...')\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, (texts, ys) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        preds_proba = model(texts).flatten()\n",
    "        loss = criterion(preds_proba, ys.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if not (i + 1) % 20:\n",
    "            print(f'Train loss: {epoch_loss/i}')\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7bb5a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, iterator, criterion):\n",
    "    print(\"\\nValidating...\")\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (texts, ys) in enumerate(iterator):   \n",
    "            predictions = model(texts).flatten()\n",
    "            loss = criterion(predictions, ys.float())\n",
    "            epoch_loss += loss.item() \n",
    "            if not (i + 1) % 5:\n",
    "              print(f'Val loss: {epoch_loss/i}')\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "870716c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "53d099f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_1.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6686eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model_1.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cdb44a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting Epoch 0\n",
      "Training...\n",
      "Train loss: 0.7277909329063014\n",
      "Train loss: 0.7042798965405195\n",
      "Train loss: 0.690976354025178\n",
      "Train loss: 0.6796815342540983\n",
      "Train loss: 0.6715721746887824\n",
      "Train loss: 0.6612438509444228\n",
      "Train loss: 0.6544976551755727\n",
      "Train loss: 0.6502682836550586\n",
      "Train loss: 0.6453015065059982\n",
      "Train loss: 0.642173263295811\n",
      "Train loss: 0.6366789588645169\n",
      "Train loss: 0.6339695326453971\n",
      "Train loss: 0.6314379666072522\n",
      "Train loss: 0.6295467481604614\n",
      "Train loss: 0.6271395492912535\n",
      "Train loss: 0.6252908822511057\n",
      "Train loss: 0.6236032834798537\n",
      "Train loss: 0.6218304062952238\n",
      "Train loss: 0.6208911332102753\n",
      "Train loss: 0.619439918370474\n",
      "Train loss: 0.6176747871753993\n",
      "Train loss: 0.616727453429498\n",
      "Train loss: 0.6155445743230433\n",
      "Train loss: 0.614575404090523\n",
      "Train loss: 0.6137703604593067\n",
      "Train loss: 0.612780506096364\n",
      "Train loss: 0.6119261743407524\n",
      "Train loss: 0.6111279799068123\n",
      "Train loss: 0.6099908103288146\n",
      "Train loss: 0.6099433622794079\n",
      "Train loss: 0.6092876605629344\n",
      "Train loss: 0.6084358132034773\n",
      "Train loss: 0.6085252940473861\n",
      "Train loss: 0.6081578585817116\n",
      "Train loss: 0.607849364330158\n",
      "Train loss: 0.6068544049173469\n",
      "Train loss: 0.6064039617575231\n",
      "Train loss: 0.6062421667167478\n",
      "Train loss: 0.6060584774754932\n",
      "Train loss: 0.6052616717146991\n",
      "Train loss: 0.6051193613636333\n",
      "Train loss: 0.6046165410824413\n",
      "Train loss: 0.604329927871058\n",
      "Train loss: 0.6043046382642578\n",
      "Train loss: 0.603555024830465\n",
      "Train loss: 0.6029702957812798\n",
      "Train loss: 0.6028353828655137\n",
      "Train loss: 0.6021225577350453\n",
      "Train loss: 0.6012380166365494\n",
      "Train loss: 0.6008753064755086\n",
      "Train loss: 0.6005507626290176\n",
      "Train loss: 0.6001256851190782\n",
      "Train loss: 0.5997676340840243\n",
      "Train loss: 0.5996811145483287\n",
      "Train loss: 0.5992629793798847\n",
      "Train loss: 0.5986457663364342\n",
      "Train loss: 0.5981564885487778\n",
      "Train loss: 0.5976668331158171\n",
      "Train loss: 0.5980480473814018\n",
      "Train loss: 0.5976349829235507\n",
      "Train loss: 0.5975678095668921\n",
      "Train loss: 0.5974223641160037\n",
      "Train loss: 0.597165950145676\n",
      "Train loss: 0.5971924877893747\n",
      "Train loss: 0.5969088717182386\n",
      "Train loss: 0.5965901564882957\n",
      "Train loss: 0.5964486775245125\n",
      "Train loss: 0.596138863966165\n",
      "Train loss: 0.5959304823142364\n",
      "Train loss: 0.5961971118929047\n",
      "Train loss: 0.5959495883579402\n",
      "Train loss: 0.5954723577270746\n",
      "Train loss: 0.5953052804394893\n",
      "Train loss: 0.5950282077236382\n",
      "Train loss: 0.5951307186728879\n",
      "Train loss: 0.5949653884429065\n",
      "Train loss: 0.5947763286216294\n",
      "Train loss: 0.5946401158841166\n",
      "Train loss: 0.5944321606263395\n",
      "Train loss: 0.5942681582738937\n",
      "Train loss: 0.5938925818943404\n",
      "Train loss: 0.5937186421745794\n",
      "Train loss: 0.5937995680345692\n",
      "Train loss: 0.5939308617923003\n",
      "Train loss: 0.5937136431488309\n",
      "Train loss: 0.5934702869485186\n",
      "Train loss: 0.5933099214272501\n",
      "Train loss: 0.5933532723653987\n",
      "Train loss: 0.5931070401066806\n",
      "Train loss: 0.5929115994431166\n",
      "Train loss: 0.5928325659957412\n",
      "Train loss: 0.5927161462058056\n",
      "Train loss: 0.5927075967633513\n",
      "Train loss: 0.5926350150703432\n",
      "Train loss: 0.5927490660898179\n",
      "Train loss: 0.5926409212327861\n",
      "Train loss: 0.5927196998550697\n",
      "Train loss: 0.5928354359604133\n",
      "Train loss: 0.592793110707241\n",
      "Train loss: 0.5926841017065673\n",
      "Train loss: 0.5924605385184938\n",
      "Train loss: 0.5924548321696342\n",
      "Train loss: 0.5923339824123022\n",
      "Train loss: 0.5924862805841747\n",
      "Train loss: 0.5924165762850874\n",
      "Train loss: 0.59236407327787\n",
      "Train loss: 0.5923492457332763\n",
      "Train loss: 0.5922995726782837\n",
      "Train loss: 0.5920344944282538\n",
      "Train loss: 0.5918895027864083\n",
      "Train loss: 0.5918923689739938\n",
      "Train loss: 0.5918468127510493\n",
      "Train loss: 0.5916801853985214\n",
      "Train loss: 0.5916397453740586\n",
      "Train loss: 0.5916627216411705\n",
      "Train loss: 0.5915308112006292\n",
      "Train loss: 0.5914848599234113\n",
      "Train loss: 0.5914092197818441\n",
      "Train loss: 0.5915256192474718\n",
      "Train loss: 0.5914562030502437\n",
      "Train loss: 0.5914751719275305\n",
      "Train loss: 0.5912993996052431\n",
      "Train loss: 0.5910712449276239\n",
      "Train loss: 0.5910552767747446\n",
      "Train loss: 0.5910099307123591\n",
      "Train loss: 0.5909218837127368\n",
      "Train loss: 0.5908496458965194\n",
      "Train loss: 0.5908073077168302\n",
      "Train loss: 0.5906674605023264\n",
      "Train loss: 0.5904606465263154\n",
      "Train loss: 0.5903604299046239\n",
      "Train loss: 0.590443984978395\n",
      "Train loss: 0.5904904874857766\n",
      "Train loss: 0.5904602254113407\n",
      "Train loss: 0.5903043071509909\n",
      "Train loss: 0.5902149140418299\n",
      "Train loss: 0.5900071801039023\n",
      "Train loss: 0.5901296432573927\n",
      "Train loss: 0.5900129072097361\n",
      "Train loss: 0.5899256373908359\n",
      "Train loss: 0.5898628663872945\n",
      "Train loss: 0.589757813455018\n",
      "Train loss: 0.5898388438643422\n",
      "Train loss: 0.5898977062196191\n",
      "Train loss: 0.5898731190116293\n",
      "Train loss: 0.5898875653559968\n",
      "Train loss: 0.5899125652051207\n",
      "Train loss: 0.5898261443309262\n",
      "Train loss: 0.5897919606666271\n",
      "Train loss: 0.5898968031224349\n",
      "Train loss: 0.5899051807107575\n",
      "Train loss: 0.5899232621057993\n",
      "Train loss: 0.5898785324862831\n",
      "Train loss: 0.5897393198553793\n",
      "Train loss: 0.5896395119535188\n",
      "Train loss: 0.5897237288240975\n",
      "Train loss: 0.5895998156366928\n",
      "Train loss: 0.5892926842474265\n",
      "Train loss: 0.5891157911516803\n",
      "Train loss: 0.5888753059637774\n",
      "Train loss: 0.5888864916557479\n",
      "Train loss: 0.5889075045767446\n",
      "Train loss: 0.5888441423855518\n",
      "Train loss: 0.588683242067472\n",
      "Train loss: 0.5887294980875468\n",
      "Train loss: 0.5887235523114257\n",
      "Train loss: 0.5885350973208389\n",
      "Train loss: 0.5884876941314372\n",
      "Train loss: 0.5883707820610663\n",
      "Train loss: 0.58835478102014\n",
      "Train loss: 0.5882071329073169\n",
      "Train loss: 0.5881114646805411\n",
      "Train loss: 0.5880403462759709\n",
      "Train loss: 0.5880886199368798\n",
      "Train loss: 0.5879917626007518\n",
      "Train loss: 0.5880970051086026\n",
      "Train loss: 0.5880038063719503\n",
      "Train loss: 0.5879501036862103\n",
      "Train loss: 0.5879861067446559\n",
      "Train loss: 0.5879736216606846\n",
      "Train loss: 0.5878979507880041\n",
      "Train loss: 0.5879065930499385\n",
      "Train loss: 0.5878937869457704\n",
      "Train loss: 0.5877647321649984\n",
      "Train loss: 0.587689135268947\n",
      "Train loss: 0.587617331204731\n",
      "Train loss: 0.5876031069313661\n",
      "Train loss: 0.5876201285219281\n",
      "Train loss: 0.5876053037088869\n",
      "Train loss: 0.587485590791288\n",
      "Train loss: 0.5875661455857626\n",
      "Train loss: 0.5875608859412463\n",
      "Train loss: 0.5875253244427456\n",
      "Train loss: 0.5875274137788041\n",
      "Train loss: 0.5875061995923809\n",
      "Train loss: 0.5874670378010198\n",
      "Train loss: 0.5874621512835992\n",
      "Train loss: 0.587398456063768\n",
      "Train loss: 0.5873748085986552\n",
      "Train loss: 0.5873171977771464\n",
      "Train loss: 0.5872742753771591\n",
      "Train loss: 0.5873136961038056\n",
      "Train loss: 0.5872940439533558\n",
      "Train loss: 0.5873682205876932\n",
      "Train loss: 0.5874061929754991\n",
      "Train loss: 0.58740398789905\n",
      "Train loss: 0.5872255978333141\n",
      "Train loss: 0.587223740947106\n",
      "Train loss: 0.5872016484428176\n",
      "Train loss: 0.5871005555892962\n",
      "Train loss: 0.5870645173171256\n",
      "Train loss: 0.5871312639417443\n",
      "Train loss: 0.5871145037746563\n",
      "Train loss: 0.5870700082466351\n",
      "Train loss: 0.5870252036513159\n",
      "Train loss: 0.5870043414766298\n",
      "Train loss: 0.5870942991500792\n",
      "Train loss: 0.5870987155905665\n",
      "Train loss: 0.5870057979275795\n",
      "Train loss: 0.5870118498747987\n",
      "Train loss: 0.5870619110457901\n",
      "Train loss: 0.5870604921928745\n",
      "Train loss: 0.58699627009762\n",
      "Train loss: 0.5869974931666669\n",
      "Train loss: 0.5868575648682254\n",
      "Train loss: 0.5867825259899085\n",
      "Train loss: 0.5867988921506575\n",
      "Train loss: 0.5868047104707593\n",
      "Train loss: 0.5868675849549139\n",
      "Train loss: 0.5867574896844062\n",
      "Train loss: 0.5868348471851002\n",
      "Train loss: 0.5868065537409834\n",
      "Train loss: 0.5867488733699685\n",
      "Train loss: 0.5866293946273512\n",
      "Train loss: 0.5867207219788916\n",
      "Train loss: 0.5867066167122885\n",
      "Train loss: 0.5867484743319928\n",
      "Train loss: 0.5867217849673131\n",
      "Train loss: 0.5865757838281335\n",
      "Train loss: 0.5865575108063125\n",
      "Train loss: 0.5865590151265637\n",
      "Train loss: 0.5865204061178857\n",
      "Train loss: 0.5864098766601289\n",
      "Train loss: 0.5863391834220135\n",
      "Train loss: 0.5862974953799862\n",
      "Train loss: 0.586259138404802\n",
      "Train loss: 0.5862646440089678\n",
      "Train loss: 0.5862072898233868\n",
      "Train loss: 0.5861304118154901\n",
      "Train loss: 0.5860929946573669\n",
      "Train loss: 0.5860517530962395\n",
      "Train loss: 0.5860670672673794\n",
      "Train loss: 0.5860713392077852\n",
      "Train loss: 0.586032252279786\n",
      "Train loss: 0.5859989794499034\n",
      "Train loss: 0.58595414871605\n",
      "Train loss: 0.5859334446862412\n",
      "Train loss: 0.5858855476372925\n",
      "Train loss: 0.5858005855529551\n",
      "Train loss: 0.5857172490519087\n",
      "Train loss: 0.5856596752130384\n",
      "Train loss: 0.5857054651932626\n",
      "Train loss: 0.5856925619546169\n",
      "Train loss: 0.5856420661796108\n",
      "Train loss: 0.585540501205038\n",
      "Train loss: 0.5854897798620505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5854772463800935\n",
      "Train loss: 0.585429216686926\n",
      "Train loss: 0.5853972506857779\n",
      "Train loss: 0.5853769347378006\n",
      "Train loss: 0.5853959665920576\n",
      "Train loss: 0.5853082797811803\n",
      "Train loss: 0.5852602970951707\n",
      "Train loss: 0.5852797976058425\n",
      "Train loss: 0.5852399993038195\n",
      "Train loss: 0.5852613563359615\n",
      "Train loss: 0.5852037273356794\n",
      "Train loss: 0.5851665496890505\n",
      "Train loss: 0.5851621415200979\n",
      "Train loss: 0.5851626510086986\n",
      "Train loss: 0.5850967670870243\n",
      "Train loss: 0.5850949149897894\n",
      "Train loss: 0.5850682049847509\n",
      "Train loss: 0.585013373350673\n",
      "Train loss: 0.5850683810272056\n",
      "Train loss: 0.5850043403537442\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.7044832706451416\n",
      "Val loss: 0.6270672016673617\n",
      "Val loss: 0.5990235784224102\n",
      "Val loss: 0.5983989379907909\n",
      "Val loss: 0.5990438821415106\n",
      "Val loss: 0.6020898664819783\n",
      "Val loss: 0.5942095062311958\n",
      "Val loss: 0.5884008132494413\n",
      "Val loss: 0.5827968445691195\n",
      "Val loss: 0.58265948052309\n",
      "Val loss: 0.5833997307000337\n",
      "Val loss: 0.5820879511914011\n",
      "Val loss: 0.5818581078201532\n",
      "Val loss: 0.5810868230418883\n",
      "Val loss: 0.5805799107293825\n",
      "Val loss: 0.5787750071362604\n",
      "Val loss: 0.5772555992007256\n",
      "Val loss: 0.5763885305168923\n",
      "Val loss: 0.5781753069542824\n",
      "Val loss: 0.5753072126947268\n",
      "Val loss: 0.5762157657971749\n",
      "Val loss: 0.5748019729732373\n",
      "Val loss: 0.5736421741414488\n",
      "Val loss: 0.5730299233388501\n",
      "Val loss: 0.5728928968310356\n",
      "Val loss: 0.5716222660948139\n",
      "Val loss: 0.5713930743843761\n",
      "Val loss: 0.5698878672911967\n",
      "Val loss: 0.5692347728957733\n",
      "Val loss: 0.5695650103508226\n",
      "Val loss: 0.5693484209961706\n",
      "Val loss: 0.569254440893917\n",
      "Val loss: 0.5688042364469389\n",
      "Val loss: 0.5682250885568427\n",
      "Val loss: 0.5681746718527256\n",
      "Val loss: 0.5680625388742159\n",
      "Val loss: 0.5661626762670019\n",
      "Val loss: 0.5662136869455772\n",
      "Val loss: 0.5670039159120973\n",
      "Val loss: 0.5660090438984148\n",
      "Val loss: 0.5663349872418478\n",
      "Val loss: 0.565686103687332\n",
      "Val loss: 0.5649842553049604\n",
      "Val loss: 0.5651193099206986\n",
      "Val loss: 0.5642748145120484\n",
      "Val loss: 0.5644871751293865\n",
      "Val loss: 0.5641374106590564\n",
      "Val loss: 0.5643934758387849\n",
      "Val loss: 0.5646196675838017\n",
      "Val loss: 0.5648190207988861\n",
      "Val loss: 0.5643194399950072\n",
      "Val loss: 0.5636053015136351\n",
      "Val loss: 0.5634462960967512\n",
      "Val loss: 0.5629570147361898\n",
      "Val loss: 0.5629435133759992\n",
      "Val loss: 0.5621377759509616\n",
      "Val loss: 0.5624220241123522\n",
      "Val loss: 0.5624591188447285\n",
      "Val loss: 0.5625510347538254\n",
      "Val loss: 0.5624389313535149\n",
      "Val loss: 0.5627824581767383\n",
      "Val loss: 0.5627838467702897\n",
      "Val loss: 0.5622686413443012\n",
      "Val loss: 0.5619964577187565\n",
      "Val loss: 0.561894964840677\n",
      "Val loss: 0.5620007976934902\n",
      "Val loss: 0.5618344374045641\n",
      "Val loss: 0.5613693467459496\n",
      "Val loss: 0.5624230541288853\n",
      "Val loss: 0.562239537693368\n",
      "Val loss: 0.5614947416519714\n",
      "Val loss: 0.5614782942868873\n",
      "Val loss: 0.5611238726875284\n",
      "Val loss: 0.5613415296161725\n",
      "Val loss: 0.5613800983696697\n",
      "Val loss: 0.5610151335714989\n",
      "Val loss: 0.5612815076019615\n",
      "Val loss: 0.5611414677226452\n",
      "Val loss: 0.5606147272483951\n",
      "Val loss: 0.5609319399024609\n",
      "Val loss: 0.56126155069854\n",
      "Val loss: 0.5606890755061124\n",
      "Val loss: 0.560929524725762\n",
      "Val loss: 0.5604860742080752\n",
      "Val loss: 0.5601853513914459\n",
      "Val loss: 0.5603578242805455\n",
      "Val loss: 0.5600952234136344\n",
      "Val loss: 0.5601388916610857\n",
      "Val loss: 0.5596069539855192\n",
      "Val loss: 0.5593583828755104\n",
      "Val loss: 0.5595888693033336\n",
      "Val loss: 0.5595975202023333\n",
      "Val loss: 0.5597242201710569\n",
      "Val loss: 0.5597443660693382\n",
      "Val loss: 0.5595235464945121\n",
      "Val loss: 0.5598230680493572\n",
      "Val loss: 0.55997659900218\n",
      "Val loss: 0.5599897070530734\n",
      "Val loss: 0.5599880263149014\n",
      "Val loss: 0.5604621708273648\n",
      "Val loss: 0.5603461788287238\n",
      "Val loss: 0.56015177150138\n",
      "Val loss: 0.560581777230311\n",
      "Val loss: 0.5603443418049858\n",
      "Val loss: 0.560267700778164\n",
      "Val loss: 0.5600856945397498\n",
      "Val loss: 0.5597449264276341\n",
      "Val loss: 0.5597082975844063\n",
      "Val loss: 0.5594284255605411\n",
      "Val loss: 0.5596349001582203\n",
      "Val loss: 0.5597151338193391\n",
      "Val loss: 0.5598104504438547\n",
      "Val loss: 0.5598110687648151\n",
      "Val loss: 0.5600640903667322\n",
      "Val loss: 0.5598746508048387\n",
      "Val loss: 0.5600035840159665\n",
      "Val loss: 0.5601245264892709\n",
      "Val loss: 0.5601416559089829\n",
      "Val loss: 0.5602503274426316\n",
      "Val loss: 0.5605486333370209\n",
      "Val loss: 0.5608112327014374\n",
      "Val loss: 0.5611434622449045\n",
      "Val loss: 0.5612692744607646\n",
      "Val loss: 0.5613569768449601\n",
      "Val loss: 0.5615050549117419\n",
      "Val loss: 0.5616807908246172\n",
      "Val loss: 0.5616138353238722\n",
      "\n",
      "starting Epoch 1\n",
      "Training...\n",
      "Train loss: 0.6158935741374367\n",
      "Train loss: 0.5932537714640299\n",
      "Train loss: 0.5829959097555129\n",
      "Train loss: 0.5755567686467231\n",
      "Train loss: 0.5751618469002271\n",
      "Train loss: 0.5768912091475575\n",
      "Train loss: 0.5782276355534148\n",
      "Train loss: 0.5799205387538334\n",
      "Train loss: 0.5778494578833021\n",
      "Train loss: 0.5799730613303544\n",
      "Train loss: 0.5817144865586877\n",
      "Train loss: 0.5822936075751253\n",
      "Train loss: 0.5828329682580292\n",
      "Train loss: 0.5828749196717389\n",
      "Train loss: 0.5824491953770053\n",
      "Train loss: 0.5832501683489282\n",
      "Train loss: 0.5814671951585111\n",
      "Train loss: 0.5826010786225204\n",
      "Train loss: 0.5835316678150348\n",
      "Train loss: 0.5836880139838484\n",
      "Train loss: 0.582754010090111\n",
      "Train loss: 0.582278085868288\n",
      "Train loss: 0.5824553555522869\n",
      "Train loss: 0.5829552744898269\n",
      "Train loss: 0.5830219637654827\n",
      "Train loss: 0.5830275211375573\n",
      "Train loss: 0.5833279399239287\n",
      "Train loss: 0.5828820174836515\n",
      "Train loss: 0.5831267425021566\n",
      "Train loss: 0.5830808521710175\n",
      "Train loss: 0.5820850055899489\n",
      "Train loss: 0.581214747984831\n",
      "Train loss: 0.5804658273463907\n",
      "Train loss: 0.5811920056286898\n",
      "Train loss: 0.581490906523021\n",
      "Train loss: 0.5810571360156997\n",
      "Train loss: 0.581391728134052\n",
      "Train loss: 0.5813336379835885\n",
      "Train loss: 0.5816363987246282\n",
      "Train loss: 0.5821996240204058\n",
      "Train loss: 0.5827044276964097\n",
      "Train loss: 0.5825310452831232\n",
      "Train loss: 0.5828937174485643\n",
      "Train loss: 0.5831051992000302\n",
      "Train loss: 0.5827548163088331\n",
      "Train loss: 0.5828731672289063\n",
      "Train loss: 0.5826461372601466\n",
      "Train loss: 0.5827417348485296\n",
      "Train loss: 0.5829596704859534\n",
      "Train loss: 0.5833752178274714\n",
      "Train loss: 0.5837511676742939\n",
      "Train loss: 0.5837068487464292\n",
      "Train loss: 0.5838432696430721\n",
      "Train loss: 0.5837663599912274\n",
      "Train loss: 0.5832994296889179\n",
      "Train loss: 0.5832101070156472\n",
      "Train loss: 0.5831610053594537\n",
      "Train loss: 0.5829714961973524\n",
      "Train loss: 0.582673551724865\n",
      "Train loss: 0.5823272524325424\n",
      "Train loss: 0.5820751521702767\n",
      "Train loss: 0.5815667126378867\n",
      "Train loss: 0.5814418429841685\n",
      "Train loss: 0.5813251105810722\n",
      "Train loss: 0.5811264321747149\n",
      "Train loss: 0.5810705664213902\n",
      "Train loss: 0.5812337381613619\n",
      "Train loss: 0.581524388109436\n",
      "Train loss: 0.5818270762559726\n",
      "Train loss: 0.5816678338770017\n",
      "Train loss: 0.5818620816009661\n",
      "Train loss: 0.5817592131586188\n",
      "Train loss: 0.58214650687655\n",
      "Train loss: 0.5821192103757336\n",
      "Train loss: 0.582009778013223\n",
      "Train loss: 0.5820714144662777\n",
      "Train loss: 0.5824081536389698\n",
      "Train loss: 0.5823416766484171\n",
      "Train loss: 0.5822493291263569\n",
      "Train loss: 0.5818581488670745\n",
      "Train loss: 0.5817420026304694\n",
      "Train loss: 0.5817175095695195\n",
      "Train loss: 0.5816534486357313\n",
      "Train loss: 0.5819884884066352\n",
      "Train loss: 0.5818762856942896\n",
      "Train loss: 0.5819108314712496\n",
      "Train loss: 0.5818477437679901\n",
      "Train loss: 0.5819059548525731\n",
      "Train loss: 0.5819941484874523\n",
      "Train loss: 0.5821922398097255\n",
      "Train loss: 0.5820249525022219\n",
      "Train loss: 0.5822867795687255\n",
      "Train loss: 0.5821458789820053\n",
      "Train loss: 0.5821755825838807\n",
      "Train loss: 0.5823257346885213\n",
      "Train loss: 0.5820868850965684\n",
      "Train loss: 0.5820211706670552\n",
      "Train loss: 0.5818914270723518\n",
      "Train loss: 0.5819141150815973\n",
      "Train loss: 0.5820446546790956\n",
      "Train loss: 0.5820248440653455\n",
      "Train loss: 0.5821203187933852\n",
      "Train loss: 0.58203603442344\n",
      "Train loss: 0.5820342298451956\n",
      "Train loss: 0.5818757077607158\n",
      "Train loss: 0.5818176745327648\n",
      "Train loss: 0.5816969314486471\n",
      "Train loss: 0.5815936527687292\n",
      "Train loss: 0.5814850250478729\n",
      "Train loss: 0.5815103678553687\n",
      "Train loss: 0.5814663649357455\n",
      "Train loss: 0.5815183311607647\n",
      "Train loss: 0.5814050025858675\n",
      "Train loss: 0.5813106899593106\n",
      "Train loss: 0.5810042441461645\n",
      "Train loss: 0.5810779098834186\n",
      "Train loss: 0.581034053156854\n",
      "Train loss: 0.5808804612662035\n",
      "Train loss: 0.5809307906020335\n",
      "Train loss: 0.5809632800007025\n",
      "Train loss: 0.5807617756392947\n",
      "Train loss: 0.5808374811118134\n",
      "Train loss: 0.5807275687378081\n",
      "Train loss: 0.5809244655730889\n",
      "Train loss: 0.5810429283431551\n",
      "Train loss: 0.580972796396965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5808581105364078\n",
      "Train loss: 0.5808557837724033\n",
      "Train loss: 0.5809710899403318\n",
      "Train loss: 0.5808664500805083\n",
      "Train loss: 0.5810784808139211\n",
      "Train loss: 0.5812137399084778\n",
      "Train loss: 0.5813021633399972\n",
      "Train loss: 0.581316935455412\n",
      "Train loss: 0.5811687592070559\n",
      "Train loss: 0.5812314043438754\n",
      "Train loss: 0.5812292629108241\n",
      "Train loss: 0.5813589325608789\n",
      "Train loss: 0.5814683144471765\n",
      "Train loss: 0.5814264068095844\n",
      "Train loss: 0.5812841504019345\n",
      "Train loss: 0.5812117579055529\n",
      "Train loss: 0.5811786032527758\n",
      "Train loss: 0.5811773547181325\n",
      "Train loss: 0.5809778981617543\n",
      "Train loss: 0.5811517868457734\n",
      "Train loss: 0.5811171305650106\n",
      "Train loss: 0.5810998335364865\n",
      "Train loss: 0.5809985729982966\n",
      "Train loss: 0.581079137428636\n",
      "Train loss: 0.581173631085904\n",
      "Train loss: 0.5813219146769311\n",
      "Train loss: 0.581389441974181\n",
      "Train loss: 0.5814094632505867\n",
      "Train loss: 0.5815030544294238\n",
      "Train loss: 0.5814894388485659\n",
      "Train loss: 0.5814465033415588\n",
      "Train loss: 0.5814119971470201\n",
      "Train loss: 0.581369938359916\n",
      "Train loss: 0.5814791997547036\n",
      "Train loss: 0.5815200293682569\n",
      "Train loss: 0.5815049095005119\n",
      "Train loss: 0.581516620022053\n",
      "Train loss: 0.5814351356676636\n",
      "Train loss: 0.581323206108532\n",
      "Train loss: 0.5812942279987071\n",
      "Train loss: 0.5812654601452986\n",
      "Train loss: 0.5812724792698233\n",
      "Train loss: 0.5812651587153511\n",
      "Train loss: 0.5812996221346236\n",
      "Train loss: 0.5814685873135937\n",
      "Train loss: 0.5814993523795717\n",
      "Train loss: 0.5815258576849975\n",
      "Train loss: 0.5815120581564255\n",
      "Train loss: 0.5815087562545636\n",
      "Train loss: 0.581409346764316\n",
      "Train loss: 0.5814217854165936\n",
      "Train loss: 0.5812484059701578\n",
      "Train loss: 0.5813362244771625\n",
      "Train loss: 0.5813052911300002\n",
      "Train loss: 0.5812917053123046\n",
      "Train loss: 0.5811608155273611\n",
      "Train loss: 0.5810882200729525\n",
      "Train loss: 0.5810748708656411\n",
      "Train loss: 0.5811122309423582\n",
      "Train loss: 0.5811721226258059\n",
      "Train loss: 0.5810594000769159\n",
      "Train loss: 0.5810859408759792\n",
      "Train loss: 0.5810001174288157\n",
      "Train loss: 0.5809894064424163\n",
      "Train loss: 0.5809802853505504\n",
      "Train loss: 0.5809306288780909\n",
      "Train loss: 0.5809566980277544\n",
      "Train loss: 0.580949082705556\n",
      "Train loss: 0.5809337022167562\n",
      "Train loss: 0.5809190110775183\n",
      "Train loss: 0.5809132363037217\n",
      "Train loss: 0.5809103202250727\n",
      "Train loss: 0.5808533024970657\n",
      "Train loss: 0.5808553530532559\n",
      "Train loss: 0.5809777188461258\n",
      "Train loss: 0.580971577418975\n",
      "Train loss: 0.5808187122047168\n",
      "Train loss: 0.580856303895154\n",
      "Train loss: 0.5808634829974867\n",
      "Train loss: 0.5808013790034067\n",
      "Train loss: 0.5808330640020392\n",
      "Train loss: 0.5807019528946529\n",
      "Train loss: 0.580791569827247\n",
      "Train loss: 0.5808488749824668\n",
      "Train loss: 0.5809201070640176\n",
      "Train loss: 0.58092018706699\n",
      "Train loss: 0.580925389337103\n",
      "Train loss: 0.5808509443652605\n",
      "Train loss: 0.5808418207971072\n",
      "Train loss: 0.580795316549605\n",
      "Train loss: 0.5808282856719783\n",
      "Train loss: 0.5807782615658668\n",
      "Train loss: 0.5808156843965635\n",
      "Train loss: 0.5807039594211262\n",
      "Train loss: 0.5806905539981594\n",
      "Train loss: 0.5807079398355444\n",
      "Train loss: 0.5807094754967408\n",
      "Train loss: 0.5806026830221293\n",
      "Train loss: 0.5805307146866445\n",
      "Train loss: 0.5805806911843729\n",
      "Train loss: 0.5805754008325194\n",
      "Train loss: 0.5806031675356003\n",
      "Train loss: 0.5806787781422251\n",
      "Train loss: 0.5806569578427806\n",
      "Train loss: 0.5806433127591765\n",
      "Train loss: 0.5806487810254534\n",
      "Train loss: 0.5806492211788502\n",
      "Train loss: 0.580678310996066\n",
      "Train loss: 0.5806078641915428\n",
      "Train loss: 0.5805651916869492\n",
      "Train loss: 0.5806155933637512\n",
      "Train loss: 0.5805987607320584\n",
      "Train loss: 0.5806191420674848\n",
      "Train loss: 0.5806348624763997\n",
      "Train loss: 0.5805899735666158\n",
      "Train loss: 0.5805982030861928\n",
      "Train loss: 0.5805683127951539\n",
      "Train loss: 0.5806635896442219\n",
      "Train loss: 0.5807157452129836\n",
      "Train loss: 0.5807027311059948\n",
      "Train loss: 0.5807353153260619\n",
      "Train loss: 0.580666542257554\n",
      "Train loss: 0.5805898522918879\n",
      "Train loss: 0.5805357633578488\n",
      "Train loss: 0.5805530773854678\n",
      "Train loss: 0.5805656277227317\n",
      "Train loss: 0.5806122909753448\n",
      "Train loss: 0.5805359234735238\n",
      "Train loss: 0.5805358942068425\n",
      "Train loss: 0.5805162699701358\n",
      "Train loss: 0.5804825522053402\n",
      "Train loss: 0.5804216437074976\n",
      "Train loss: 0.580324518718552\n",
      "Train loss: 0.5803150620729242\n",
      "Train loss: 0.5803204116224672\n",
      "Train loss: 0.5801809634853351\n",
      "Train loss: 0.5802293202284965\n",
      "Train loss: 0.5802370092988037\n",
      "Train loss: 0.5802886045739479\n",
      "Train loss: 0.580288144565655\n",
      "Train loss: 0.5802601639821213\n",
      "Train loss: 0.5802274104663075\n",
      "Train loss: 0.580223862522928\n",
      "Train loss: 0.5802295527142007\n",
      "Train loss: 0.5802327626898491\n",
      "Train loss: 0.5802158394167146\n",
      "Train loss: 0.5802158446926886\n",
      "Train loss: 0.5801753623628294\n",
      "Train loss: 0.5801202193739892\n",
      "Train loss: 0.5801525310904981\n",
      "Train loss: 0.5800186354108053\n",
      "Train loss: 0.5799813299572367\n",
      "Train loss: 0.5800015059000789\n",
      "Train loss: 0.5799875526284294\n",
      "Train loss: 0.5799608816082611\n",
      "Train loss: 0.5800390887934808\n",
      "Train loss: 0.5801081626285489\n",
      "Train loss: 0.580033944403124\n",
      "Train loss: 0.5800828231042008\n",
      "Train loss: 0.5800768092631626\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6943595632910728\n",
      "Val loss: 0.6201393637392256\n",
      "Val loss: 0.592947204198156\n",
      "Val loss: 0.5927362300847706\n",
      "Val loss: 0.5931954545279344\n",
      "Val loss: 0.5970035698907129\n",
      "Val loss: 0.5909744229386834\n",
      "Val loss: 0.5860393192523565\n",
      "Val loss: 0.5812708186832342\n",
      "Val loss: 0.581204137631825\n",
      "Val loss: 0.5815013376650987\n",
      "Val loss: 0.5804160767692631\n",
      "Val loss: 0.5795735553838313\n",
      "Val loss: 0.5792029141515925\n",
      "Val loss: 0.5782102696798943\n",
      "Val loss: 0.5765361284153371\n",
      "Val loss: 0.5749831288343384\n",
      "Val loss: 0.5742151723818832\n",
      "Val loss: 0.5750427284139268\n",
      "Val loss: 0.5729794914674278\n",
      "Val loss: 0.5734928826299998\n",
      "Val loss: 0.5721411494486922\n",
      "Val loss: 0.5710819828928563\n",
      "Val loss: 0.5707787578346348\n",
      "Val loss: 0.5707134380936623\n",
      "Val loss: 0.5694560334663983\n",
      "Val loss: 0.5690841834936569\n",
      "Val loss: 0.5677829161822368\n",
      "Val loss: 0.5670819456378619\n",
      "Val loss: 0.5673912019537599\n",
      "Val loss: 0.5670202618295496\n",
      "Val loss: 0.567022202149877\n",
      "Val loss: 0.5668658283425541\n",
      "Val loss: 0.566291304735037\n",
      "Val loss: 0.5659533996006538\n",
      "Val loss: 0.5657513411351422\n",
      "Val loss: 0.56423076711919\n",
      "Val loss: 0.5640877496313166\n",
      "Val loss: 0.5648397844784039\n",
      "Val loss: 0.5640088288328755\n",
      "Val loss: 0.5644850716287014\n",
      "Val loss: 0.5639805400200437\n",
      "Val loss: 0.5634750838034621\n",
      "Val loss: 0.563620241537486\n",
      "Val loss: 0.5631899647414684\n",
      "Val loss: 0.5634360982341017\n",
      "Val loss: 0.5632463591730493\n",
      "Val loss: 0.5633097559088942\n",
      "Val loss: 0.5633685797697208\n",
      "Val loss: 0.5634435619934496\n",
      "Val loss: 0.5628980710281162\n",
      "Val loss: 0.5621873521206462\n",
      "Val loss: 0.5620112045470512\n",
      "Val loss: 0.5616128527763607\n",
      "Val loss: 0.5616925786228946\n",
      "Val loss: 0.5611355303649834\n",
      "Val loss: 0.5612117480224287\n",
      "Val loss: 0.5614002540862272\n",
      "Val loss: 0.5614662611362885\n",
      "Val loss: 0.5613337566820674\n",
      "Val loss: 0.5614956473245433\n",
      "Val loss: 0.561553073064409\n",
      "Val loss: 0.5610255370284342\n",
      "Val loss: 0.5607266277541935\n",
      "Val loss: 0.5605522225852366\n",
      "Val loss: 0.560644653702217\n",
      "Val loss: 0.5604585825861571\n",
      "Val loss: 0.560064150872132\n",
      "Val loss: 0.5609686807837597\n",
      "Val loss: 0.5608339512074916\n",
      "Val loss: 0.5601743369621072\n",
      "Val loss: 0.5601502734140433\n",
      "Val loss: 0.5598389893606469\n",
      "Val loss: 0.5601207918429439\n",
      "Val loss: 0.5602448632851004\n",
      "Val loss: 0.5598733747540174\n",
      "Val loss: 0.5599964605644345\n",
      "Val loss: 0.5597056731160318\n",
      "Val loss: 0.559275288267184\n",
      "Val loss: 0.5594109897326706\n",
      "Val loss: 0.5597067776292858\n",
      "Val loss: 0.5592733196058017\n",
      "Val loss: 0.5594654581397052\n",
      "Val loss: 0.5591476903894921\n",
      "Val loss: 0.5588749758196327\n",
      "Val loss: 0.5590263437002133\n",
      "Val loss: 0.5588898149228865\n",
      "Val loss: 0.5588759353600765\n",
      "Val loss: 0.5583987405171266\n",
      "Val loss: 0.5582277897604324\n",
      "Val loss: 0.5584231187724857\n",
      "Val loss: 0.5584676450763653\n",
      "Val loss: 0.5585061899418461\n",
      "Val loss: 0.5584668244507267\n",
      "Val loss: 0.5582518888802468\n",
      "Val loss: 0.5584495941756413\n",
      "Val loss: 0.5585121423129208\n",
      "Val loss: 0.5585638058575628\n",
      "Val loss: 0.5585388857946705\n",
      "Val loss: 0.5589985483753418\n",
      "Val loss: 0.5589005426163711\n",
      "Val loss: 0.5587395914996771\n",
      "Val loss: 0.5590462454446095\n",
      "Val loss: 0.5588400140548258\n",
      "Val loss: 0.5587710505564705\n",
      "Val loss: 0.5586641580148095\n",
      "Val loss: 0.5583185959286457\n",
      "Val loss: 0.5582746285565046\n",
      "Val loss: 0.5580729732070776\n",
      "Val loss: 0.5581901645074125\n",
      "Val loss: 0.5582498368481006\n",
      "Val loss: 0.5583107575427654\n",
      "Val loss: 0.5583331402628979\n",
      "Val loss: 0.5585840803772368\n",
      "Val loss: 0.5584319154456102\n",
      "Val loss: 0.5585676207752426\n",
      "Val loss: 0.558738078207594\n",
      "Val loss: 0.5587889546044613\n",
      "Val loss: 0.5588960361099403\n",
      "Val loss: 0.5591112941951306\n",
      "Val loss: 0.5593106036648056\n",
      "Val loss: 0.5596247394194548\n",
      "Val loss: 0.5597163857679802\n",
      "Val loss: 0.559750545824094\n",
      "Val loss: 0.559932880055828\n",
      "Val loss: 0.5600359355038793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5599152684587786\n",
      "\n",
      "starting Epoch 2\n",
      "Training...\n",
      "Train loss: 0.5985343973887595\n",
      "Train loss: 0.5862400936774719\n",
      "Train loss: 0.5837015666193881\n",
      "Train loss: 0.5794080725953549\n",
      "Train loss: 0.5770099328623878\n",
      "Train loss: 0.5773335770398629\n",
      "Train loss: 0.5788474263047143\n",
      "Train loss: 0.5776897093410012\n",
      "Train loss: 0.5777240392216091\n",
      "Train loss: 0.577887612221828\n",
      "Train loss: 0.5796865476593035\n",
      "Train loss: 0.5808389027248366\n",
      "Train loss: 0.5793048878664215\n",
      "Train loss: 0.58077821391885\n",
      "Train loss: 0.5809343083447038\n",
      "Train loss: 0.5813448661733944\n",
      "Train loss: 0.5814033193750016\n",
      "Train loss: 0.5808309153095925\n",
      "Train loss: 0.5808554183996447\n",
      "Train loss: 0.5803705757124382\n",
      "Train loss: 0.5802512114827559\n",
      "Train loss: 0.5783061377008304\n",
      "Train loss: 0.5776148915031117\n",
      "Train loss: 0.5786555233977284\n",
      "Train loss: 0.5792921427973287\n",
      "Train loss: 0.5787403926913679\n",
      "Train loss: 0.5796216808688\n",
      "Train loss: 0.5797153531652017\n",
      "Train loss: 0.5800035044113383\n",
      "Train loss: 0.5796606428734647\n",
      "Train loss: 0.5794505142337478\n",
      "Train loss: 0.5792529006052838\n",
      "Train loss: 0.579527824936217\n",
      "Train loss: 0.5789447870100018\n",
      "Train loss: 0.5784384484198983\n",
      "Train loss: 0.5781952909949094\n",
      "Train loss: 0.5781289466905013\n",
      "Train loss: 0.5782320374949333\n",
      "Train loss: 0.5777675006371255\n",
      "Train loss: 0.5778304048637872\n",
      "Train loss: 0.5780208215244815\n",
      "Train loss: 0.5778205699872345\n",
      "Train loss: 0.5778746468918149\n",
      "Train loss: 0.5772034779479208\n",
      "Train loss: 0.5775628699211444\n",
      "Train loss: 0.5773909068276236\n",
      "Train loss: 0.5776558916726179\n",
      "Train loss: 0.5770737754726808\n",
      "Train loss: 0.5773742150237539\n",
      "Train loss: 0.5774549902738394\n",
      "Train loss: 0.5769995841416106\n",
      "Train loss: 0.5770937864371971\n",
      "Train loss: 0.5772137708040315\n",
      "Train loss: 0.5770283852383646\n",
      "Train loss: 0.5770570940327059\n",
      "Train loss: 0.5768730982112714\n",
      "Train loss: 0.5771060442851237\n",
      "Train loss: 0.5768137207259588\n",
      "Train loss: 0.5767432220463433\n",
      "Train loss: 0.5768767707391617\n",
      "Train loss: 0.5770396895346043\n",
      "Train loss: 0.5768647422176681\n",
      "Train loss: 0.5769182006407579\n",
      "Train loss: 0.5767800750398747\n",
      "Train loss: 0.5770186392548821\n",
      "Train loss: 0.5768614109263807\n",
      "Train loss: 0.5767674948894951\n",
      "Train loss: 0.5769162340976457\n",
      "Train loss: 0.5770876374125394\n",
      "Train loss: 0.5771952019451856\n",
      "Train loss: 0.5773798222899689\n",
      "Train loss: 0.5774748640353685\n",
      "Train loss: 0.5775227046854452\n",
      "Train loss: 0.5776128848716163\n",
      "Train loss: 0.5779262187880146\n",
      "Train loss: 0.577701236500875\n",
      "Train loss: 0.5775413027903412\n",
      "Train loss: 0.5777109160761255\n",
      "Train loss: 0.5778964777359712\n",
      "Train loss: 0.5778352639651284\n",
      "Train loss: 0.5779377261638052\n",
      "Train loss: 0.5780521462864088\n",
      "Train loss: 0.578100751580209\n",
      "Train loss: 0.5781639061065001\n",
      "Train loss: 0.5779730093661584\n",
      "Train loss: 0.5782181326042017\n",
      "Train loss: 0.5783603582970366\n",
      "Train loss: 0.5784202974550151\n",
      "Train loss: 0.5783218245381649\n",
      "Train loss: 0.5784336401728407\n",
      "Train loss: 0.5783934438208946\n",
      "Train loss: 0.5785371757448206\n",
      "Train loss: 0.5785862041648323\n",
      "Train loss: 0.5786885018676059\n",
      "Train loss: 0.5789241987256517\n",
      "Train loss: 0.5787933939071047\n",
      "Train loss: 0.5790344829593754\n",
      "Train loss: 0.578887385242505\n",
      "Train loss: 0.578648991043246\n",
      "Train loss: 0.5787357107557732\n",
      "Train loss: 0.5788644422187021\n",
      "Train loss: 0.578827168074468\n",
      "Train loss: 0.5791341829873105\n",
      "Train loss: 0.5791594372195885\n",
      "Train loss: 0.5793140319740846\n",
      "Train loss: 0.5793541755497371\n",
      "Train loss: 0.5793128488023686\n",
      "Train loss: 0.5793216922262191\n",
      "Train loss: 0.5792609739599014\n",
      "Train loss: 0.5791415284578775\n",
      "Train loss: 0.5790750760788206\n",
      "Train loss: 0.5791641859909209\n",
      "Train loss: 0.5791768174273824\n",
      "Train loss: 0.579225260301764\n",
      "Train loss: 0.5791642483341637\n",
      "Train loss: 0.5791266001777023\n",
      "Train loss: 0.5790873366627443\n",
      "Train loss: 0.5790297205087339\n",
      "Train loss: 0.579125144242138\n",
      "Train loss: 0.5791988706976336\n",
      "Train loss: 0.5792803939535086\n",
      "Train loss: 0.5791427045632894\n",
      "Train loss: 0.5791053856204516\n",
      "Train loss: 0.5791574057887379\n",
      "Train loss: 0.579242366929205\n",
      "Train loss: 0.5792324009065639\n",
      "Train loss: 0.5791572961881434\n",
      "Train loss: 0.5790671046449133\n",
      "Train loss: 0.5790344831692983\n",
      "Train loss: 0.5790395887941983\n",
      "Train loss: 0.5789419544384925\n",
      "Train loss: 0.5790508519229947\n",
      "Train loss: 0.5790621131468017\n",
      "Train loss: 0.5788992496736461\n",
      "Train loss: 0.5789384464062157\n",
      "Train loss: 0.5790180994246364\n",
      "Train loss: 0.5789554993812078\n",
      "Train loss: 0.5789226132508161\n",
      "Train loss: 0.578988756913425\n",
      "Train loss: 0.5790471549586085\n",
      "Train loss: 0.5790813668013549\n",
      "Train loss: 0.5790499560679294\n",
      "Train loss: 0.5791005575081484\n",
      "Train loss: 0.5791256667611996\n",
      "Train loss: 0.579100176841072\n",
      "Train loss: 0.5790465671279407\n",
      "Train loss: 0.5789262912901294\n",
      "Train loss: 0.5789476207943131\n",
      "Train loss: 0.5789976151708229\n",
      "Train loss: 0.5789245549600575\n",
      "Train loss: 0.5788849633077234\n",
      "Train loss: 0.578897751399505\n",
      "Train loss: 0.5788654613997586\n",
      "Train loss: 0.5788668811940729\n",
      "Train loss: 0.5789875627314133\n",
      "Train loss: 0.5789956764920318\n",
      "Train loss: 0.578854162446554\n",
      "Train loss: 0.5789192742130054\n",
      "Train loss: 0.5788253427170552\n",
      "Train loss: 0.5788481958529546\n",
      "Train loss: 0.5787698236653137\n",
      "Train loss: 0.5786920707786698\n",
      "Train loss: 0.5786636945714099\n",
      "Train loss: 0.5787562221952167\n",
      "Train loss: 0.5787029289303133\n",
      "Train loss: 0.5786241630934634\n",
      "Train loss: 0.5786632524881937\n",
      "Train loss: 0.5787372745708824\n",
      "Train loss: 0.5789479969185674\n",
      "Train loss: 0.5789701547402429\n",
      "Train loss: 0.5789278173314162\n",
      "Train loss: 0.5789417611647913\n",
      "Train loss: 0.5790425421668464\n",
      "Train loss: 0.5790023558456413\n",
      "Train loss: 0.5789633028914023\n",
      "Train loss: 0.5789348528648717\n",
      "Train loss: 0.5789199279388084\n",
      "Train loss: 0.5789392225174262\n",
      "Train loss: 0.5789508637968273\n",
      "Train loss: 0.5789968896803176\n",
      "Train loss: 0.5790172735937856\n",
      "Train loss: 0.5790147871294988\n",
      "Train loss: 0.579035501958699\n",
      "Train loss: 0.5790192874578329\n",
      "Train loss: 0.5790733074893756\n",
      "Train loss: 0.5790212738895391\n",
      "Train loss: 0.5790528436337093\n",
      "Train loss: 0.5790331777215924\n",
      "Train loss: 0.5790810645115568\n",
      "Train loss: 0.5791081381051219\n",
      "Train loss: 0.5790905877772224\n",
      "Train loss: 0.5792736046930315\n",
      "Train loss: 0.5793193795344903\n",
      "Train loss: 0.5793315888250203\n",
      "Train loss: 0.5792195551454731\n",
      "Train loss: 0.5793049545365957\n",
      "Train loss: 0.5793757242171648\n",
      "Train loss: 0.5793047418239653\n",
      "Train loss: 0.5792004445010197\n",
      "Train loss: 0.5791650277490943\n",
      "Train loss: 0.5792242352064939\n",
      "Train loss: 0.5792358476105053\n",
      "Train loss: 0.5793463981968042\n",
      "Train loss: 0.5792741681773104\n",
      "Train loss: 0.5792009343269774\n",
      "Train loss: 0.5792692628451475\n",
      "Train loss: 0.5793508239185847\n",
      "Train loss: 0.579407420839705\n",
      "Train loss: 0.5793401761384751\n",
      "Train loss: 0.5792843002056786\n",
      "Train loss: 0.5793437987290505\n",
      "Train loss: 0.5792975509453562\n",
      "Train loss: 0.5792507381401703\n",
      "Train loss: 0.579157380654896\n",
      "Train loss: 0.5791595494381132\n",
      "Train loss: 0.5791404857958984\n",
      "Train loss: 0.5790636252219514\n",
      "Train loss: 0.5789023417826502\n",
      "Train loss: 0.578874532226069\n",
      "Train loss: 0.5788736822700522\n",
      "Train loss: 0.5790043639621467\n",
      "Train loss: 0.5790443753747688\n",
      "Train loss: 0.5790776138710425\n",
      "Train loss: 0.5790976885573942\n",
      "Train loss: 0.5789934228091379\n",
      "Train loss: 0.5790440638420408\n",
      "Train loss: 0.5790820508558442\n",
      "Train loss: 0.5789795158319396\n",
      "Train loss: 0.5790161707628292\n",
      "Train loss: 0.5789348150587881\n",
      "Train loss: 0.5788623385291048\n",
      "Train loss: 0.5788604860599762\n",
      "Train loss: 0.5788867122166452\n",
      "Train loss: 0.5789138309515856\n",
      "Train loss: 0.5789323366431638\n",
      "Train loss: 0.5789830454718539\n",
      "Train loss: 0.5789199064655328\n",
      "Train loss: 0.5789779447023797\n",
      "Train loss: 0.5789265397216615\n",
      "Train loss: 0.5789173673368737\n",
      "Train loss: 0.5789051171184254\n",
      "Train loss: 0.5789762982580134\n",
      "Train loss: 0.5790930999897959\n",
      "Train loss: 0.5790707439911075\n",
      "Train loss: 0.5789844377730958\n",
      "Train loss: 0.5789945980753204\n",
      "Train loss: 0.579011333093732\n",
      "Train loss: 0.5790657380471477\n",
      "Train loss: 0.5790300977220495\n",
      "Train loss: 0.5790240126362942\n",
      "Train loss: 0.5790801673955379\n",
      "Train loss: 0.5790860538599534\n",
      "Train loss: 0.5790831528531867\n",
      "Train loss: 0.5792118355288565\n",
      "Train loss: 0.5792234821468271\n",
      "Train loss: 0.5792192769926212\n",
      "Train loss: 0.5792598219946865\n",
      "Train loss: 0.5792689020145214\n",
      "Train loss: 0.5793184362960428\n",
      "Train loss: 0.5792482884360359\n",
      "Train loss: 0.5792517356259038\n",
      "Train loss: 0.5791951232817502\n",
      "Train loss: 0.579196334984575\n",
      "Train loss: 0.5792556760843606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5792729540081784\n",
      "Train loss: 0.579250003699186\n",
      "Train loss: 0.5792655532258261\n",
      "Train loss: 0.5793290114832094\n",
      "Train loss: 0.5793044645616968\n",
      "Train loss: 0.5792079317799363\n",
      "Train loss: 0.5792123822075653\n",
      "Train loss: 0.5792151532600109\n",
      "Train loss: 0.5792332545678942\n",
      "Train loss: 0.5792539543719551\n",
      "Train loss: 0.5792441552260676\n",
      "Train loss: 0.579228530658676\n",
      "Train loss: 0.5792362580479731\n",
      "Train loss: 0.579270339742327\n",
      "Train loss: 0.5792945654914892\n",
      "Train loss: 0.5792353657297331\n",
      "Train loss: 0.5792989055071677\n",
      "Train loss: 0.5793254788219137\n",
      "Train loss: 0.579393688197649\n",
      "Train loss: 0.5793822592602877\n",
      "Train loss: 0.5793908513320247\n",
      "Train loss: 0.5793789100930457\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6978821381926537\n",
      "Val loss: 0.6223322517342038\n",
      "Val loss: 0.5934756930385318\n",
      "Val loss: 0.5936701250703711\n",
      "Val loss: 0.5935251973569393\n",
      "Val loss: 0.5971459633317487\n",
      "Val loss: 0.5907586576307521\n",
      "Val loss: 0.5850107081425495\n",
      "Val loss: 0.5806620872833512\n",
      "Val loss: 0.5810868490715416\n",
      "Val loss: 0.581477979267085\n",
      "Val loss: 0.5807500213889752\n",
      "Val loss: 0.579549971036613\n",
      "Val loss: 0.5793204005213751\n",
      "Val loss: 0.5781456763679916\n",
      "Val loss: 0.5764344010172011\n",
      "Val loss: 0.5751317696911948\n",
      "Val loss: 0.5740237530697597\n",
      "Val loss: 0.5744816007766318\n",
      "Val loss: 0.5722214070835498\n",
      "Val loss: 0.572965626246654\n",
      "Val loss: 0.5712845582480824\n",
      "Val loss: 0.5703753920500738\n",
      "Val loss: 0.5703276037668982\n",
      "Val loss: 0.5703130621583231\n",
      "Val loss: 0.5691812280998674\n",
      "Val loss: 0.5689475289476451\n",
      "Val loss: 0.5677549004983559\n",
      "Val loss: 0.5666490590406789\n",
      "Val loss: 0.5670207618866991\n",
      "Val loss: 0.5666012984591645\n",
      "Val loss: 0.566855806599623\n",
      "Val loss: 0.5668148202140156\n",
      "Val loss: 0.5660761608879947\n",
      "Val loss: 0.5657367793650463\n",
      "Val loss: 0.5656120442478351\n",
      "Val loss: 0.5639149703733299\n",
      "Val loss: 0.5637544038749877\n",
      "Val loss: 0.5644476049646889\n",
      "Val loss: 0.5636426271805212\n",
      "Val loss: 0.5640923458279348\n",
      "Val loss: 0.5637406186338817\n",
      "Val loss: 0.5630249340679044\n",
      "Val loss: 0.5633544193827398\n",
      "Val loss: 0.563025273781802\n",
      "Val loss: 0.563399952962409\n",
      "Val loss: 0.5630367855485688\n",
      "Val loss: 0.5629300053648371\n",
      "Val loss: 0.5630075207254925\n",
      "Val loss: 0.5630483530372022\n",
      "Val loss: 0.5623568084061615\n",
      "Val loss: 0.5615430774605873\n",
      "Val loss: 0.5615059655498374\n",
      "Val loss: 0.5610452300997029\n",
      "Val loss: 0.5611636648430441\n",
      "Val loss: 0.5605801584259156\n",
      "Val loss: 0.5606763920104\n",
      "Val loss: 0.5609806973010198\n",
      "Val loss: 0.5610076378600127\n",
      "Val loss: 0.5608845878404917\n",
      "Val loss: 0.561094573551887\n",
      "Val loss: 0.5610657268743299\n",
      "Val loss: 0.5605053002857099\n",
      "Val loss: 0.5602112409649972\n",
      "Val loss: 0.5599828949313105\n",
      "Val loss: 0.5600059769798557\n",
      "Val loss: 0.559796660692392\n",
      "Val loss: 0.559436799238565\n",
      "Val loss: 0.5604657179914242\n",
      "Val loss: 0.5604466080324014\n",
      "Val loss: 0.5598625517160879\n",
      "Val loss: 0.5597799371378004\n",
      "Val loss: 0.5594901011867838\n",
      "Val loss: 0.559782781252047\n",
      "Val loss: 0.5598118018339024\n",
      "Val loss: 0.5592781728364546\n",
      "Val loss: 0.5593832313703994\n",
      "Val loss: 0.5590824338494965\n",
      "Val loss: 0.5586601909495852\n",
      "Val loss: 0.5587233343070611\n",
      "Val loss: 0.5590558324267368\n",
      "Val loss: 0.5585869697166247\n",
      "Val loss: 0.5588212342221955\n",
      "Val loss: 0.5584915887768912\n",
      "Val loss: 0.5582337026607316\n",
      "Val loss: 0.5583275652431942\n",
      "Val loss: 0.5581579322364473\n",
      "Val loss: 0.5581333770295753\n",
      "Val loss: 0.5576753044450605\n",
      "Val loss: 0.5574901513506416\n",
      "Val loss: 0.557675571628079\n",
      "Val loss: 0.5576932988005786\n",
      "Val loss: 0.5577069770801684\n",
      "Val loss: 0.5576693354639163\n",
      "Val loss: 0.5574915124897213\n",
      "Val loss: 0.5576481926789613\n",
      "Val loss: 0.5576774543228228\n",
      "Val loss: 0.5576825591684119\n",
      "Val loss: 0.55764536378596\n",
      "Val loss: 0.558167974492591\n",
      "Val loss: 0.5581522997291315\n",
      "Val loss: 0.5579415002951219\n",
      "Val loss: 0.5581914896747018\n",
      "Val loss: 0.5579475508374745\n",
      "Val loss: 0.557882756961666\n",
      "Val loss: 0.5578487606152244\n",
      "Val loss: 0.5575117811504822\n",
      "Val loss: 0.5574788857590068\n",
      "Val loss: 0.5572229905382675\n",
      "Val loss: 0.5573005855409174\n",
      "Val loss: 0.557285514764407\n",
      "Val loss: 0.5573419907225265\n",
      "Val loss: 0.5573265371276132\n",
      "Val loss: 0.5576197363056491\n",
      "Val loss: 0.5573950770216952\n",
      "Val loss: 0.5575485330377424\n",
      "Val loss: 0.5576924556126334\n",
      "Val loss: 0.557725285836108\n",
      "Val loss: 0.5578499502605863\n",
      "Val loss: 0.5580816751031923\n",
      "Val loss: 0.5582987081432185\n",
      "Val loss: 0.5586172548323037\n",
      "Val loss: 0.5588096223941456\n",
      "Val loss: 0.5588544954775994\n",
      "Val loss: 0.5591076238988302\n",
      "Val loss: 0.5592556755948332\n",
      "Val loss: 0.5591101937395541\n",
      "\n",
      "starting Epoch 3\n",
      "Training...\n",
      "Train loss: 0.587384178450233\n",
      "Train loss: 0.5940526609237378\n",
      "Train loss: 0.5930983343366849\n",
      "Train loss: 0.5908143158200421\n",
      "Train loss: 0.5904629847618065\n",
      "Train loss: 0.5898592639871004\n",
      "Train loss: 0.5897083601934446\n",
      "Train loss: 0.59033114505264\n",
      "Train loss: 0.5907146434544185\n",
      "Train loss: 0.5889685790442941\n",
      "Train loss: 0.5896870639498375\n",
      "Train loss: 0.5891123601083477\n",
      "Train loss: 0.5884709482487565\n",
      "Train loss: 0.5887759943162242\n",
      "Train loss: 0.5886668595781295\n",
      "Train loss: 0.5883148156550237\n",
      "Train loss: 0.5868653107357588\n",
      "Train loss: 0.5841739986268258\n",
      "Train loss: 0.5853808403486941\n",
      "Train loss: 0.5837727762702712\n",
      "Train loss: 0.5828728609693933\n",
      "Train loss: 0.5826228408308398\n",
      "Train loss: 0.5831468696817593\n",
      "Train loss: 0.5825752907107916\n",
      "Train loss: 0.5827859512311901\n",
      "Train loss: 0.5826519767328494\n",
      "Train loss: 0.5827520376897256\n",
      "Train loss: 0.583378060132745\n",
      "Train loss: 0.5832139504799982\n",
      "Train loss: 0.5829812799749868\n",
      "Train loss: 0.5825227516340709\n",
      "Train loss: 0.58274146592673\n",
      "Train loss: 0.5830638288818469\n",
      "Train loss: 0.5835451805451131\n",
      "Train loss: 0.5834087332516099\n",
      "Train loss: 0.5837809534248622\n",
      "Train loss: 0.5832448188361683\n",
      "Train loss: 0.5833425869740674\n",
      "Train loss: 0.5832348208448853\n",
      "Train loss: 0.5832457430893847\n",
      "Train loss: 0.5831047788135006\n",
      "Train loss: 0.5826302428992911\n",
      "Train loss: 0.5829429694812428\n",
      "Train loss: 0.5826500578612869\n",
      "Train loss: 0.5828031262049288\n",
      "Train loss: 0.5823906314710279\n",
      "Train loss: 0.5821187828803341\n",
      "Train loss: 0.581833804206878\n",
      "Train loss: 0.5821823799512725\n",
      "Train loss: 0.5820878851819444\n",
      "Train loss: 0.5821172846252715\n",
      "Train loss: 0.5821480372126454\n",
      "Train loss: 0.582191742316409\n",
      "Train loss: 0.582273069551634\n",
      "Train loss: 0.5823459093466578\n",
      "Train loss: 0.5821741570895862\n",
      "Train loss: 0.5821893759680171\n",
      "Train loss: 0.5817857212656217\n",
      "Train loss: 0.581723417641249\n",
      "Train loss: 0.582131553581101\n",
      "Train loss: 0.5823176695099612\n",
      "Train loss: 0.5820879343539308\n",
      "Train loss: 0.5817384525742959\n",
      "Train loss: 0.5818207951799979\n",
      "Train loss: 0.5817332353566224\n",
      "Train loss: 0.5815195675778335\n",
      "Train loss: 0.5819754811522674\n",
      "Train loss: 0.5817976438709705\n",
      "Train loss: 0.5813636196705636\n",
      "Train loss: 0.5817968239734819\n",
      "Train loss: 0.5820905319017957\n",
      "Train loss: 0.5820226166047845\n",
      "Train loss: 0.5821969357968357\n",
      "Train loss: 0.5819662454121818\n",
      "Train loss: 0.5816528419401743\n",
      "Train loss: 0.5814887219506553\n",
      "Train loss: 0.581327684731976\n",
      "Train loss: 0.5813375654283283\n",
      "Train loss: 0.5813635460102837\n",
      "Train loss: 0.5811582309816539\n",
      "Train loss: 0.5809683337706412\n",
      "Train loss: 0.5808764081164204\n",
      "Train loss: 0.5810499632480707\n",
      "Train loss: 0.5810764529223666\n",
      "Train loss: 0.581217120927246\n",
      "Train loss: 0.5812911599475745\n",
      "Train loss: 0.5809913842996144\n",
      "Train loss: 0.5809153653533029\n",
      "Train loss: 0.580748747177778\n",
      "Train loss: 0.5806185873499706\n",
      "Train loss: 0.5807034768135486\n",
      "Train loss: 0.5807795514549621\n",
      "Train loss: 0.580783092234967\n",
      "Train loss: 0.5807560736223254\n",
      "Train loss: 0.5805624000896837\n",
      "Train loss: 0.5806083215624793\n",
      "Train loss: 0.5804446502374951\n",
      "Train loss: 0.5806182626408543\n",
      "Train loss: 0.5804587796729523\n",
      "Train loss: 0.5803355227088022\n",
      "Train loss: 0.5801699627679784\n",
      "Train loss: 0.5802943443129495\n",
      "Train loss: 0.5800563703178491\n",
      "Train loss: 0.5803767787028299\n",
      "Train loss: 0.5804287327534248\n",
      "Train loss: 0.5802324765158352\n",
      "Train loss: 0.580334935513207\n",
      "Train loss: 0.5804771943714068\n",
      "Train loss: 0.5805427893934691\n",
      "Train loss: 0.5804740983066151\n",
      "Train loss: 0.5805107591494938\n",
      "Train loss: 0.5804377840187146\n",
      "Train loss: 0.5804259781422685\n",
      "Train loss: 0.5804194595244643\n",
      "Train loss: 0.5803116364886627\n",
      "Train loss: 0.5802591871140896\n",
      "Train loss: 0.5802158671930948\n",
      "Train loss: 0.5800602298613674\n",
      "Train loss: 0.5798512293466653\n",
      "Train loss: 0.5799131541512519\n",
      "Train loss: 0.5800973500482008\n",
      "Train loss: 0.5802028718518644\n",
      "Train loss: 0.5804423221294935\n",
      "Train loss: 0.5804019837477554\n",
      "Train loss: 0.580524052081465\n",
      "Train loss: 0.580384924445847\n",
      "Train loss: 0.5803259648573403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5802630661008134\n",
      "Train loss: 0.580329723139057\n",
      "Train loss: 0.5803663260251478\n",
      "Train loss: 0.5802701366748825\n",
      "Train loss: 0.580129655282034\n",
      "Train loss: 0.5802094007892329\n",
      "Train loss: 0.580124832792663\n",
      "Train loss: 0.5799207112090418\n",
      "Train loss: 0.579932712936454\n",
      "Train loss: 0.579768184405167\n",
      "Train loss: 0.5796616777059348\n",
      "Train loss: 0.5797631574667972\n",
      "Train loss: 0.5797645101465468\n",
      "Train loss: 0.5797581463343879\n",
      "Train loss: 0.5797220571501316\n",
      "Train loss: 0.5796158085777526\n",
      "Train loss: 0.5795894329976026\n",
      "Train loss: 0.5794565116754685\n",
      "Train loss: 0.5794088883978245\n",
      "Train loss: 0.5793248354151526\n",
      "Train loss: 0.5793178238631825\n",
      "Train loss: 0.5792369692467251\n",
      "Train loss: 0.5792745839619327\n",
      "Train loss: 0.5792481312593035\n",
      "Train loss: 0.5792427172626309\n",
      "Train loss: 0.5791484043270048\n",
      "Train loss: 0.5791971048158111\n",
      "Train loss: 0.5792395373089309\n",
      "Train loss: 0.5792111433748635\n",
      "Train loss: 0.5790850666629619\n",
      "Train loss: 0.5789660822084789\n",
      "Train loss: 0.5789281607105433\n",
      "Train loss: 0.5790124300421309\n",
      "Train loss: 0.5791447995871111\n",
      "Train loss: 0.5792368109412457\n",
      "Train loss: 0.5792124671561472\n",
      "Train loss: 0.5792289488233419\n",
      "Train loss: 0.5793422832023595\n",
      "Train loss: 0.5793878803252456\n",
      "Train loss: 0.5794203244450922\n",
      "Train loss: 0.5792926273093262\n",
      "Train loss: 0.5792736394160526\n",
      "Train loss: 0.5793213148999473\n",
      "Train loss: 0.5793926180472462\n",
      "Train loss: 0.5795013872680709\n",
      "Train loss: 0.5795449443758137\n",
      "Train loss: 0.5795772512879445\n",
      "Train loss: 0.5795729723052319\n",
      "Train loss: 0.579638182735809\n",
      "Train loss: 0.5797019820575089\n",
      "Train loss: 0.5795839050573792\n",
      "Train loss: 0.5796547534246623\n",
      "Train loss: 0.5797774619644104\n",
      "Train loss: 0.5796389979086188\n",
      "Train loss: 0.5795755532702498\n",
      "Train loss: 0.5796492391920832\n",
      "Train loss: 0.5797335112198567\n",
      "Train loss: 0.5798047834568715\n",
      "Train loss: 0.5798205143583497\n",
      "Train loss: 0.5798131351198031\n",
      "Train loss: 0.5798725717306835\n",
      "Train loss: 0.5799532479380577\n",
      "Train loss: 0.5799170978824287\n",
      "Train loss: 0.579992750859067\n",
      "Train loss: 0.5799643351063501\n",
      "Train loss: 0.5799976681668266\n",
      "Train loss: 0.5799870349507137\n",
      "Train loss: 0.5799389571738751\n",
      "Train loss: 0.5798561312039612\n",
      "Train loss: 0.5798308715568518\n",
      "Train loss: 0.5798246884668196\n",
      "Train loss: 0.579727715712212\n",
      "Train loss: 0.5796430225177358\n",
      "Train loss: 0.579683909652897\n",
      "Train loss: 0.5796499991363805\n",
      "Train loss: 0.5796214451957847\n",
      "Train loss: 0.5796628903161957\n",
      "Train loss: 0.5796428806434524\n",
      "Train loss: 0.5796562571727005\n",
      "Train loss: 0.579698073313875\n",
      "Train loss: 0.5798912134615382\n",
      "Train loss: 0.579861071705105\n",
      "Train loss: 0.5798208439639705\n",
      "Train loss: 0.5798469332121198\n",
      "Train loss: 0.5797845030286509\n",
      "Train loss: 0.5797736889797075\n",
      "Train loss: 0.5797617646766952\n",
      "Train loss: 0.5797612356235604\n",
      "Train loss: 0.5797465200376168\n",
      "Train loss: 0.5798436584808387\n",
      "Train loss: 0.5798140947398693\n",
      "Train loss: 0.5798588789511173\n",
      "Train loss: 0.5799037843475181\n",
      "Train loss: 0.5799209485835796\n",
      "Train loss: 0.5799881471159437\n",
      "Train loss: 0.5800302429072497\n",
      "Train loss: 0.579976266597259\n",
      "Train loss: 0.5799585021941178\n",
      "Train loss: 0.5799775740878196\n",
      "Train loss: 0.5799962886631502\n",
      "Train loss: 0.580002475383643\n",
      "Train loss: 0.580012052496283\n",
      "Train loss: 0.5800331863573671\n",
      "Train loss: 0.5800073360578661\n",
      "Train loss: 0.5799238589861496\n",
      "Train loss: 0.5799227984301085\n",
      "Train loss: 0.579947426994786\n",
      "Train loss: 0.5799251136931493\n",
      "Train loss: 0.5798924436607611\n",
      "Train loss: 0.5798760878056206\n",
      "Train loss: 0.5798465749074692\n",
      "Train loss: 0.5798180708702599\n",
      "Train loss: 0.5798257016084869\n",
      "Train loss: 0.5798643047902613\n",
      "Train loss: 0.5798387522169272\n",
      "Train loss: 0.5799038737098433\n",
      "Train loss: 0.5799296839263113\n",
      "Train loss: 0.5799657002556784\n",
      "Train loss: 0.579879335536538\n",
      "Train loss: 0.5798575088885612\n",
      "Train loss: 0.5799048227524897\n",
      "Train loss: 0.5799465132755648\n",
      "Train loss: 0.5799158228519178\n",
      "Train loss: 0.5799670361455965\n",
      "Train loss: 0.579937989500265\n",
      "Train loss: 0.5800018701647128\n",
      "Train loss: 0.5799331674368792\n",
      "Train loss: 0.5800062716942391\n",
      "Train loss: 0.5800172477046548\n",
      "Train loss: 0.5799837553923314\n",
      "Train loss: 0.580006618385607\n",
      "Train loss: 0.5799590496338802\n",
      "Train loss: 0.579923062438941\n",
      "Train loss: 0.5799158702444405\n",
      "Train loss: 0.5798736709996839\n",
      "Train loss: 0.5797504297645175\n",
      "Train loss: 0.5797793209372111\n",
      "Train loss: 0.579773549590162\n",
      "Train loss: 0.5796937994388602\n",
      "Train loss: 0.5797222312145587\n",
      "Train loss: 0.5797534260059787\n",
      "Train loss: 0.5797429292878823\n",
      "Train loss: 0.5797458361543799\n",
      "Train loss: 0.5797919488399753\n",
      "Train loss: 0.5798331541455303\n",
      "Train loss: 0.5798660225157642\n",
      "Train loss: 0.579848096109172\n",
      "Train loss: 0.5797918389051215\n",
      "Train loss: 0.5798218730060188\n",
      "Train loss: 0.5798139849075408\n",
      "Train loss: 0.5798711081131432\n",
      "Train loss: 0.5798326032598464\n",
      "Train loss: 0.579808842206474\n",
      "Train loss: 0.5798194271842659\n",
      "Train loss: 0.5797982160811265\n",
      "Train loss: 0.5797772557359636\n",
      "Train loss: 0.5797405394896548\n",
      "Train loss: 0.5797848494759065\n",
      "Train loss: 0.5798098637697517\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.7056534290313721\n",
      "Val loss: 0.6277084019449022\n",
      "Val loss: 0.5988420631204333\n",
      "Val loss: 0.5980752330077322\n",
      "Val loss: 0.596524732808272\n",
      "Val loss: 0.5990320793513594\n",
      "Val loss: 0.5924973645631004\n",
      "Val loss: 0.5866972941618699\n",
      "Val loss: 0.5817010687156157\n",
      "Val loss: 0.5817889084621352\n",
      "Val loss: 0.5826572201870106\n",
      "Val loss: 0.5819947194244902\n",
      "Val loss: 0.5808374043554068\n",
      "Val loss: 0.5805993632993837\n",
      "Val loss: 0.5799831892992999\n",
      "Val loss: 0.5785634532759462\n",
      "Val loss: 0.5772146749354544\n",
      "Val loss: 0.576084449719847\n",
      "Val loss: 0.5768844314078068\n",
      "Val loss: 0.5746258205235607\n",
      "Val loss: 0.5755403348459647\n",
      "Val loss: 0.5740378239286055\n",
      "Val loss: 0.5730059267136088\n",
      "Val loss: 0.5727711637981799\n",
      "Val loss: 0.5728022160549318\n",
      "Val loss: 0.5716451212417247\n",
      "Val loss: 0.5714376388645884\n",
      "Val loss: 0.5701348612205588\n",
      "Val loss: 0.5693636153721147\n",
      "Val loss: 0.5695450143926095\n",
      "Val loss: 0.5691757149897613\n",
      "Val loss: 0.5694291556031449\n",
      "Val loss: 0.5694719551176559\n",
      "Val loss: 0.5686955129253793\n",
      "Val loss: 0.5683233528986744\n",
      "Val loss: 0.568176449011158\n",
      "Val loss: 0.5666652958354225\n",
      "Val loss: 0.566530055627621\n",
      "Val loss: 0.5671593241163135\n",
      "Val loss: 0.566280251621601\n",
      "Val loss: 0.5665545950041098\n",
      "Val loss: 0.566063479943709\n",
      "Val loss: 0.5654442407817484\n",
      "Val loss: 0.5656784576912449\n",
      "Val loss: 0.5652674338115113\n",
      "Val loss: 0.5655770546484202\n",
      "Val loss: 0.5652799173298045\n",
      "Val loss: 0.5650974051972314\n",
      "Val loss: 0.5650884087701313\n",
      "Val loss: 0.5650801714883751\n",
      "Val loss: 0.5644685799681296\n",
      "Val loss: 0.5637142670891\n",
      "Val loss: 0.5636346790600907\n",
      "Val loss: 0.5632014105089536\n",
      "Val loss: 0.5633166478063069\n",
      "Val loss: 0.5626957661148467\n",
      "Val loss: 0.5627394149630842\n",
      "Val loss: 0.5630113002956945\n",
      "Val loss: 0.5631397974734403\n",
      "Val loss: 0.5630214509357976\n",
      "Val loss: 0.5632031285449078\n",
      "Val loss: 0.5630987042555146\n",
      "Val loss: 0.5626674193865174\n",
      "Val loss: 0.5624524675379726\n",
      "Val loss: 0.5622788635484967\n",
      "Val loss: 0.5623257941931576\n",
      "Val loss: 0.5621640539454843\n",
      "Val loss: 0.5618068966190372\n",
      "Val loss: 0.5628614259320636\n",
      "Val loss: 0.5627893442581581\n",
      "Val loss: 0.562119120510958\n",
      "Val loss: 0.5621171886708412\n",
      "Val loss: 0.5618216252097716\n",
      "Val loss: 0.5620415238991663\n",
      "Val loss: 0.5620482073907547\n",
      "Val loss: 0.5615452057767983\n",
      "Val loss: 0.561640658415854\n",
      "Val loss: 0.5613582723398135\n",
      "Val loss: 0.5609494811538512\n",
      "Val loss: 0.5610877382277247\n",
      "Val loss: 0.5614119788060093\n",
      "Val loss: 0.5609673120281807\n",
      "Val loss: 0.5612235152778994\n",
      "Val loss: 0.5609395807711207\n",
      "Val loss: 0.5607066401192602\n",
      "Val loss: 0.5607921294260136\n",
      "Val loss: 0.5606032684788725\n",
      "Val loss: 0.5606197450725798\n",
      "Val loss: 0.5601573477725725\n",
      "Val loss: 0.5600009970781797\n",
      "Val loss: 0.5601638533207813\n",
      "Val loss: 0.5601289657985463\n",
      "Val loss: 0.5602107272194378\n",
      "Val loss: 0.5602004265607293\n",
      "Val loss: 0.5600266830704886\n",
      "Val loss: 0.5601402649924252\n",
      "Val loss: 0.5601580930519695\n",
      "Val loss: 0.5601643414707028\n",
      "Val loss: 0.5601166179064314\n",
      "Val loss: 0.5606029326308944\n",
      "Val loss: 0.5605537664322626\n",
      "Val loss: 0.5603303971365533\n",
      "Val loss: 0.5606442207492279\n",
      "Val loss: 0.5604458208947742\n",
      "Val loss: 0.5603745446632836\n",
      "Val loss: 0.5603073939033176\n",
      "Val loss: 0.5600217530566655\n",
      "Val loss: 0.5599557634629655\n",
      "Val loss: 0.5597341874723926\n",
      "Val loss: 0.5598615856878527\n",
      "Val loss: 0.5598716451265321\n",
      "Val loss: 0.5599445220289588\n",
      "Val loss: 0.5599442782237175\n",
      "Val loss: 0.5601606995967234\n",
      "Val loss: 0.5600096826873174\n",
      "Val loss: 0.5601896627682272\n",
      "Val loss: 0.5603124844803385\n",
      "Val loss: 0.5603210435253655\n",
      "Val loss: 0.5604153058745645\n",
      "Val loss: 0.560627861940403\n",
      "Val loss: 0.5608946666713582\n",
      "Val loss: 0.5612161525070961\n",
      "Val loss: 0.5613783397484292\n",
      "Val loss: 0.561425930319387\n",
      "Val loss: 0.5616110507398844\n",
      "Val loss: 0.5617449933186245\n",
      "Val loss: 0.5616545829487147\n",
      "\n",
      "starting Epoch 4\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6135598985772384\n",
      "Train loss: 0.6074904937010545\n",
      "Train loss: 0.5957499045436665\n",
      "Train loss: 0.5928562325012835\n",
      "Train loss: 0.5873410135808618\n",
      "Train loss: 0.5879822298258293\n",
      "Train loss: 0.5866716555554232\n",
      "Train loss: 0.5847490398014117\n",
      "Train loss: 0.5845425974723347\n",
      "Train loss: 0.582405498279399\n",
      "Train loss: 0.5825322809284681\n",
      "Train loss: 0.5833694732837598\n",
      "Train loss: 0.5843389264405003\n",
      "Train loss: 0.5827137893032429\n",
      "Train loss: 0.5807899283525537\n",
      "Train loss: 0.5801851031922248\n",
      "Train loss: 0.5802116833605245\n",
      "Train loss: 0.5797337672172482\n",
      "Train loss: 0.5802277544400308\n",
      "Train loss: 0.5798070577152988\n",
      "Train loss: 0.5799895061940168\n",
      "Train loss: 0.5792250838258001\n",
      "Train loss: 0.579969178682319\n",
      "Train loss: 0.5799148979291537\n",
      "Train loss: 0.5800855977382354\n",
      "Train loss: 0.5809527517858039\n",
      "Train loss: 0.5810678165012034\n",
      "Train loss: 0.5808081137472914\n",
      "Train loss: 0.580533819083311\n",
      "Train loss: 0.5809372269450523\n",
      "Train loss: 0.5805480071916719\n",
      "Train loss: 0.5807862607805941\n",
      "Train loss: 0.5808922446280582\n",
      "Train loss: 0.5810447421709463\n",
      "Train loss: 0.581009769686984\n",
      "Train loss: 0.580899110506904\n",
      "Train loss: 0.5806922428259508\n",
      "Train loss: 0.5807502804930188\n",
      "Train loss: 0.5806845579588061\n",
      "Train loss: 0.5811022083287842\n",
      "Train loss: 0.5813244567758725\n",
      "Train loss: 0.5812771596343072\n",
      "Train loss: 0.5816954745194409\n",
      "Train loss: 0.5808309618726932\n",
      "Train loss: 0.5809553160484429\n",
      "Train loss: 0.5810105441314481\n",
      "Train loss: 0.5805980372733582\n",
      "Train loss: 0.5804144387076122\n",
      "Train loss: 0.5803851856335435\n",
      "Train loss: 0.5802777752921626\n",
      "Train loss: 0.5802359319236258\n",
      "Train loss: 0.5798272526929652\n",
      "Train loss: 0.5801162959777375\n",
      "Train loss: 0.5801099363966492\n",
      "Train loss: 0.5801588753823045\n",
      "Train loss: 0.580323293312412\n",
      "Train loss: 0.5801229471695643\n",
      "Train loss: 0.5798947882096864\n",
      "Train loss: 0.5803445990152335\n",
      "Train loss: 0.5802366956757744\n",
      "Train loss: 0.5799510481662687\n",
      "Train loss: 0.5799621968282818\n",
      "Train loss: 0.5803754784882873\n",
      "Train loss: 0.5804384211901112\n",
      "Train loss: 0.5803882559507604\n",
      "Train loss: 0.5804752475208546\n",
      "Train loss: 0.5803399850754172\n",
      "Train loss: 0.5804665857505237\n",
      "Train loss: 0.5804958424981389\n",
      "Train loss: 0.580447792644753\n",
      "Train loss: 0.5802796895315817\n",
      "Train loss: 0.5802995582021219\n",
      "Train loss: 0.5802571016112936\n",
      "Train loss: 0.5801054389191448\n",
      "Train loss: 0.580005779633767\n",
      "Train loss: 0.5798037751989195\n",
      "Train loss: 0.5797237155110593\n",
      "Train loss: 0.5798482828372713\n",
      "Train loss: 0.5797379304141165\n",
      "Train loss: 0.5798495133419646\n",
      "Train loss: 0.5798430650450699\n",
      "Train loss: 0.5798127078806358\n",
      "Train loss: 0.5799266033103626\n",
      "Train loss: 0.5801072258761839\n",
      "Train loss: 0.5800668737900966\n",
      "Train loss: 0.5802196672380213\n",
      "Train loss: 0.5803711380404669\n",
      "Train loss: 0.580282840541034\n",
      "Train loss: 0.5801560804038453\n",
      "Train loss: 0.5801584407347583\n",
      "Train loss: 0.5801455236483957\n",
      "Train loss: 0.5799655073067622\n",
      "Train loss: 0.579893421892712\n",
      "Train loss: 0.5798404074178344\n",
      "Train loss: 0.5799642730192363\n",
      "Train loss: 0.5799050331892229\n",
      "Train loss: 0.5800486646127676\n",
      "Train loss: 0.5801680477344606\n",
      "Train loss: 0.5799221944453561\n",
      "Train loss: 0.5799332626495676\n",
      "Train loss: 0.5798966064495636\n",
      "Train loss: 0.5799703241973136\n",
      "Train loss: 0.5801764048523784\n",
      "Train loss: 0.5803174212926164\n",
      "Train loss: 0.5802955967207759\n",
      "Train loss: 0.5802274609917231\n",
      "Train loss: 0.580095542416276\n",
      "Train loss: 0.5800342204091503\n",
      "Train loss: 0.5800007174681392\n",
      "Train loss: 0.5798425779065093\n",
      "Train loss: 0.5799690497759604\n",
      "Train loss: 0.5797445662003534\n",
      "Train loss: 0.5796528009119789\n",
      "Train loss: 0.5797677970915941\n",
      "Train loss: 0.5799161565334914\n",
      "Train loss: 0.579988593850068\n",
      "Train loss: 0.5799741406326571\n",
      "Train loss: 0.5800388291156003\n",
      "Train loss: 0.5799017849215042\n",
      "Train loss: 0.5798551575771815\n",
      "Train loss: 0.5798759810586467\n",
      "Train loss: 0.5798830555911532\n",
      "Train loss: 0.5797513529821934\n",
      "Train loss: 0.579770532405487\n",
      "Train loss: 0.5798206191961648\n",
      "Train loss: 0.5797601923932154\n",
      "Train loss: 0.5796667916326834\n",
      "Train loss: 0.5796913840240592\n",
      "Train loss: 0.5796564065148175\n",
      "Train loss: 0.579528201240996\n",
      "Train loss: 0.5796009643176168\n",
      "Train loss: 0.5796770952944053\n",
      "Train loss: 0.5797341749626875\n",
      "Train loss: 0.5797165003050704\n",
      "Train loss: 0.5796678520852259\n",
      "Train loss: 0.5796583743273401\n",
      "Train loss: 0.5796326842130471\n",
      "Train loss: 0.5797752658929372\n",
      "Train loss: 0.5797997260638474\n",
      "Train loss: 0.5796759539563642\n",
      "Train loss: 0.5796625675166401\n",
      "Train loss: 0.5794484464687207\n",
      "Train loss: 0.5793692273312242\n",
      "Train loss: 0.5793450879591047\n",
      "Train loss: 0.5792256121328676\n",
      "Train loss: 0.5790536963764059\n",
      "Train loss: 0.5792181486984466\n",
      "Train loss: 0.579029176244465\n",
      "Train loss: 0.5789570177249678\n",
      "Train loss: 0.5789866962505206\n",
      "Train loss: 0.5789466341053583\n",
      "Train loss: 0.5790320315538326\n",
      "Train loss: 0.5790888763362576\n",
      "Train loss: 0.5791231260434417\n",
      "Train loss: 0.5790123835076205\n",
      "Train loss: 0.5791253060456001\n",
      "Train loss: 0.5790606487534083\n",
      "Train loss: 0.5790551948505853\n",
      "Train loss: 0.5789203475615857\n",
      "Train loss: 0.5788899645018332\n",
      "Train loss: 0.5789837454482982\n",
      "Train loss: 0.5790483892129579\n",
      "Train loss: 0.5792112844456336\n",
      "Train loss: 0.5792042376265071\n",
      "Train loss: 0.579211306394899\n",
      "Train loss: 0.5792116521801277\n",
      "Train loss: 0.5790717554649359\n",
      "Train loss: 0.5790493440844537\n",
      "Train loss: 0.5790803413413692\n",
      "Train loss: 0.5789564690298107\n",
      "Train loss: 0.5789695672557938\n",
      "Train loss: 0.5788551876020833\n",
      "Train loss: 0.578982345800794\n",
      "Train loss: 0.5789863797534845\n",
      "Train loss: 0.5789937903265095\n",
      "Train loss: 0.5789803935828348\n",
      "Train loss: 0.5790108606265608\n",
      "Train loss: 0.5789947771446595\n",
      "Train loss: 0.5789143286143711\n",
      "Train loss: 0.5790050985101661\n",
      "Train loss: 0.5789862538559986\n",
      "Train loss: 0.5789312528823555\n",
      "Train loss: 0.5789433700949093\n",
      "Train loss: 0.578925974442279\n",
      "Train loss: 0.5789084838157926\n",
      "Train loss: 0.5789113987931387\n",
      "Train loss: 0.5789024359993706\n",
      "Train loss: 0.5787945205415618\n",
      "Train loss: 0.5788176524777928\n",
      "Train loss: 0.5787648425333812\n",
      "Train loss: 0.5787445687520376\n",
      "Train loss: 0.578773779953231\n",
      "Train loss: 0.5787568942936445\n",
      "Train loss: 0.5787220683470311\n",
      "Train loss: 0.5788013792154146\n",
      "Train loss: 0.5788711760384174\n",
      "Train loss: 0.5788358288932253\n",
      "Train loss: 0.5788385061023512\n",
      "Train loss: 0.5788415072475974\n",
      "Train loss: 0.578766781438974\n",
      "Train loss: 0.5787407745841012\n",
      "Train loss: 0.5786655990683108\n",
      "Train loss: 0.5787449080784294\n",
      "Train loss: 0.5787424431343411\n",
      "Train loss: 0.5787301766607755\n",
      "Train loss: 0.5786121523070608\n",
      "Train loss: 0.5785304162202177\n",
      "Train loss: 0.5785382994810931\n",
      "Train loss: 0.5784326104648263\n",
      "Train loss: 0.5784785305780864\n",
      "Train loss: 0.5784526792754209\n",
      "Train loss: 0.5785233271054486\n",
      "Train loss: 0.5785250993409217\n",
      "Train loss: 0.5784691183693528\n",
      "Train loss: 0.5784854792555867\n",
      "Train loss: 0.5785206354969932\n",
      "Train loss: 0.5785352780566728\n",
      "Train loss: 0.5786011299290605\n",
      "Train loss: 0.5785836619134634\n",
      "Train loss: 0.5786088597885612\n",
      "Train loss: 0.5785721856953198\n",
      "Train loss: 0.5785644827702207\n",
      "Train loss: 0.5786086313926097\n",
      "Train loss: 0.5786377521970757\n",
      "Train loss: 0.5786455787865685\n",
      "Train loss: 0.5787482091311623\n",
      "Train loss: 0.578682544466997\n",
      "Train loss: 0.5787421369479397\n",
      "Train loss: 0.5788618592969662\n",
      "Train loss: 0.5788244965556498\n",
      "Train loss: 0.5787906927266506\n",
      "Train loss: 0.578869716967969\n",
      "Train loss: 0.5788719323884522\n",
      "Train loss: 0.5789571323056434\n",
      "Train loss: 0.5789204746085194\n",
      "Train loss: 0.5789256088055148\n",
      "Train loss: 0.5788603022924127\n",
      "Train loss: 0.5787980058446704\n",
      "Train loss: 0.578861571569327\n",
      "Train loss: 0.578815948388109\n",
      "Train loss: 0.5787305945020712\n",
      "Train loss: 0.5787151016635229\n",
      "Train loss: 0.5786862975041251\n",
      "Train loss: 0.5786089895567608\n",
      "Train loss: 0.5786474048787171\n",
      "Train loss: 0.5786763758018604\n",
      "Train loss: 0.5786218665635255\n",
      "Train loss: 0.5786965927738268\n",
      "Train loss: 0.5787418900208435\n",
      "Train loss: 0.5786976684246284\n",
      "Train loss: 0.5786930981754045\n",
      "Train loss: 0.5786204106764179\n",
      "Train loss: 0.5786817301978564\n",
      "Train loss: 0.5787333146245208\n",
      "Train loss: 0.5788201709884502\n",
      "Train loss: 0.5787839598140113\n",
      "Train loss: 0.5787559248960814\n",
      "Train loss: 0.5786716588178007\n",
      "Train loss: 0.5787318766082569\n",
      "Train loss: 0.5787492717062196\n",
      "Train loss: 0.5787508982560442\n",
      "Train loss: 0.5787460910987253\n",
      "Train loss: 0.5787925368752582\n",
      "Train loss: 0.5787840890859559\n",
      "Train loss: 0.5787377855894272\n",
      "Train loss: 0.5786984809965854\n",
      "Train loss: 0.578643218288486\n",
      "Train loss: 0.5786882209163878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5787005850341954\n",
      "Train loss: 0.5786624058548401\n",
      "Train loss: 0.5787225950136025\n",
      "Train loss: 0.5786463669675044\n",
      "Train loss: 0.5786616655906318\n",
      "Train loss: 0.5786354843554955\n",
      "Train loss: 0.5786082219006604\n",
      "Train loss: 0.5785021607017016\n",
      "Train loss: 0.5785520084660973\n",
      "Train loss: 0.5786331411713125\n",
      "Train loss: 0.5786501021288184\n",
      "Train loss: 0.578578769670978\n",
      "Train loss: 0.5786100840479438\n",
      "Train loss: 0.5786756447262534\n",
      "Train loss: 0.5786549456655863\n",
      "Train loss: 0.5786820522578885\n",
      "Train loss: 0.5787248476636391\n",
      "Train loss: 0.578747095631686\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6933976337313652\n",
      "Val loss: 0.6221711801158057\n",
      "Val loss: 0.5967265729393277\n",
      "Val loss: 0.5961927191207284\n",
      "Val loss: 0.5949435817698637\n",
      "Val loss: 0.5982052632446947\n",
      "Val loss: 0.591771629803321\n",
      "Val loss: 0.5858668234103765\n",
      "Val loss: 0.5816694531928409\n",
      "Val loss: 0.5814101835902856\n",
      "Val loss: 0.5817026479376687\n",
      "Val loss: 0.5812278526314234\n",
      "Val loss: 0.5802078116685152\n",
      "Val loss: 0.5800646033839904\n",
      "Val loss: 0.5794643373102755\n",
      "Val loss: 0.5780123962631708\n",
      "Val loss: 0.5769124279419581\n",
      "Val loss: 0.5757319160391775\n",
      "Val loss: 0.576358326255007\n",
      "Val loss: 0.574073970016807\n",
      "Val loss: 0.574851877700824\n",
      "Val loss: 0.5731539652435058\n",
      "Val loss: 0.5723486728312677\n",
      "Val loss: 0.5723492871312534\n",
      "Val loss: 0.5722476121398711\n",
      "Val loss: 0.5713021771390309\n",
      "Val loss: 0.5710823613761077\n",
      "Val loss: 0.5698896052597238\n",
      "Val loss: 0.5689441849374108\n",
      "Val loss: 0.5691588450998268\n",
      "Val loss: 0.5687518055949893\n",
      "Val loss: 0.5690302903172355\n",
      "Val loss: 0.5689822913306516\n",
      "Val loss: 0.568221572059146\n",
      "Val loss: 0.5677991408383709\n",
      "Val loss: 0.5676289485486526\n",
      "Val loss: 0.5660681585254876\n",
      "Val loss: 0.5659334801492237\n",
      "Val loss: 0.5665740137247696\n",
      "Val loss: 0.565718604841424\n",
      "Val loss: 0.5659532535309885\n",
      "Val loss: 0.565421536921314\n",
      "Val loss: 0.5646671872829723\n",
      "Val loss: 0.5648122516941262\n",
      "Val loss: 0.5644244801785264\n",
      "Val loss: 0.564828249564858\n",
      "Val loss: 0.5645112216982067\n",
      "Val loss: 0.5644154755640229\n",
      "Val loss: 0.5644128210231906\n",
      "Val loss: 0.5643822218040865\n",
      "Val loss: 0.5638124549717415\n",
      "Val loss: 0.5629994513675513\n",
      "Val loss: 0.5629792553231572\n",
      "Val loss: 0.5625401087188366\n",
      "Val loss: 0.5626524259356687\n",
      "Val loss: 0.5620835598651653\n",
      "Val loss: 0.5621887614399614\n",
      "Val loss: 0.5624266925773819\n",
      "Val loss: 0.5625207857007072\n",
      "Val loss: 0.5624761454835784\n",
      "Val loss: 0.5627172753018769\n",
      "Val loss: 0.5626123628570038\n",
      "Val loss: 0.5621643789634583\n",
      "Val loss: 0.5619916193724426\n",
      "Val loss: 0.5618261874273971\n",
      "Val loss: 0.5618505693496542\n",
      "Val loss: 0.5617085672067311\n",
      "Val loss: 0.5613310724760578\n",
      "Val loss: 0.5624688398526159\n",
      "Val loss: 0.5624012582452386\n",
      "Val loss: 0.5618099292456094\n",
      "Val loss: 0.5617944940219018\n",
      "Val loss: 0.5615074971056246\n",
      "Val loss: 0.5616865084584812\n",
      "Val loss: 0.5616979462737068\n",
      "Val loss: 0.5612318485896631\n",
      "Val loss: 0.5613806064551076\n",
      "Val loss: 0.5611058938625846\n",
      "Val loss: 0.5606922910449469\n",
      "Val loss: 0.5608406016850531\n",
      "Val loss: 0.561153930971528\n",
      "Val loss: 0.5606471499050159\n",
      "Val loss: 0.5608794554564112\n",
      "Val loss: 0.5605235343758986\n",
      "Val loss: 0.5602840599867532\n",
      "Val loss: 0.5603900040502037\n",
      "Val loss: 0.560206867651456\n",
      "Val loss: 0.5601669276358054\n",
      "Val loss: 0.5597005465680415\n",
      "Val loss: 0.5595127612285996\n",
      "Val loss: 0.559662629591736\n",
      "Val loss: 0.5596271366335468\n",
      "Val loss: 0.5597141118527487\n",
      "Val loss: 0.5596754446085582\n",
      "Val loss: 0.5595044926877766\n",
      "Val loss: 0.5596161418145286\n",
      "Val loss: 0.5596085844700002\n",
      "Val loss: 0.5595518888140018\n",
      "Val loss: 0.5595122792942804\n",
      "Val loss: 0.5600055162318962\n",
      "Val loss: 0.5599372736991398\n",
      "Val loss: 0.5597166283191538\n",
      "Val loss: 0.5599932587100374\n",
      "Val loss: 0.559741575766161\n",
      "Val loss: 0.5596317453807547\n",
      "Val loss: 0.5595880947401483\n",
      "Val loss: 0.5593138723337695\n",
      "Val loss: 0.5592593621455672\n",
      "Val loss: 0.5590378640986541\n",
      "Val loss: 0.5591314077811598\n",
      "Val loss: 0.5591346779670096\n",
      "Val loss: 0.5591917594558224\n",
      "Val loss: 0.5592177317391896\n",
      "Val loss: 0.5594706767367982\n",
      "Val loss: 0.5593033852477522\n",
      "Val loss: 0.559480439298923\n",
      "Val loss: 0.5596138504270005\n",
      "Val loss: 0.5596479330281451\n",
      "Val loss: 0.5597711117179306\n",
      "Val loss: 0.5599619636750579\n",
      "Val loss: 0.5602034998453216\n",
      "Val loss: 0.5605088179726123\n",
      "Val loss: 0.5607119831666108\n",
      "Val loss: 0.5607403490770614\n",
      "Val loss: 0.5609396649285768\n",
      "Val loss: 0.5610485114808681\n",
      "Val loss: 0.5609329754621825\n",
      "\n",
      "starting Epoch 5\n",
      "Training...\n",
      "Train loss: 0.6194084631769281\n",
      "Train loss: 0.5959705664561346\n",
      "Train loss: 0.5896353317519366\n",
      "Train loss: 0.5885216642784167\n",
      "Train loss: 0.5895035456527363\n",
      "Train loss: 0.5874688307277295\n",
      "Train loss: 0.585085005640126\n",
      "Train loss: 0.5826842730525155\n",
      "Train loss: 0.5797031696615272\n",
      "Train loss: 0.5768138119323769\n",
      "Train loss: 0.5778742586368839\n",
      "Train loss: 0.5758256263812715\n",
      "Train loss: 0.5760623767799392\n",
      "Train loss: 0.5746004341110107\n",
      "Train loss: 0.5755474440828215\n",
      "Train loss: 0.574883562363801\n",
      "Train loss: 0.5757896665450746\n",
      "Train loss: 0.575744121021547\n",
      "Train loss: 0.5769308434312765\n",
      "Train loss: 0.5775063322450882\n",
      "Train loss: 0.5767000725132754\n",
      "Train loss: 0.5763451198374764\n",
      "Train loss: 0.5769860604230095\n",
      "Train loss: 0.5772950232402267\n",
      "Train loss: 0.5777482992422605\n",
      "Train loss: 0.5774554877841633\n",
      "Train loss: 0.5779006972715452\n",
      "Train loss: 0.5776000056475774\n",
      "Train loss: 0.5771150115446312\n",
      "Train loss: 0.5774103963315388\n",
      "Train loss: 0.57746927769966\n",
      "Train loss: 0.5774114604846973\n",
      "Train loss: 0.5769082049589056\n",
      "Train loss: 0.5768534623207154\n",
      "Train loss: 0.5768867785241641\n",
      "Train loss: 0.5768559665441182\n",
      "Train loss: 0.5769095769270508\n",
      "Train loss: 0.5773022707271953\n",
      "Train loss: 0.5774848571714296\n",
      "Train loss: 0.5779870106595031\n",
      "Train loss: 0.5783791495111835\n",
      "Train loss: 0.5783602868132427\n",
      "Train loss: 0.5788030644512288\n",
      "Train loss: 0.5791982686967166\n",
      "Train loss: 0.5791982580742926\n",
      "Train loss: 0.5788555372591506\n",
      "Train loss: 0.5788562534271846\n",
      "Train loss: 0.5787848473154591\n",
      "Train loss: 0.5790011550664171\n",
      "Train loss: 0.5790367044426419\n",
      "Train loss: 0.5787305494695463\n",
      "Train loss: 0.5787751157485715\n",
      "Train loss: 0.5790320240573235\n",
      "Train loss: 0.5788380704177983\n",
      "Train loss: 0.5789624314561987\n",
      "Train loss: 0.5787582732770356\n",
      "Train loss: 0.5786698134297127\n",
      "Train loss: 0.5787423822620595\n",
      "Train loss: 0.578618764902597\n",
      "Train loss: 0.578817323334919\n",
      "Train loss: 0.578763563017849\n",
      "Train loss: 0.5788547755297968\n",
      "Train loss: 0.5788700292752412\n",
      "Train loss: 0.5789041094149902\n",
      "Train loss: 0.5790061805457496\n",
      "Train loss: 0.5788970020930816\n",
      "Train loss: 0.5790752957479735\n",
      "Train loss: 0.5790765931757168\n",
      "Train loss: 0.5788964120221363\n",
      "Train loss: 0.579037914069914\n",
      "Train loss: 0.5788015478971225\n",
      "Train loss: 0.5787888320474843\n",
      "Train loss: 0.5788390496726555\n",
      "Train loss: 0.5789025594770626\n",
      "Train loss: 0.5789915512529034\n",
      "Train loss: 0.5787257155436603\n",
      "Train loss: 0.5789157314646004\n",
      "Train loss: 0.579175707218049\n",
      "Train loss: 0.579145989892149\n",
      "Train loss: 0.5788374510946089\n",
      "Train loss: 0.5789221579679521\n",
      "Train loss: 0.5788928861658772\n",
      "Train loss: 0.5788576932575704\n",
      "Train loss: 0.5788193912006262\n",
      "Train loss: 0.5789357918792083\n",
      "Train loss: 0.5787576490394455\n",
      "Train loss: 0.5788028269166436\n",
      "Train loss: 0.578945721175062\n",
      "Train loss: 0.5789768833032547\n",
      "Train loss: 0.5790427536716588\n",
      "Train loss: 0.578666352386852\n",
      "Train loss: 0.5783799293412016\n",
      "Train loss: 0.5785717616772511\n",
      "Train loss: 0.5786333096001998\n",
      "Train loss: 0.5786431082114852\n",
      "Train loss: 0.5785584201441263\n",
      "Train loss: 0.5783981689938323\n",
      "Train loss: 0.578297038005894\n",
      "Train loss: 0.5780344685711843\n",
      "Train loss: 0.5779068760719223\n",
      "Train loss: 0.5778046913404403\n",
      "Train loss: 0.5780282014137743\n",
      "Train loss: 0.5779860359537422\n",
      "Train loss: 0.5782285373104256\n",
      "Train loss: 0.5782780498195228\n",
      "Train loss: 0.578388731777246\n",
      "Train loss: 0.5783039636983109\n",
      "Train loss: 0.5781582536356159\n",
      "Train loss: 0.5780184640557244\n",
      "Train loss: 0.5782171059950421\n",
      "Train loss: 0.5782237092537512\n",
      "Train loss: 0.578360162259426\n",
      "Train loss: 0.5783739397883574\n",
      "Train loss: 0.5785005976359747\n",
      "Train loss: 0.5785188940412431\n",
      "Train loss: 0.5783918915174294\n",
      "Train loss: 0.5783909220071672\n",
      "Train loss: 0.5783521373957181\n",
      "Train loss: 0.5783640077981993\n",
      "Train loss: 0.578384858399642\n",
      "Train loss: 0.5783542484944396\n",
      "Train loss: 0.5783676701938094\n",
      "Train loss: 0.578244876127119\n",
      "Train loss: 0.5781279559599963\n",
      "Train loss: 0.5780500515955551\n",
      "Train loss: 0.5780329413478362\n",
      "Train loss: 0.5779188993574362\n",
      "Train loss: 0.5779984294902352\n",
      "Train loss: 0.5780115688018163\n",
      "Train loss: 0.5780138556271069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5779830104820416\n",
      "Train loss: 0.5780696668219232\n",
      "Train loss: 0.578088156495071\n",
      "Train loss: 0.5781105196422586\n",
      "Train loss: 0.5780346663340943\n",
      "Train loss: 0.5780239171098812\n",
      "Train loss: 0.5779791188070312\n",
      "Train loss: 0.5780446885849696\n",
      "Train loss: 0.5780504061275783\n",
      "Train loss: 0.5781747310256992\n",
      "Train loss: 0.5783835053761067\n",
      "Train loss: 0.5783232718800608\n",
      "Train loss: 0.5785846403529903\n",
      "Train loss: 0.5786589610883037\n",
      "Train loss: 0.5786043834423106\n",
      "Train loss: 0.5786831556586788\n",
      "Train loss: 0.5787387106158207\n",
      "Train loss: 0.5786770809820755\n",
      "Train loss: 0.5787723102838491\n",
      "Train loss: 0.5787433133637281\n",
      "Train loss: 0.578707893742122\n",
      "Train loss: 0.578782440800932\n",
      "Train loss: 0.5786557011781233\n",
      "Train loss: 0.5787739740365819\n",
      "Train loss: 0.5787124082075853\n",
      "Train loss: 0.5788234707234418\n",
      "Train loss: 0.5788130653857729\n",
      "Train loss: 0.5788854278692455\n",
      "Train loss: 0.5786936435638714\n",
      "Train loss: 0.5788003918099083\n",
      "Train loss: 0.5787697882432307\n",
      "Train loss: 0.5787071161025025\n",
      "Train loss: 0.5786566460523989\n",
      "Train loss: 0.5786414221224533\n",
      "Train loss: 0.5786150848641473\n",
      "Train loss: 0.5786902845516446\n",
      "Train loss: 0.578718289023354\n",
      "Train loss: 0.5787592153440596\n",
      "Train loss: 0.5787587930602976\n",
      "Train loss: 0.5787377031840588\n",
      "Train loss: 0.5787632090371058\n",
      "Train loss: 0.5787396315884957\n",
      "Train loss: 0.578788460705383\n",
      "Train loss: 0.5787353975894842\n",
      "Train loss: 0.578758562052376\n",
      "Train loss: 0.5787741266451427\n",
      "Train loss: 0.5787043382418159\n",
      "Train loss: 0.5786614387130496\n",
      "Train loss: 0.578548381892487\n",
      "Train loss: 0.5785127975487583\n",
      "Train loss: 0.5785472256006144\n",
      "Train loss: 0.5786009668386648\n",
      "Train loss: 0.5785783844653555\n",
      "Train loss: 0.5785633504601334\n",
      "Train loss: 0.5786211258754694\n",
      "Train loss: 0.5785533890957536\n",
      "Train loss: 0.5785457271007524\n",
      "Train loss: 0.5785431568611172\n",
      "Train loss: 0.5785814772036442\n",
      "Train loss: 0.578565171790079\n",
      "Train loss: 0.5785632407705832\n",
      "Train loss: 0.5785186310769618\n",
      "Train loss: 0.5786263674737011\n",
      "Train loss: 0.5786583349550468\n",
      "Train loss: 0.5786445762873246\n",
      "Train loss: 0.5787276812097375\n",
      "Train loss: 0.5787209437700902\n",
      "Train loss: 0.5788120906832965\n",
      "Train loss: 0.5788789472857981\n",
      "Train loss: 0.5788271910221943\n",
      "Train loss: 0.5788964700805633\n",
      "Train loss: 0.5788666493572732\n",
      "Train loss: 0.5789314876066167\n",
      "Train loss: 0.5789616484579487\n",
      "Train loss: 0.5789395494704189\n",
      "Train loss: 0.5789494835352197\n",
      "Train loss: 0.579004451189677\n",
      "Train loss: 0.5789436961085959\n",
      "Train loss: 0.5789872871100774\n",
      "Train loss: 0.5789422171687648\n",
      "Train loss: 0.5788498734178971\n",
      "Train loss: 0.5788733699491041\n",
      "Train loss: 0.5788299760073389\n",
      "Train loss: 0.578858381309195\n",
      "Train loss: 0.5789743293008296\n",
      "Train loss: 0.5790384397804255\n",
      "Train loss: 0.5790375108116095\n",
      "Train loss: 0.5790985601091636\n",
      "Train loss: 0.5790111813528349\n",
      "Train loss: 0.5790156824706928\n",
      "Train loss: 0.5790855740969417\n",
      "Train loss: 0.5790699366098268\n",
      "Train loss: 0.5789816500186706\n",
      "Train loss: 0.5790224246785534\n",
      "Train loss: 0.5790568235967657\n",
      "Train loss: 0.5791295269773343\n",
      "Train loss: 0.5791190117893042\n",
      "Train loss: 0.5790773875019377\n",
      "Train loss: 0.5791577726739232\n",
      "Train loss: 0.579126083382992\n",
      "Train loss: 0.5790687771774159\n",
      "Train loss: 0.5789505124837321\n",
      "Train loss: 0.5789605635868988\n",
      "Train loss: 0.5789769737479472\n",
      "Train loss: 0.5789569363134367\n",
      "Train loss: 0.5789235500517375\n",
      "Train loss: 0.5789654993588322\n",
      "Train loss: 0.5790447424279214\n",
      "Train loss: 0.5790984210084487\n",
      "Train loss: 0.579100618923324\n",
      "Train loss: 0.5791344007260326\n",
      "Train loss: 0.5790760334943734\n",
      "Train loss: 0.5789967287246718\n",
      "Train loss: 0.5790112762757563\n",
      "Train loss: 0.5790186132197722\n",
      "Train loss: 0.5790229124384553\n",
      "Train loss: 0.5790151351435654\n",
      "Train loss: 0.578979599419369\n",
      "Train loss: 0.5790896473531384\n",
      "Train loss: 0.5790506616118527\n",
      "Train loss: 0.5790253152147092\n",
      "Train loss: 0.5790026380763117\n",
      "Train loss: 0.5789717139227396\n",
      "Train loss: 0.5789345991813332\n",
      "Train loss: 0.5788414547795945\n",
      "Train loss: 0.5788740983498843\n",
      "Train loss: 0.5788315605168493\n",
      "Train loss: 0.5787871097970735\n",
      "Train loss: 0.5787618147875725\n",
      "Train loss: 0.5787686606420189\n",
      "Train loss: 0.5787909611028875\n",
      "Train loss: 0.5787538828957193\n",
      "Train loss: 0.5787851055848505\n",
      "Train loss: 0.5788056042061586\n",
      "Train loss: 0.5788401375909777\n",
      "Train loss: 0.578862555337638\n",
      "Train loss: 0.5788477286180098\n",
      "Train loss: 0.5788229827528059\n",
      "Train loss: 0.5788146319066222\n",
      "Train loss: 0.5788246433444323\n",
      "Train loss: 0.5787607578361006\n",
      "Train loss: 0.5787147178788421\n",
      "Train loss: 0.5787300208884076\n",
      "Train loss: 0.5788071427897481\n",
      "Train loss: 0.5788977247874376\n",
      "Train loss: 0.5789220079031732\n",
      "Train loss: 0.5788169437355245\n",
      "Train loss: 0.578790009246925\n",
      "Train loss: 0.578823097153021\n",
      "Train loss: 0.5787820412039991\n",
      "Train loss: 0.5787061521444357\n",
      "Train loss: 0.5787051012328297\n",
      "Train loss: 0.5787385060743385\n",
      "Train loss: 0.5787348768507056\n",
      "Train loss: 0.578748572116401\n",
      "Train loss: 0.5787327781092935\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.697371631860733\n",
      "Val loss: 0.624263174004025\n",
      "Val loss: 0.5963208419936044\n",
      "Val loss: 0.5960419083896437\n",
      "Val loss: 0.5958581765492758\n",
      "Val loss: 0.5980725596691\n",
      "Val loss: 0.5911484465879553\n",
      "Val loss: 0.586123179166745\n",
      "Val loss: 0.5808349752968008\n",
      "Val loss: 0.5807729217470908\n",
      "Val loss: 0.581182039446301\n",
      "Val loss: 0.5801764908483473\n",
      "Val loss: 0.5799117218703032\n",
      "Val loss: 0.5795042238373688\n",
      "Val loss: 0.5788882543911805\n",
      "Val loss: 0.5772185446340826\n",
      "Val loss: 0.5761293779526439\n",
      "Val loss: 0.5752882234166178\n",
      "Val loss: 0.5764363623679952\n",
      "Val loss: 0.5740243906926628\n",
      "Val loss: 0.5746449988621932\n",
      "Val loss: 0.573130459413616\n",
      "Val loss: 0.5719703530010424\n",
      "Val loss: 0.5714961896423533\n",
      "Val loss: 0.5713924325762256\n",
      "Val loss: 0.5702448766822963\n",
      "Val loss: 0.5699854926831687\n",
      "Val loss: 0.568787676610535\n",
      "Val loss: 0.5682656220677826\n",
      "Val loss: 0.568462334023226\n",
      "Val loss: 0.5681521998984473\n",
      "Val loss: 0.5681746120347917\n",
      "Val loss: 0.5679694564967621\n",
      "Val loss: 0.5673147041416733\n",
      "Val loss: 0.5670269924676281\n",
      "Val loss: 0.5670978208826907\n",
      "Val loss: 0.5654838499815568\n",
      "Val loss: 0.5654018757835267\n",
      "Val loss: 0.5661338699847153\n",
      "Val loss: 0.5654284825277089\n",
      "Val loss: 0.5657183603913176\n",
      "Val loss: 0.5652076584870735\n",
      "Val loss: 0.5645826286244615\n",
      "Val loss: 0.5646396516120598\n",
      "Val loss: 0.5640983717249972\n",
      "Val loss: 0.5643404948659339\n",
      "Val loss: 0.5641150168883495\n",
      "Val loss: 0.5642730481455017\n",
      "Val loss: 0.5643958422492762\n",
      "Val loss: 0.564520061734211\n",
      "Val loss: 0.5639348236594613\n",
      "Val loss: 0.5632642581195905\n",
      "Val loss: 0.5631423213265159\n",
      "Val loss: 0.5627298935638484\n",
      "Val loss: 0.5626903914839682\n",
      "Val loss: 0.5620919366677603\n",
      "Val loss: 0.5622961822320038\n",
      "Val loss: 0.562377016952706\n",
      "Val loss: 0.5623895319140687\n",
      "Val loss: 0.5622548250649685\n",
      "Val loss: 0.5624999873536197\n",
      "Val loss: 0.5624914264794693\n",
      "Val loss: 0.5620284825563431\n",
      "Val loss: 0.5617244043694021\n",
      "Val loss: 0.5616258796718385\n",
      "Val loss: 0.561698366443437\n",
      "Val loss: 0.5615076104145564\n",
      "Val loss: 0.5610392003108619\n",
      "Val loss: 0.5620084963738918\n",
      "Val loss: 0.5618740981151177\n",
      "Val loss: 0.5612381234512491\n",
      "Val loss: 0.5611894503443354\n",
      "Val loss: 0.5609000893412056\n",
      "Val loss: 0.5610938703787682\n",
      "Val loss: 0.5611282880293494\n",
      "Val loss: 0.5608075169900476\n",
      "Val loss: 0.5610350556671619\n",
      "Val loss: 0.560812015689737\n",
      "Val loss: 0.5604158086976424\n",
      "Val loss: 0.5605919388750741\n",
      "Val loss: 0.560836956126265\n",
      "Val loss: 0.5603754384884916\n",
      "Val loss: 0.5605085496453271\n",
      "Val loss: 0.5601601102044873\n",
      "Val loss: 0.5599016084001874\n",
      "Val loss: 0.5600104427143132\n",
      "Val loss: 0.5598276720343647\n",
      "Val loss: 0.5598498275991453\n",
      "Val loss: 0.5593582879047136\n",
      "Val loss: 0.5591547985650384\n",
      "Val loss: 0.5593409638047744\n",
      "Val loss: 0.5593712819427706\n",
      "Val loss: 0.5594536820105438\n",
      "Val loss: 0.5594174883513054\n",
      "Val loss: 0.5591908782594818\n",
      "Val loss: 0.5594615861617946\n",
      "Val loss: 0.5595739281867161\n",
      "Val loss: 0.5595687167044797\n",
      "Val loss: 0.5596031160489751\n",
      "Val loss: 0.5600498305532879\n",
      "Val loss: 0.5599462210185944\n",
      "Val loss: 0.559784995315117\n",
      "Val loss: 0.5601460717995344\n",
      "Val loss: 0.5599352369992949\n",
      "Val loss: 0.5597874173567495\n",
      "Val loss: 0.5596540937576943\n",
      "Val loss: 0.5593477076805486\n",
      "Val loss: 0.5593354814791281\n",
      "Val loss: 0.559120778213529\n",
      "Val loss: 0.5592449089217055\n",
      "Val loss: 0.5592805652842194\n",
      "Val loss: 0.5593384144984332\n",
      "Val loss: 0.5593457617763932\n",
      "Val loss: 0.5595306619504214\n",
      "Val loss: 0.5593751715658434\n",
      "Val loss: 0.559514271883561\n",
      "Val loss: 0.5596305971888647\n",
      "Val loss: 0.5596617367656203\n",
      "Val loss: 0.5597917309614143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5600477074939938\n",
      "Val loss: 0.5602724735705268\n",
      "Val loss: 0.5605645983285701\n",
      "Val loss: 0.5606731281218389\n",
      "Val loss: 0.5606975213583913\n",
      "Val loss: 0.5608376871125821\n",
      "Val loss: 0.5609504369180798\n",
      "Val loss: 0.5608425003120951\n",
      "\n",
      "starting Epoch 6\n",
      "Training...\n",
      "Train loss: 0.6148779800063685\n",
      "Train loss: 0.5986858782095786\n",
      "Train loss: 0.5912192832615416\n",
      "Train loss: 0.5924261789533156\n",
      "Train loss: 0.5931346118450165\n",
      "Train loss: 0.5909308337864756\n",
      "Train loss: 0.5880927758679974\n",
      "Train loss: 0.5881014113906045\n",
      "Train loss: 0.5869948272931509\n",
      "Train loss: 0.5853395393146342\n",
      "Train loss: 0.5874925062536649\n",
      "Train loss: 0.5864588573387975\n",
      "Train loss: 0.5866985898680669\n",
      "Train loss: 0.5869029726178843\n",
      "Train loss: 0.5865717021916622\n",
      "Train loss: 0.586237253813908\n",
      "Train loss: 0.5849816931032501\n",
      "Train loss: 0.584697397306437\n",
      "Train loss: 0.5846003869592672\n",
      "Train loss: 0.583855322429112\n",
      "Train loss: 0.5832993331274156\n",
      "Train loss: 0.5820434175618417\n",
      "Train loss: 0.5824853712307342\n",
      "Train loss: 0.5824527652129251\n",
      "Train loss: 0.5825190186978342\n",
      "Train loss: 0.5818393567508815\n",
      "Train loss: 0.5808539644906605\n",
      "Train loss: 0.5799923252334321\n",
      "Train loss: 0.5801347401475659\n",
      "Train loss: 0.5802140098083796\n",
      "Train loss: 0.5806962826043993\n",
      "Train loss: 0.5808316159043141\n",
      "Train loss: 0.5806923230067008\n",
      "Train loss: 0.5802222807207233\n",
      "Train loss: 0.5803756746270284\n",
      "Train loss: 0.5796331534050768\n",
      "Train loss: 0.5795932428278684\n",
      "Train loss: 0.5797235774114355\n",
      "Train loss: 0.5798571465563254\n",
      "Train loss: 0.5797428120212054\n",
      "Train loss: 0.5801816132187989\n",
      "Train loss: 0.5801264112981767\n",
      "Train loss: 0.5797548846399132\n",
      "Train loss: 0.5799132863544902\n",
      "Train loss: 0.5799861648125166\n",
      "Train loss: 0.5799384029243664\n",
      "Train loss: 0.5799672098015063\n",
      "Train loss: 0.5798196614845205\n",
      "Train loss: 0.5797661454307412\n",
      "Train loss: 0.5794775608900908\n",
      "Train loss: 0.5795573370204005\n",
      "Train loss: 0.5793493279364387\n",
      "Train loss: 0.579425218591609\n",
      "Train loss: 0.5796470873824306\n",
      "Train loss: 0.5801417529582977\n",
      "Train loss: 0.5799841721799355\n",
      "Train loss: 0.5799841490673119\n",
      "Train loss: 0.5801693892766938\n",
      "Train loss: 0.5802281114941841\n",
      "Train loss: 0.5803507557752433\n",
      "Train loss: 0.580309971445794\n",
      "Train loss: 0.5803842686471485\n",
      "Train loss: 0.580287565850757\n",
      "Train loss: 0.5802934860651897\n",
      "Train loss: 0.5801477684159753\n",
      "Train loss: 0.5803038325609811\n",
      "Train loss: 0.5803765206082353\n",
      "Train loss: 0.5805923031085198\n",
      "Train loss: 0.5804608765289868\n",
      "Train loss: 0.5802683994717219\n",
      "Train loss: 0.5803923739817379\n",
      "Train loss: 0.5805484896902412\n",
      "Train loss: 0.5807611773568037\n",
      "Train loss: 0.5807054922579107\n",
      "Train loss: 0.5805402322758667\n",
      "Train loss: 0.5810220720855871\n",
      "Train loss: 0.5807416982237126\n",
      "Train loss: 0.5809874216702751\n",
      "Train loss: 0.5811889536815629\n",
      "Train loss: 0.5808614344243485\n",
      "Train loss: 0.580999150421238\n",
      "Train loss: 0.5809041417503299\n",
      "Train loss: 0.580856828516402\n",
      "Train loss: 0.580612604393712\n",
      "Train loss: 0.5809095498742322\n",
      "Train loss: 0.5808423448434189\n",
      "Train loss: 0.5807891773416641\n",
      "Train loss: 0.5806521958522731\n",
      "Train loss: 0.5805356440717012\n",
      "Train loss: 0.5807669871114505\n",
      "Train loss: 0.580981682312233\n",
      "Train loss: 0.5806639943516987\n",
      "Train loss: 0.5807476084100232\n",
      "Train loss: 0.5804902735607874\n",
      "Train loss: 0.5805793634648697\n",
      "Train loss: 0.5806250616154862\n",
      "Train loss: 0.5806287603165576\n",
      "Train loss: 0.5802619010590363\n",
      "Train loss: 0.5801967398243278\n",
      "Train loss: 0.580162287101321\n",
      "Train loss: 0.5799570567038698\n",
      "Train loss: 0.5798459979264744\n",
      "Train loss: 0.5799938674659738\n",
      "Train loss: 0.5799996817467475\n",
      "Train loss: 0.5799723135226678\n",
      "Train loss: 0.5799724862169353\n",
      "Train loss: 0.5799688646842409\n",
      "Train loss: 0.5799454866514652\n",
      "Train loss: 0.5800580397255106\n",
      "Train loss: 0.5800861548081806\n",
      "Train loss: 0.5800888143100412\n",
      "Train loss: 0.5801063108492132\n",
      "Train loss: 0.5801186951692783\n",
      "Train loss: 0.5801905319585042\n",
      "Train loss: 0.5801821438163817\n",
      "Train loss: 0.5802094448532921\n",
      "Train loss: 0.5802739978397234\n",
      "Train loss: 0.5801417735634642\n",
      "Train loss: 0.5801352248215886\n",
      "Train loss: 0.5800757219688054\n",
      "Train loss: 0.579976415969262\n",
      "Train loss: 0.5799732239288402\n",
      "Train loss: 0.5799039255171687\n",
      "Train loss: 0.5799257214774332\n",
      "Train loss: 0.5800721551142201\n",
      "Train loss: 0.5801210845922279\n",
      "Train loss: 0.5800394728081393\n",
      "Train loss: 0.5802327760172431\n",
      "Train loss: 0.5802542965002604\n",
      "Train loss: 0.5803324701053081\n",
      "Train loss: 0.5802637076004629\n",
      "Train loss: 0.5800907615799487\n",
      "Train loss: 0.5801668506047025\n",
      "Train loss: 0.5801118439193447\n",
      "Train loss: 0.5800891478400002\n",
      "Train loss: 0.5801722401860621\n",
      "Train loss: 0.5802275190805162\n",
      "Train loss: 0.5803371949898415\n",
      "Train loss: 0.5802752805318109\n",
      "Train loss: 0.5800749849276527\n",
      "Train loss: 0.5800308847774128\n",
      "Train loss: 0.5799879116170547\n",
      "Train loss: 0.5800639678440381\n",
      "Train loss: 0.5800140257323108\n",
      "Train loss: 0.5799124529440677\n",
      "Train loss: 0.5800506836770127\n",
      "Train loss: 0.5800356468342324\n",
      "Train loss: 0.5801200792571104\n",
      "Train loss: 0.5800947967032611\n",
      "Train loss: 0.5800431218792018\n",
      "Train loss: 0.5799650343375002\n",
      "Train loss: 0.5798537878073843\n",
      "Train loss: 0.5799314816156296\n",
      "Train loss: 0.5798193194739188\n",
      "Train loss: 0.579878683098288\n",
      "Train loss: 0.5798373503748443\n",
      "Train loss: 0.579806636599987\n",
      "Train loss: 0.5799799602633225\n",
      "Train loss: 0.5799813042949977\n",
      "Train loss: 0.5800486323366913\n",
      "Train loss: 0.5800312273007588\n",
      "Train loss: 0.5799452080775205\n",
      "Train loss: 0.5797812746653128\n",
      "Train loss: 0.5798293068918668\n",
      "Train loss: 0.5798304454598798\n",
      "Train loss: 0.5800177004187177\n",
      "Train loss: 0.5800712098079943\n",
      "Train loss: 0.5801073820614396\n",
      "Train loss: 0.580029526831801\n",
      "Train loss: 0.5799329785537495\n",
      "Train loss: 0.5798983643821333\n",
      "Train loss: 0.5798724556114827\n",
      "Train loss: 0.5799825962274259\n",
      "Train loss: 0.5800625666286384\n",
      "Train loss: 0.5801204948058705\n",
      "Train loss: 0.5801750253330208\n",
      "Train loss: 0.5801303008917059\n",
      "Train loss: 0.5801527226497096\n",
      "Train loss: 0.5802176087786612\n",
      "Train loss: 0.580157358875272\n",
      "Train loss: 0.5800985849654684\n",
      "Train loss: 0.5800614144799473\n",
      "Train loss: 0.5800911944871654\n",
      "Train loss: 0.5801036934484506\n",
      "Train loss: 0.5800827046477108\n",
      "Train loss: 0.5802005254379297\n",
      "Train loss: 0.5802091971920531\n",
      "Train loss: 0.5802714938050605\n",
      "Train loss: 0.5803109042920206\n",
      "Train loss: 0.5802962251574343\n",
      "Train loss: 0.5803518092157829\n",
      "Train loss: 0.5803206137658781\n",
      "Train loss: 0.5802727176152106\n",
      "Train loss: 0.5801894061726435\n",
      "Train loss: 0.5802479856944934\n",
      "Train loss: 0.5802399836638412\n",
      "Train loss: 0.5802664366900906\n",
      "Train loss: 0.5802507409545983\n",
      "Train loss: 0.580251380715067\n",
      "Train loss: 0.580226425618999\n",
      "Train loss: 0.5802576245879557\n",
      "Train loss: 0.5802487303240225\n",
      "Train loss: 0.5802706210197384\n",
      "Train loss: 0.5802130757096527\n",
      "Train loss: 0.5800971774305765\n",
      "Train loss: 0.5800670634762527\n",
      "Train loss: 0.5800401522546438\n",
      "Train loss: 0.5799146454748152\n",
      "Train loss: 0.579923365569109\n",
      "Train loss: 0.5799052886634704\n",
      "Train loss: 0.5799394825194848\n",
      "Train loss: 0.579925399810589\n",
      "Train loss: 0.5799002448391204\n",
      "Train loss: 0.5798679020575314\n",
      "Train loss: 0.5799103337308644\n",
      "Train loss: 0.5799106912539496\n",
      "Train loss: 0.5798268816896396\n",
      "Train loss: 0.5798883922509191\n",
      "Train loss: 0.5799003142108926\n",
      "Train loss: 0.5798951170815205\n",
      "Train loss: 0.5798781282304504\n",
      "Train loss: 0.5798251531123578\n",
      "Train loss: 0.5797662234180683\n",
      "Train loss: 0.5796879722846355\n",
      "Train loss: 0.5797850131577824\n",
      "Train loss: 0.5797584123264823\n",
      "Train loss: 0.5797034493756468\n",
      "Train loss: 0.5796817770964432\n",
      "Train loss: 0.5797272947879045\n",
      "Train loss: 0.5797732808092776\n",
      "Train loss: 0.5798842388524408\n",
      "Train loss: 0.5798821529326549\n",
      "Train loss: 0.5798806663771304\n",
      "Train loss: 0.579903060578719\n",
      "Train loss: 0.5799042742590773\n",
      "Train loss: 0.5798767117543048\n",
      "Train loss: 0.5798130342856719\n",
      "Train loss: 0.5797742136726111\n",
      "Train loss: 0.5798171466116278\n",
      "Train loss: 0.5798109022806922\n",
      "Train loss: 0.5798096009484711\n",
      "Train loss: 0.579886365901293\n",
      "Train loss: 0.5799236880098996\n",
      "Train loss: 0.5798503528867351\n",
      "Train loss: 0.5798932904186335\n",
      "Train loss: 0.5798402397788669\n",
      "Train loss: 0.579793202349666\n",
      "Train loss: 0.5797263618846362\n",
      "Train loss: 0.5796921616171373\n",
      "Train loss: 0.5797310572167496\n",
      "Train loss: 0.5796989935355064\n",
      "Train loss: 0.5796367541786507\n",
      "Train loss: 0.5795997558881417\n",
      "Train loss: 0.5796635104378135\n",
      "Train loss: 0.5797241546811251\n",
      "Train loss: 0.5797396552623262\n",
      "Train loss: 0.5797535171012838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5797495737131725\n",
      "Train loss: 0.5796186915176397\n",
      "Train loss: 0.5796321939277888\n",
      "Train loss: 0.579617648134745\n",
      "Train loss: 0.5796034938905912\n",
      "Train loss: 0.5794939501166775\n",
      "Train loss: 0.5794683756092593\n",
      "Train loss: 0.5794628502276961\n",
      "Train loss: 0.5795246845384047\n",
      "Train loss: 0.5795518842768772\n",
      "Train loss: 0.5795774990374705\n",
      "Train loss: 0.5795445023449098\n",
      "Train loss: 0.5794593274295274\n",
      "Train loss: 0.5794628025597148\n",
      "Train loss: 0.5794460760317517\n",
      "Train loss: 0.5794881351768457\n",
      "Train loss: 0.5794517482082042\n",
      "Train loss: 0.5794539386672439\n",
      "Train loss: 0.5794329207743568\n",
      "Train loss: 0.5794144520789283\n",
      "Train loss: 0.5793877219066579\n",
      "Train loss: 0.579422496428817\n",
      "Train loss: 0.5794755262410051\n",
      "Train loss: 0.5794371821517982\n",
      "Train loss: 0.5793946555350389\n",
      "Train loss: 0.5793526390349197\n",
      "Train loss: 0.5794286250669282\n",
      "Train loss: 0.5794128727172421\n",
      "Train loss: 0.5795143098029917\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6976910829544067\n",
      "Val loss: 0.62647854619556\n",
      "Val loss: 0.5987283630030495\n",
      "Val loss: 0.5982063569520649\n",
      "Val loss: 0.597625620663166\n",
      "Val loss: 0.5985273410534037\n",
      "Val loss: 0.5924606393365299\n",
      "Val loss: 0.5875339523339883\n",
      "Val loss: 0.5829408358443867\n",
      "Val loss: 0.5825963543385876\n",
      "Val loss: 0.5834767917792002\n",
      "Val loss: 0.5824459669953685\n",
      "Val loss: 0.5821929471567273\n",
      "Val loss: 0.5816831156827401\n",
      "Val loss: 0.5809942097277254\n",
      "Val loss: 0.5796542062035089\n",
      "Val loss: 0.5782886136855397\n",
      "Val loss: 0.5775413278783306\n",
      "Val loss: 0.5787647131909716\n",
      "Val loss: 0.5765173886761521\n",
      "Val loss: 0.5769294190865296\n",
      "Val loss: 0.5755603400939101\n",
      "Val loss: 0.5745842263363955\n",
      "Val loss: 0.5739533144886754\n",
      "Val loss: 0.573763265725105\n",
      "Val loss: 0.5726917118065117\n",
      "Val loss: 0.5725525297335724\n",
      "Val loss: 0.5714833046034943\n",
      "Val loss: 0.5709580646620857\n",
      "Val loss: 0.5710417844305102\n",
      "Val loss: 0.5708427974930057\n",
      "Val loss: 0.5707207303377068\n",
      "Val loss: 0.5704613186237288\n",
      "Val loss: 0.5698974732111192\n",
      "Val loss: 0.5696728750206959\n",
      "Val loss: 0.5695283819177297\n",
      "Val loss: 0.5680698169314343\n",
      "Val loss: 0.5680322142505141\n",
      "Val loss: 0.5686657471755117\n",
      "Val loss: 0.5680631497996537\n",
      "Val loss: 0.5683199129268235\n",
      "Val loss: 0.5678107108225663\n",
      "Val loss: 0.567388068571269\n",
      "Val loss: 0.5674296094946665\n",
      "Val loss: 0.5667903138590711\n",
      "Val loss: 0.5670452609853452\n",
      "Val loss: 0.566870552352351\n",
      "Val loss: 0.5669353053160792\n",
      "Val loss: 0.5670943651043001\n",
      "Val loss: 0.5673058305878237\n",
      "Val loss: 0.5667631689488418\n",
      "Val loss: 0.5662451481957234\n",
      "Val loss: 0.5660986194782185\n",
      "Val loss: 0.5657458576792678\n",
      "Val loss: 0.5657276648239498\n",
      "Val loss: 0.5652156412387834\n",
      "Val loss: 0.5653609058386843\n",
      "Val loss: 0.565381200874553\n",
      "Val loss: 0.5654335136518998\n",
      "Val loss: 0.5653269339763999\n",
      "Val loss: 0.5655518108488697\n",
      "Val loss: 0.5655473550739412\n",
      "Val loss: 0.5651431043816221\n",
      "Val loss: 0.5648302946905357\n",
      "Val loss: 0.5647411584670161\n",
      "Val loss: 0.564855941976095\n",
      "Val loss: 0.5647666687201597\n",
      "Val loss: 0.5642927920220524\n",
      "Val loss: 0.565029383572035\n",
      "Val loss: 0.5648519534265415\n",
      "Val loss: 0.564283133394974\n",
      "Val loss: 0.5642680942514149\n",
      "Val loss: 0.564059935100786\n",
      "Val loss: 0.5642995060620916\n",
      "Val loss: 0.5643498041731788\n",
      "Val loss: 0.5641476052732141\n",
      "Val loss: 0.5643292826910814\n",
      "Val loss: 0.5641282712585822\n",
      "Val loss: 0.5637506828997946\n",
      "Val loss: 0.5639681768297851\n",
      "Val loss: 0.5642362101243275\n",
      "Val loss: 0.5638387876501293\n",
      "Val loss: 0.5639398611974025\n",
      "Val loss: 0.5636475224199045\n",
      "Val loss: 0.5634195790538248\n",
      "Val loss: 0.5635486282668747\n",
      "Val loss: 0.5633921516106425\n",
      "Val loss: 0.5634255755463602\n",
      "Val loss: 0.562950604737879\n",
      "Val loss: 0.5627781672440553\n",
      "Val loss: 0.5629503082992747\n",
      "Val loss: 0.5629766662006545\n",
      "Val loss: 0.5630669846231567\n",
      "Val loss: 0.5630563903592034\n",
      "Val loss: 0.5628427399738931\n",
      "Val loss: 0.5630875013939772\n",
      "Val loss: 0.5631801490325573\n",
      "Val loss: 0.5631984216913367\n",
      "Val loss: 0.5632415194863732\n",
      "Val loss: 0.5636517953179881\n",
      "Val loss: 0.563520577869245\n",
      "Val loss: 0.5633841312118972\n",
      "Val loss: 0.5637019585658604\n",
      "Val loss: 0.5634910627941175\n",
      "Val loss: 0.5633676938769472\n",
      "Val loss: 0.5632589419419914\n",
      "Val loss: 0.5629731160536241\n",
      "Val loss: 0.5629399249084805\n",
      "Val loss: 0.5627574042471892\n",
      "Val loss: 0.5629048326302096\n",
      "Val loss: 0.5629483130541949\n",
      "Val loss: 0.5630121858580595\n",
      "Val loss: 0.5630112509672524\n",
      "Val loss: 0.5631682339054\n",
      "Val loss: 0.563038835386366\n",
      "Val loss: 0.5631532365896953\n",
      "Val loss: 0.5632558434895457\n",
      "Val loss: 0.5632694521966328\n",
      "Val loss: 0.5633810918640207\n",
      "Val loss: 0.5636144800357309\n",
      "Val loss: 0.5637967355105261\n",
      "Val loss: 0.5640408260188079\n",
      "Val loss: 0.5640924654003076\n",
      "Val loss: 0.5640922110281006\n",
      "Val loss: 0.5642043508302707\n",
      "Val loss: 0.5643508350432204\n",
      "Val loss: 0.5642310584945257\n",
      "\n",
      "starting Epoch 7\n",
      "Training...\n",
      "Train loss: 0.607398039416263\n",
      "Train loss: 0.6025414115343338\n",
      "Train loss: 0.5933956236152326\n",
      "Train loss: 0.5966258494159843\n",
      "Train loss: 0.5957989180930937\n",
      "Train loss: 0.5938692127957064\n",
      "Train loss: 0.5917255063708737\n",
      "Train loss: 0.5880177995693758\n",
      "Train loss: 0.586834508946488\n",
      "Train loss: 0.5856149930450785\n",
      "Train loss: 0.5830370144212627\n",
      "Train loss: 0.5851528284190589\n",
      "Train loss: 0.5849485064795579\n",
      "Train loss: 0.5834972733451474\n",
      "Train loss: 0.5827524278275544\n",
      "Train loss: 0.5831397521458449\n",
      "Train loss: 0.5829818108967976\n",
      "Train loss: 0.5830657594217231\n",
      "Train loss: 0.5826940784982767\n",
      "Train loss: 0.5827879431403071\n",
      "Train loss: 0.5823797725123266\n",
      "Train loss: 0.5820735708591879\n",
      "Train loss: 0.5818807293776593\n",
      "Train loss: 0.5816188868998485\n",
      "Train loss: 0.5812299165553703\n",
      "Train loss: 0.5815338062872547\n",
      "Train loss: 0.5822397709335158\n",
      "Train loss: 0.5819053035196123\n",
      "Train loss: 0.5827144341662346\n",
      "Train loss: 0.5831767259436179\n",
      "Train loss: 0.5833262450186617\n",
      "Train loss: 0.5829652132749185\n",
      "Train loss: 0.5828027964000094\n",
      "Train loss: 0.5825545338362411\n",
      "Train loss: 0.5820078076381711\n",
      "Train loss: 0.581104940847495\n",
      "Train loss: 0.5799853686792763\n",
      "Train loss: 0.5802111923694611\n",
      "Train loss: 0.5799254271965737\n",
      "Train loss: 0.580405223168181\n",
      "Train loss: 0.5802844381812728\n",
      "Train loss: 0.5809336598425286\n",
      "Train loss: 0.5810313809389841\n",
      "Train loss: 0.5813615350254024\n",
      "Train loss: 0.5813435597533777\n",
      "Train loss: 0.5810308930858304\n",
      "Train loss: 0.5814074177267199\n",
      "Train loss: 0.5812938035901819\n",
      "Train loss: 0.5809676384475306\n",
      "Train loss: 0.5808930736225288\n",
      "Train loss: 0.5806818814752615\n",
      "Train loss: 0.5804750870006147\n",
      "Train loss: 0.5805106300920445\n",
      "Train loss: 0.5806931247857017\n",
      "Train loss: 0.5807047268550324\n",
      "Train loss: 0.5804420945844658\n",
      "Train loss: 0.5806369286707559\n",
      "Train loss: 0.5805096237745853\n",
      "Train loss: 0.580582954754154\n",
      "Train loss: 0.580441441408687\n",
      "Train loss: 0.580055252634374\n",
      "Train loss: 0.5797085639350928\n",
      "Train loss: 0.5799894768439937\n",
      "Train loss: 0.5800046761861716\n",
      "Train loss: 0.5800869736513603\n",
      "Train loss: 0.5799919133433977\n",
      "Train loss: 0.5802914530283663\n",
      "Train loss: 0.5803018697620403\n",
      "Train loss: 0.5800851932939539\n",
      "Train loss: 0.5802058848719498\n",
      "Train loss: 0.5802762990816115\n",
      "Train loss: 0.5801055041181288\n",
      "Train loss: 0.580169076234041\n",
      "Train loss: 0.5801327860661663\n",
      "Train loss: 0.5801814506855545\n",
      "Train loss: 0.5801309381487183\n",
      "Train loss: 0.5801863607801037\n",
      "Train loss: 0.5802751959058395\n",
      "Train loss: 0.579889941151343\n",
      "Train loss: 0.5796332450454276\n",
      "Train loss: 0.5798204476594777\n",
      "Train loss: 0.579927145980203\n",
      "Train loss: 0.5797768052153159\n",
      "Train loss: 0.5798285627485813\n",
      "Train loss: 0.5798298041388313\n",
      "Train loss: 0.579659045834816\n",
      "Train loss: 0.5798750752310016\n",
      "Train loss: 0.5798378276885945\n",
      "Train loss: 0.5801190959227895\n",
      "Train loss: 0.5802794656632939\n",
      "Train loss: 0.5800843098420504\n",
      "Train loss: 0.5799423902659652\n",
      "Train loss: 0.5802845836102546\n",
      "Train loss: 0.5803145938664437\n",
      "Train loss: 0.5804989966767408\n",
      "Train loss: 0.5805865515328249\n",
      "Train loss: 0.5807083982238455\n",
      "Train loss: 0.5805528630037099\n",
      "Train loss: 0.580739037941778\n",
      "Train loss: 0.5807469418163357\n",
      "Train loss: 0.5806164601139647\n",
      "Train loss: 0.580741543565795\n",
      "Train loss: 0.5808681869547364\n",
      "Train loss: 0.5810248515621982\n",
      "Train loss: 0.5810347750051752\n",
      "Train loss: 0.5810565037820743\n",
      "Train loss: 0.5810128493028135\n",
      "Train loss: 0.5808391607762929\n",
      "Train loss: 0.5809151411275352\n",
      "Train loss: 0.5808888593821593\n",
      "Train loss: 0.5807639924106752\n",
      "Train loss: 0.5808553585201143\n",
      "Train loss: 0.580671551297227\n",
      "Train loss: 0.5806098611483504\n",
      "Train loss: 0.5805312491282529\n",
      "Train loss: 0.5807098943953783\n",
      "Train loss: 0.5806604619207623\n",
      "Train loss: 0.5806279171695644\n",
      "Train loss: 0.5805780653469499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5803307674983582\n",
      "Train loss: 0.5804178740131604\n",
      "Train loss: 0.5802917218491795\n",
      "Train loss: 0.5802603854838305\n",
      "Train loss: 0.5802350808844157\n",
      "Train loss: 0.5801914626953839\n",
      "Train loss: 0.5803338227266355\n",
      "Train loss: 0.5803674991063219\n",
      "Train loss: 0.5804104863339544\n",
      "Train loss: 0.5805818411489474\n",
      "Train loss: 0.5806114862817395\n",
      "Train loss: 0.5807578441704353\n",
      "Train loss: 0.5807082796467213\n",
      "Train loss: 0.5806695528835585\n",
      "Train loss: 0.5805821752494821\n",
      "Train loss: 0.5806703415346305\n",
      "Train loss: 0.5806298343168106\n",
      "Train loss: 0.5806448855901469\n",
      "Train loss: 0.5805047533571742\n",
      "Train loss: 0.5803360732479377\n",
      "Train loss: 0.5804525394872411\n",
      "Train loss: 0.5802526264251568\n",
      "Train loss: 0.5802487871016193\n",
      "Train loss: 0.5802407696681575\n",
      "Train loss: 0.5802253702110034\n",
      "Train loss: 0.5802517718551652\n",
      "Train loss: 0.5802758360979041\n",
      "Train loss: 0.580150196638672\n",
      "Train loss: 0.5801865134077243\n",
      "Train loss: 0.5802359352199052\n",
      "Train loss: 0.5802989776291899\n",
      "Train loss: 0.5801896118651798\n",
      "Train loss: 0.5802455563692719\n",
      "Train loss: 0.5802555910782629\n",
      "Train loss: 0.5804202217643778\n",
      "Train loss: 0.58051862738601\n",
      "Train loss: 0.5805207388681569\n",
      "Train loss: 0.5804150133480043\n",
      "Train loss: 0.5804644919773867\n",
      "Train loss: 0.5805158356556768\n",
      "Train loss: 0.5806239004170906\n",
      "Train loss: 0.580583870188721\n",
      "Train loss: 0.5806332307581035\n",
      "Train loss: 0.5804543168715225\n",
      "Train loss: 0.5803744661149981\n",
      "Train loss: 0.5803608029276792\n",
      "Train loss: 0.5802513865524475\n",
      "Train loss: 0.5801884363193146\n",
      "Train loss: 0.5801632617708117\n",
      "Train loss: 0.5801036703353734\n",
      "Train loss: 0.5800991152692943\n",
      "Train loss: 0.5800302582165064\n",
      "Train loss: 0.5799607415294952\n",
      "Train loss: 0.5800700959493467\n",
      "Train loss: 0.5801027755039662\n",
      "Train loss: 0.5801535419018891\n",
      "Train loss: 0.5801481400389128\n",
      "Train loss: 0.5802416602630136\n",
      "Train loss: 0.5803206740604019\n",
      "Train loss: 0.5802771651518348\n",
      "Train loss: 0.5802026965628733\n",
      "Train loss: 0.5801863343585885\n",
      "Train loss: 0.5801587125599205\n",
      "Train loss: 0.5800517456685953\n",
      "Train loss: 0.5799887130833347\n",
      "Train loss: 0.5800461251047954\n",
      "Train loss: 0.5799573595387925\n",
      "Train loss: 0.5799321193814947\n",
      "Train loss: 0.5799115816308134\n",
      "Train loss: 0.5799392861612073\n",
      "Train loss: 0.5798428152290951\n",
      "Train loss: 0.5800208169835249\n",
      "Train loss: 0.5799927646121994\n",
      "Train loss: 0.5800823127707535\n",
      "Train loss: 0.580080907087899\n",
      "Train loss: 0.5801707355219085\n",
      "Train loss: 0.5801423001529672\n",
      "Train loss: 0.58011643516288\n",
      "Train loss: 0.5800649808660002\n",
      "Train loss: 0.580136556913278\n",
      "Train loss: 0.5801559668849426\n",
      "Train loss: 0.580209724529076\n",
      "Train loss: 0.5801601537783921\n",
      "Train loss: 0.5800701298790631\n",
      "Train loss: 0.5801503854906714\n",
      "Train loss: 0.5801366159801921\n",
      "Train loss: 0.5801942602802058\n",
      "Train loss: 0.5802046645015989\n",
      "Train loss: 0.5801731199205251\n",
      "Train loss: 0.5801435699015606\n",
      "Train loss: 0.5800952614134453\n",
      "Train loss: 0.5801132027290251\n",
      "Train loss: 0.5801303613492412\n",
      "Train loss: 0.5800790000784758\n",
      "Train loss: 0.5800817281418689\n",
      "Train loss: 0.580086129138458\n",
      "Train loss: 0.5799814869019411\n",
      "Train loss: 0.5799694639467704\n",
      "Train loss: 0.5799894571359017\n",
      "Train loss: 0.5799415414056759\n",
      "Train loss: 0.5798947627055534\n",
      "Train loss: 0.5800074376208018\n",
      "Train loss: 0.5799678093102312\n",
      "Train loss: 0.5800207844093846\n",
      "Train loss: 0.580041288850456\n",
      "Train loss: 0.5799611465855158\n",
      "Train loss: 0.5798560373813184\n",
      "Train loss: 0.579793742940562\n",
      "Train loss: 0.5798679998920371\n",
      "Train loss: 0.5797498247253241\n",
      "Train loss: 0.5797470094626768\n",
      "Train loss: 0.579810587900184\n",
      "Train loss: 0.5798339612796456\n",
      "Train loss: 0.5798063583897531\n",
      "Train loss: 0.5797671506383986\n",
      "Train loss: 0.5798231328857988\n",
      "Train loss: 0.579801025501018\n",
      "Train loss: 0.5797507267796709\n",
      "Train loss: 0.5796534475921508\n",
      "Train loss: 0.5796018055004664\n",
      "Train loss: 0.5795994674098768\n",
      "Train loss: 0.5796852034883248\n",
      "Train loss: 0.5797095114003776\n",
      "Train loss: 0.5797738799540489\n",
      "Train loss: 0.5798292028339556\n",
      "Train loss: 0.5798141922262596\n",
      "Train loss: 0.579722679450178\n",
      "Train loss: 0.5796819220889698\n",
      "Train loss: 0.5797405288473785\n",
      "Train loss: 0.579773547348188\n",
      "Train loss: 0.5798066545186173\n",
      "Train loss: 0.5798495953765659\n",
      "Train loss: 0.579941340082102\n",
      "Train loss: 0.5799885412630627\n",
      "Train loss: 0.5799725958976626\n",
      "Train loss: 0.5799474709874206\n",
      "Train loss: 0.5798253130577439\n",
      "Train loss: 0.5798191508214097\n",
      "Train loss: 0.5797955738837277\n",
      "Train loss: 0.5797742514116329\n",
      "Train loss: 0.5797444014944556\n",
      "Train loss: 0.5796960604375841\n",
      "Train loss: 0.5797245670853054\n",
      "Train loss: 0.57972179312152\n",
      "Train loss: 0.5796334715216059\n",
      "Train loss: 0.5795734970383789\n",
      "Train loss: 0.5794854584572167\n",
      "Train loss: 0.5795486266413394\n",
      "Train loss: 0.579568541128926\n",
      "Train loss: 0.5794364766441639\n",
      "Train loss: 0.5794002818142756\n",
      "Train loss: 0.5794676666518027\n",
      "Train loss: 0.579456278480662\n",
      "Train loss: 0.579429062229741\n",
      "Train loss: 0.5794853265811153\n",
      "Train loss: 0.579469575044523\n",
      "Train loss: 0.5794329669007272\n",
      "Train loss: 0.579450831750731\n",
      "Train loss: 0.5794349472435671\n",
      "Train loss: 0.5793994061620801\n",
      "Train loss: 0.5794066160273224\n",
      "Train loss: 0.5794155660537368\n",
      "Train loss: 0.5793699168856632\n",
      "Train loss: 0.5793418380971468\n",
      "Train loss: 0.5793025250333278\n",
      "Train loss: 0.5792684679042668\n",
      "Train loss: 0.5792877013490717\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6900290474295616\n",
      "Val loss: 0.6208942896789975\n",
      "Val loss: 0.5937631619828088\n",
      "Val loss: 0.5935683266112679\n",
      "Val loss: 0.5929955778022608\n",
      "Val loss: 0.5945216098736072\n",
      "Val loss: 0.5889448199202033\n",
      "Val loss: 0.5841866311354514\n",
      "Val loss: 0.5795238295739348\n",
      "Val loss: 0.5796264136324123\n",
      "Val loss: 0.5798318965567483\n",
      "Val loss: 0.578793386786671\n",
      "Val loss: 0.5783183532766998\n",
      "Val loss: 0.5777514987233756\n",
      "Val loss: 0.5768341788569012\n",
      "Val loss: 0.5752829815013499\n",
      "Val loss: 0.5739328964125543\n",
      "Val loss: 0.5727010861541448\n",
      "Val loss: 0.5738128109815273\n",
      "Val loss: 0.5713918299385996\n",
      "Val loss: 0.5722325484340007\n",
      "Val loss: 0.5707334504214996\n",
      "Val loss: 0.5699642405175326\n",
      "Val loss: 0.569625425489009\n",
      "Val loss: 0.5694761889115456\n",
      "Val loss: 0.5683318184789761\n",
      "Val loss: 0.568166829534431\n",
      "Val loss: 0.5671582502855671\n",
      "Val loss: 0.5664405961417489\n",
      "Val loss: 0.5665218500082925\n",
      "Val loss: 0.5662565333889676\n",
      "Val loss: 0.5663044533264712\n",
      "Val loss: 0.5661259233588125\n",
      "Val loss: 0.5655004472069486\n",
      "Val loss: 0.5653429435587477\n",
      "Val loss: 0.5653040139368792\n",
      "Val loss: 0.5637334319560424\n",
      "Val loss: 0.5637482417323602\n",
      "Val loss: 0.564452376869536\n",
      "Val loss: 0.5637547123671776\n",
      "Val loss: 0.5640469779278717\n",
      "Val loss: 0.5635281561664417\n",
      "Val loss: 0.5629742866921648\n",
      "Val loss: 0.5630878174685996\n",
      "Val loss: 0.5624561480113438\n",
      "Val loss: 0.562664195439701\n",
      "Val loss: 0.5624592202341455\n",
      "Val loss: 0.5625147753430211\n",
      "Val loss: 0.562637589627602\n",
      "Val loss: 0.5627254169868178\n",
      "Val loss: 0.5621771875798233\n",
      "Val loss: 0.5614842543500731\n",
      "Val loss: 0.5614127845249393\n",
      "Val loss: 0.5609539961061513\n",
      "Val loss: 0.5610107965495464\n",
      "Val loss: 0.5604410874373597\n",
      "Val loss: 0.560567857933716\n",
      "Val loss: 0.560709446771747\n",
      "Val loss: 0.5607469146754466\n",
      "Val loss: 0.5606558435537344\n",
      "Val loss: 0.5609972860271993\n",
      "Val loss: 0.5610028604666392\n",
      "Val loss: 0.5605651842560738\n",
      "Val loss: 0.5603499205134879\n",
      "Val loss: 0.5602047356926365\n",
      "Val loss: 0.560321488369562\n",
      "Val loss: 0.5601357955061746\n",
      "Val loss: 0.5596803933118297\n",
      "Val loss: 0.5605471383693607\n",
      "Val loss: 0.5603392680941475\n",
      "Val loss: 0.5597691184888451\n",
      "Val loss: 0.5597285375123568\n",
      "Val loss: 0.5594081195009933\n",
      "Val loss: 0.5595911667760471\n",
      "Val loss: 0.5596342433423283\n",
      "Val loss: 0.5592879793732022\n",
      "Val loss: 0.5594526410568506\n",
      "Val loss: 0.5592307076815775\n",
      "Val loss: 0.5588347286288509\n",
      "Val loss: 0.5589801598162878\n",
      "Val loss: 0.5592145002862015\n",
      "Val loss: 0.5588148432285104\n",
      "Val loss: 0.5589992946040803\n",
      "Val loss: 0.558680240393823\n",
      "Val loss: 0.5584897247125518\n",
      "Val loss: 0.5586652726560206\n",
      "Val loss: 0.5584917422813205\n",
      "Val loss: 0.5584996420049994\n",
      "Val loss: 0.5580484196543694\n",
      "Val loss: 0.5578906658895828\n",
      "Val loss: 0.5580558378397106\n",
      "Val loss: 0.5580842994267124\n",
      "Val loss: 0.5581530274759079\n",
      "Val loss: 0.5581397107923463\n",
      "Val loss: 0.5578747970142445\n",
      "Val loss: 0.5580998482435382\n",
      "Val loss: 0.5582116854584906\n",
      "Val loss: 0.5582163141305218\n",
      "Val loss: 0.5582041958806968\n",
      "Val loss: 0.5586357185979166\n",
      "Val loss: 0.5585551123533931\n",
      "Val loss: 0.5583875934595209\n",
      "Val loss: 0.5586885779748167\n",
      "Val loss: 0.5584699072245228\n",
      "Val loss: 0.5583632287519579\n",
      "Val loss: 0.558270055934477\n",
      "Val loss: 0.5579557129171457\n",
      "Val loss: 0.557940439357386\n",
      "Val loss: 0.5577471022877623\n",
      "Val loss: 0.5578825055792901\n",
      "Val loss: 0.55791485815272\n",
      "Val loss: 0.5579984169953198\n",
      "Val loss: 0.5579686153759348\n",
      "Val loss: 0.5581786004541628\n",
      "Val loss: 0.5580336234503092\n",
      "Val loss: 0.5581653918009348\n",
      "Val loss: 0.5582632105032058\n",
      "Val loss: 0.5583007209167415\n",
      "Val loss: 0.5584263122242308\n",
      "Val loss: 0.5586581812279053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5588531318484553\n",
      "Val loss: 0.5591230178114228\n",
      "Val loss: 0.5592116999412593\n",
      "Val loss: 0.5592165009174286\n",
      "Val loss: 0.5593738445582298\n",
      "Val loss: 0.5595219041944877\n",
      "Val loss: 0.5594074053907244\n",
      "\n",
      "starting Epoch 8\n",
      "Training...\n",
      "Train loss: 0.6078489485539889\n",
      "Train loss: 0.5919164006526654\n",
      "Train loss: 0.586013409040742\n",
      "Train loss: 0.5827733342406116\n",
      "Train loss: 0.5803595802398643\n",
      "Train loss: 0.5780960313913202\n",
      "Train loss: 0.57712583083043\n",
      "Train loss: 0.5755606696665662\n",
      "Train loss: 0.576621249567863\n",
      "Train loss: 0.5753076295157773\n",
      "Train loss: 0.5742211277898588\n",
      "Train loss: 0.5728599303187686\n",
      "Train loss: 0.5727205348981393\n",
      "Train loss: 0.5738131531036883\n",
      "Train loss: 0.5745998905055898\n",
      "Train loss: 0.5740313322566519\n",
      "Train loss: 0.5736272822087494\n",
      "Train loss: 0.5744582236975349\n",
      "Train loss: 0.5748625878136516\n",
      "Train loss: 0.5756387130210274\n",
      "Train loss: 0.5760876532812961\n",
      "Train loss: 0.5767220454905734\n",
      "Train loss: 0.577310755556705\n",
      "Train loss: 0.5780900390138208\n",
      "Train loss: 0.5770335497860918\n",
      "Train loss: 0.5762385763299718\n",
      "Train loss: 0.5763384201641649\n",
      "Train loss: 0.5772043084842361\n",
      "Train loss: 0.5777632199832606\n",
      "Train loss: 0.5773973175400685\n",
      "Train loss: 0.5776376326557893\n",
      "Train loss: 0.5777830887288554\n",
      "Train loss: 0.5782826981522787\n",
      "Train loss: 0.5788275034157271\n",
      "Train loss: 0.5783116380885265\n",
      "Train loss: 0.578565880179571\n",
      "Train loss: 0.5789959286723957\n",
      "Train loss: 0.5792883314555498\n",
      "Train loss: 0.5791289170914651\n",
      "Train loss: 0.5786914121224376\n",
      "Train loss: 0.5785552116249653\n",
      "Train loss: 0.578272643830978\n",
      "Train loss: 0.5783951050050045\n",
      "Train loss: 0.5786565852110974\n",
      "Train loss: 0.578776077712338\n",
      "Train loss: 0.5787684471874424\n",
      "Train loss: 0.5790953733923591\n",
      "Train loss: 0.5789695075704357\n",
      "Train loss: 0.5788379628763014\n",
      "Train loss: 0.5788278899810932\n",
      "Train loss: 0.5785462435141162\n",
      "Train loss: 0.5788263136429552\n",
      "Train loss: 0.5788835788020782\n",
      "Train loss: 0.5791641999108577\n",
      "Train loss: 0.5792579209132017\n",
      "Train loss: 0.5794462095529933\n",
      "Train loss: 0.5792604604240464\n",
      "Train loss: 0.5791253386848029\n",
      "Train loss: 0.5788929106523871\n",
      "Train loss: 0.5791800633582401\n",
      "Train loss: 0.5791080751509037\n",
      "Train loss: 0.5790811244569737\n",
      "Train loss: 0.5790906096849298\n",
      "Train loss: 0.5788892500096695\n",
      "Train loss: 0.5788798159227819\n",
      "Train loss: 0.578861962131156\n",
      "Train loss: 0.5788207340810262\n",
      "Train loss: 0.5787731930120101\n",
      "Train loss: 0.5788849403323948\n",
      "Train loss: 0.5791125101349539\n",
      "Train loss: 0.5788437298185981\n",
      "Train loss: 0.5790395616987996\n",
      "Train loss: 0.578870641666377\n",
      "Train loss: 0.5789126106780312\n",
      "Train loss: 0.5788746788113653\n",
      "Train loss: 0.5789632418973419\n",
      "Train loss: 0.5792243099971744\n",
      "Train loss: 0.5793568364621127\n",
      "Train loss: 0.5797365852327873\n",
      "Train loss: 0.5796361695870226\n",
      "Train loss: 0.5795264326546794\n",
      "Train loss: 0.5794320298339478\n",
      "Train loss: 0.5797116633026337\n",
      "Train loss: 0.5796848944498713\n",
      "Train loss: 0.5798550447412909\n",
      "Train loss: 0.5798853314359774\n",
      "Train loss: 0.5801042365472573\n",
      "Train loss: 0.5800169849849818\n",
      "Train loss: 0.579801022185577\n",
      "Train loss: 0.5797807516787965\n",
      "Train loss: 0.5798883267390066\n",
      "Train loss: 0.5799948301828705\n",
      "Train loss: 0.5803049839331682\n",
      "Train loss: 0.5803799804313948\n",
      "Train loss: 0.5803692009870851\n",
      "Train loss: 0.5800279491173842\n",
      "Train loss: 0.5800530231294095\n",
      "Train loss: 0.579828205523654\n",
      "Train loss: 0.5797694779936263\n",
      "Train loss: 0.579691635080431\n",
      "Train loss: 0.5795698600307794\n",
      "Train loss: 0.5795740827209169\n",
      "Train loss: 0.5797551276760231\n",
      "Train loss: 0.5798187978483267\n",
      "Train loss: 0.579858536050682\n",
      "Train loss: 0.5798761643655687\n",
      "Train loss: 0.5797604484054294\n",
      "Train loss: 0.5797032984995963\n",
      "Train loss: 0.5798803963427043\n",
      "Train loss: 0.579951947659132\n",
      "Train loss: 0.5799508824602018\n",
      "Train loss: 0.5798785892140924\n",
      "Train loss: 0.5800033432422037\n",
      "Train loss: 0.5799438367814754\n",
      "Train loss: 0.5797333985424498\n",
      "Train loss: 0.5798546767353034\n",
      "Train loss: 0.579914828927255\n",
      "Train loss: 0.5797472935755812\n",
      "Train loss: 0.5798330849436664\n",
      "Train loss: 0.5797929095198284\n",
      "Train loss: 0.5797898622523068\n",
      "Train loss: 0.5796680336191108\n",
      "Train loss: 0.5798033094347953\n",
      "Train loss: 0.5798776174514897\n",
      "Train loss: 0.5798628589972442\n",
      "Train loss: 0.5797747109867647\n",
      "Train loss: 0.5798600528267436\n",
      "Train loss: 0.579955118389137\n",
      "Train loss: 0.5799048924663355\n",
      "Train loss: 0.5798970871060662\n",
      "Train loss: 0.5797074968325996\n",
      "Train loss: 0.5798300298699469\n",
      "Train loss: 0.5797097700885832\n",
      "Train loss: 0.5798598616681201\n",
      "Train loss: 0.5798502046561056\n",
      "Train loss: 0.5800444874946753\n",
      "Train loss: 0.5799205202327762\n",
      "Train loss: 0.5799445370134559\n",
      "Train loss: 0.5800051941057468\n",
      "Train loss: 0.5798341962067984\n",
      "Train loss: 0.5798318952205714\n",
      "Train loss: 0.5798151880324244\n",
      "Train loss: 0.5797568157904213\n",
      "Train loss: 0.5797569160351285\n",
      "Train loss: 0.5797836100101142\n",
      "Train loss: 0.5797847528272232\n",
      "Train loss: 0.5797615948786903\n",
      "Train loss: 0.5798995860145075\n",
      "Train loss: 0.5797724785141274\n",
      "Train loss: 0.579763373431463\n",
      "Train loss: 0.5797476069486548\n",
      "Train loss: 0.5796103820023313\n",
      "Train loss: 0.5795404244361652\n",
      "Train loss: 0.5795927144231329\n",
      "Train loss: 0.579506076326598\n",
      "Train loss: 0.5794280445052096\n",
      "Train loss: 0.5794111977777271\n",
      "Train loss: 0.579312519819233\n",
      "Train loss: 0.5792274254697342\n",
      "Train loss: 0.5793114158744996\n",
      "Train loss: 0.5791707981369739\n",
      "Train loss: 0.5790771559897526\n",
      "Train loss: 0.579280284174237\n",
      "Train loss: 0.5791661322353162\n",
      "Train loss: 0.5792579886478957\n",
      "Train loss: 0.5792575914409369\n",
      "Train loss: 0.5792720243401711\n",
      "Train loss: 0.5793061940469143\n",
      "Train loss: 0.5792341789362451\n",
      "Train loss: 0.5792547295499668\n",
      "Train loss: 0.5791468685588211\n",
      "Train loss: 0.5790715867972506\n",
      "Train loss: 0.5789486484208456\n",
      "Train loss: 0.5788483433286223\n",
      "Train loss: 0.5787646918424234\n",
      "Train loss: 0.5788486976029352\n",
      "Train loss: 0.5788131393460643\n",
      "Train loss: 0.5788107926623143\n",
      "Train loss: 0.5788717207195173\n",
      "Train loss: 0.5789239438797706\n",
      "Train loss: 0.5789400199791054\n",
      "Train loss: 0.5787593531441512\n",
      "Train loss: 0.5787842608061798\n",
      "Train loss: 0.5788503154670781\n",
      "Train loss: 0.5788533779278611\n",
      "Train loss: 0.5788167501031215\n",
      "Train loss: 0.5788717857415295\n",
      "Train loss: 0.5788678719772877\n",
      "Train loss: 0.5788296079802809\n",
      "Train loss: 0.5787267358593641\n",
      "Train loss: 0.5786460348841219\n",
      "Train loss: 0.5786301944366996\n",
      "Train loss: 0.5786693439053636\n",
      "Train loss: 0.5785880963975666\n",
      "Train loss: 0.5785534791944577\n",
      "Train loss: 0.578485042510759\n",
      "Train loss: 0.57843045843044\n",
      "Train loss: 0.5784801749094293\n",
      "Train loss: 0.578415368588374\n",
      "Train loss: 0.5784431931509856\n",
      "Train loss: 0.5784441198644783\n",
      "Train loss: 0.5784029423614987\n",
      "Train loss: 0.5783465102436097\n",
      "Train loss: 0.578305728939942\n",
      "Train loss: 0.5782652443634297\n",
      "Train loss: 0.5783198512810241\n",
      "Train loss: 0.5783279436269397\n",
      "Train loss: 0.5782247649617251\n",
      "Train loss: 0.5780765445616348\n",
      "Train loss: 0.5780614798595349\n",
      "Train loss: 0.5780472704148683\n",
      "Train loss: 0.5780780137949154\n",
      "Train loss: 0.5780955050214968\n",
      "Train loss: 0.5780571730901883\n",
      "Train loss: 0.5780099648726654\n",
      "Train loss: 0.5779721199789928\n",
      "Train loss: 0.5780604492686646\n",
      "Train loss: 0.5780911859390686\n",
      "Train loss: 0.5780382406346791\n",
      "Train loss: 0.5780454694709878\n",
      "Train loss: 0.5780466687331294\n",
      "Train loss: 0.5781116927225973\n",
      "Train loss: 0.5780430438540447\n",
      "Train loss: 0.578105276558223\n",
      "Train loss: 0.5781538798812231\n",
      "Train loss: 0.578083811225182\n",
      "Train loss: 0.57809076302113\n",
      "Train loss: 0.5780905046649187\n",
      "Train loss: 0.5781754648927571\n",
      "Train loss: 0.5782692284914793\n",
      "Train loss: 0.5783072386786235\n",
      "Train loss: 0.5783247970627619\n",
      "Train loss: 0.5782918173129734\n",
      "Train loss: 0.5782891860131041\n",
      "Train loss: 0.5783801156616029\n",
      "Train loss: 0.5783840622164784\n",
      "Train loss: 0.5783866148416089\n",
      "Train loss: 0.5784620341445047\n",
      "Train loss: 0.578526208635952\n",
      "Train loss: 0.5785083540046332\n",
      "Train loss: 0.5785449422551131\n",
      "Train loss: 0.57856500673501\n",
      "Train loss: 0.5785803787316725\n",
      "Train loss: 0.5785123036762416\n",
      "Train loss: 0.5785371381717206\n",
      "Train loss: 0.5784567121047512\n",
      "Train loss: 0.5784826064355992\n",
      "Train loss: 0.5784853183771242\n",
      "Train loss: 0.5784476553012291\n",
      "Train loss: 0.5784580637083265\n",
      "Train loss: 0.5785012262279376\n",
      "Train loss: 0.5784522207229472\n",
      "Train loss: 0.5783928729406717\n",
      "Train loss: 0.5783338734286346\n",
      "Train loss: 0.5783561044263943\n",
      "Train loss: 0.5782409946267366\n",
      "Train loss: 0.5781723144662638\n",
      "Train loss: 0.5782059562208579\n",
      "Train loss: 0.578182748574091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5782373714020537\n",
      "Train loss: 0.578258426807511\n",
      "Train loss: 0.578274469676876\n",
      "Train loss: 0.57822792864748\n",
      "Train loss: 0.5781081050922302\n",
      "Train loss: 0.5780933381622975\n",
      "Train loss: 0.578167826912192\n",
      "Train loss: 0.5781754621641759\n",
      "Train loss: 0.5781139982883378\n",
      "Train loss: 0.5781786565300611\n",
      "Train loss: 0.5781525925179944\n",
      "Train loss: 0.5782782215048278\n",
      "Train loss: 0.578344885616711\n",
      "Train loss: 0.578306887766275\n",
      "Train loss: 0.5782761139892753\n",
      "Train loss: 0.5782577997663928\n",
      "Train loss: 0.5783295444570432\n",
      "Train loss: 0.5783938558270816\n",
      "Train loss: 0.5784193415889681\n",
      "Train loss: 0.578409906836645\n",
      "Train loss: 0.5783865714492618\n",
      "Train loss: 0.5783927861662771\n",
      "Train loss: 0.578481835007435\n",
      "Train loss: 0.5785144217592769\n",
      "Train loss: 0.5784970972923813\n",
      "Train loss: 0.5785394652587611\n",
      "Train loss: 0.5785509521184966\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6990196332335472\n",
      "Val loss: 0.624040832122167\n",
      "Val loss: 0.5944347615752902\n",
      "Val loss: 0.5937655175987043\n",
      "Val loss: 0.5937872938811779\n",
      "Val loss: 0.5971870021573429\n",
      "Val loss: 0.5910858152543798\n",
      "Val loss: 0.5857022488728548\n",
      "Val loss: 0.5810047882524404\n",
      "Val loss: 0.5814203005664202\n",
      "Val loss: 0.5819464408689075\n",
      "Val loss: 0.5809447517839529\n",
      "Val loss: 0.5797825888730586\n",
      "Val loss: 0.579475289237672\n",
      "Val loss: 0.5785157499281136\n",
      "Val loss: 0.5771942383880857\n",
      "Val loss: 0.5759195043217569\n",
      "Val loss: 0.574922101886085\n",
      "Val loss: 0.5756904086534013\n",
      "Val loss: 0.5735638222911141\n",
      "Val loss: 0.574127943183367\n",
      "Val loss: 0.5728028605290509\n",
      "Val loss: 0.571717497288135\n",
      "Val loss: 0.571361528224304\n",
      "Val loss: 0.5711935710522437\n",
      "Val loss: 0.5699169755906097\n",
      "Val loss: 0.5698644343151975\n",
      "Val loss: 0.5687962716860737\n",
      "Val loss: 0.5679612199051512\n",
      "Val loss: 0.5682288834312618\n",
      "Val loss: 0.5678704173920991\n",
      "Val loss: 0.5679407279071568\n",
      "Val loss: 0.5679267214202299\n",
      "Val loss: 0.5672733513913917\n",
      "Val loss: 0.5668592048787523\n",
      "Val loss: 0.5666868879808394\n",
      "Val loss: 0.565080849048884\n",
      "Val loss: 0.5649652276089583\n",
      "Val loss: 0.5655852608459512\n",
      "Val loss: 0.564798942163362\n",
      "Val loss: 0.5651586283655727\n",
      "Val loss: 0.5647159121823653\n",
      "Val loss: 0.5641884291283438\n",
      "Val loss: 0.5644269335215495\n",
      "Val loss: 0.5641171035489866\n",
      "Val loss: 0.564470270315112\n",
      "Val loss: 0.5641805593274597\n",
      "Val loss: 0.5641313048087403\n",
      "Val loss: 0.5641784672854376\n",
      "Val loss: 0.5643097128734053\n",
      "Val loss: 0.5637451146296629\n",
      "Val loss: 0.5630547504397433\n",
      "Val loss: 0.5629190250553868\n",
      "Val loss: 0.5624999195669663\n",
      "Val loss: 0.5625287830612086\n",
      "Val loss: 0.5620072019356553\n",
      "Val loss: 0.5620948035322445\n",
      "Val loss: 0.5622667168044714\n",
      "Val loss: 0.5623727373930872\n",
      "Val loss: 0.5622206174809\n",
      "Val loss: 0.5623780641900865\n",
      "Val loss: 0.5623425893413211\n",
      "Val loss: 0.5618514538190926\n",
      "Val loss: 0.5615808521879131\n",
      "Val loss: 0.5614165555179855\n",
      "Val loss: 0.5614732906992312\n",
      "Val loss: 0.5613379399041216\n",
      "Val loss: 0.5609368152674672\n",
      "Val loss: 0.5617886346786521\n",
      "Val loss: 0.5616845230659988\n",
      "Val loss: 0.5610306490275819\n",
      "Val loss: 0.5609481152053662\n",
      "Val loss: 0.5606262963060494\n",
      "Val loss: 0.5608742079599117\n",
      "Val loss: 0.5609809612048501\n",
      "Val loss: 0.5605471394464648\n",
      "Val loss: 0.5606507450186958\n",
      "Val loss: 0.5603939710821775\n",
      "Val loss: 0.5599870745300641\n",
      "Val loss: 0.5600598297621074\n",
      "Val loss: 0.5603654129375325\n",
      "Val loss: 0.5599518983754669\n",
      "Val loss: 0.5601332763542876\n",
      "Val loss: 0.5598367844532669\n",
      "Val loss: 0.5595949652060023\n",
      "Val loss: 0.5597126657590443\n",
      "Val loss: 0.5595401206873529\n",
      "Val loss: 0.5595156728270928\n",
      "Val loss: 0.5590865605989018\n",
      "Val loss: 0.5589021924874831\n",
      "Val loss: 0.5590609843772939\n",
      "Val loss: 0.5590838149741844\n",
      "Val loss: 0.5591492218061768\n",
      "Val loss: 0.559129220844586\n",
      "Val loss: 0.5589216140252126\n",
      "Val loss: 0.559069818892907\n",
      "Val loss: 0.5591100538072508\n",
      "Val loss: 0.5591171053533418\n",
      "Val loss: 0.5590780276035973\n",
      "Val loss: 0.5595536284551831\n",
      "Val loss: 0.5594577273679158\n",
      "Val loss: 0.5592343844692927\n",
      "Val loss: 0.5595549770598282\n",
      "Val loss: 0.5593283837930315\n",
      "Val loss: 0.5592506836161358\n",
      "Val loss: 0.5591685414764966\n",
      "Val loss: 0.5588554850455081\n",
      "Val loss: 0.5588202288509079\n",
      "Val loss: 0.5586371437701232\n",
      "Val loss: 0.5587516488381856\n",
      "Val loss: 0.5587957264176344\n",
      "Val loss: 0.5588594030502231\n",
      "Val loss: 0.5588245174669205\n",
      "Val loss: 0.5590270104538159\n",
      "Val loss: 0.5588561618888835\n",
      "Val loss: 0.5589878668748035\n",
      "Val loss: 0.5591378506426126\n",
      "Val loss: 0.5591549430112078\n",
      "Val loss: 0.5592585852250507\n",
      "Val loss: 0.5594730579196312\n",
      "Val loss: 0.5596651503976607\n",
      "Val loss: 0.5599366824810924\n",
      "Val loss: 0.5600885478506648\n",
      "Val loss: 0.5601194313635541\n",
      "Val loss: 0.5603055064208232\n",
      "Val loss: 0.5604467134691763\n",
      "Val loss: 0.5603344279226821\n",
      "\n",
      "starting Epoch 9\n",
      "Training...\n",
      "Train loss: 0.5981949740334561\n",
      "Train loss: 0.575827133961213\n",
      "Train loss: 0.5753860311993098\n",
      "Train loss: 0.5743601880496061\n",
      "Train loss: 0.5800214722903088\n",
      "Train loss: 0.5822173121596584\n",
      "Train loss: 0.5804960376067128\n",
      "Train loss: 0.5803668513987799\n",
      "Train loss: 0.5806581361333751\n",
      "Train loss: 0.5797953605651855\n",
      "Train loss: 0.5807854791754457\n",
      "Train loss: 0.5811206354256954\n",
      "Train loss: 0.5820107310435026\n",
      "Train loss: 0.5831444460004034\n",
      "Train loss: 0.5833361668132221\n",
      "Train loss: 0.5826548321120044\n",
      "Train loss: 0.5822446686504161\n",
      "Train loss: 0.5836449181635067\n",
      "Train loss: 0.5843881260908373\n",
      "Train loss: 0.5840649745966259\n",
      "Train loss: 0.5841689143232058\n",
      "Train loss: 0.5822690776920536\n",
      "Train loss: 0.5826591661033548\n",
      "Train loss: 0.5824105319623409\n",
      "Train loss: 0.581884312546086\n",
      "Train loss: 0.5811814755947833\n",
      "Train loss: 0.5813825238834728\n",
      "Train loss: 0.5804300665962036\n",
      "Train loss: 0.5807218336997254\n",
      "Train loss: 0.5805760240913035\n",
      "Train loss: 0.5804351321714952\n",
      "Train loss: 0.5804377516297294\n",
      "Train loss: 0.5802913145338819\n",
      "Train loss: 0.5805149654225502\n",
      "Train loss: 0.5799029143753652\n",
      "Train loss: 0.5806163886823641\n",
      "Train loss: 0.5804070587167882\n",
      "Train loss: 0.5802530546197778\n",
      "Train loss: 0.58022755763191\n",
      "Train loss: 0.5810974216804338\n",
      "Train loss: 0.5813548583469111\n",
      "Train loss: 0.5809337986240228\n",
      "Train loss: 0.5813175664653045\n",
      "Train loss: 0.5809089464810385\n",
      "Train loss: 0.5809673322520081\n",
      "Train loss: 0.5810587650997984\n",
      "Train loss: 0.5806025197140325\n",
      "Train loss: 0.5806913862822576\n",
      "Train loss: 0.5803383092592881\n",
      "Train loss: 0.5806262199585144\n",
      "Train loss: 0.5804732242147634\n",
      "Train loss: 0.5800866880476532\n",
      "Train loss: 0.5802307596343997\n",
      "Train loss: 0.5803353530151961\n",
      "Train loss: 0.5801650008307032\n",
      "Train loss: 0.579986707854207\n",
      "Train loss: 0.579880107197456\n",
      "Train loss: 0.5796262540109617\n",
      "Train loss: 0.5796532977476678\n",
      "Train loss: 0.5790886286897795\n",
      "Train loss: 0.5791844156455368\n",
      "Train loss: 0.57908398552803\n",
      "Train loss: 0.5788302291850422\n",
      "Train loss: 0.5789068982040071\n",
      "Train loss: 0.5792532217236461\n",
      "Train loss: 0.5793490036135827\n",
      "Train loss: 0.5792392160662793\n",
      "Train loss: 0.5791976713493872\n",
      "Train loss: 0.5792435104428555\n",
      "Train loss: 0.5793765505524172\n",
      "Train loss: 0.5792838246617374\n",
      "Train loss: 0.5793364841421086\n",
      "Train loss: 0.5790542699524276\n",
      "Train loss: 0.5791087682139157\n",
      "Train loss: 0.5791540336736128\n",
      "Train loss: 0.5791446717653407\n",
      "Train loss: 0.578896680170255\n",
      "Train loss: 0.5787355369715602\n",
      "Train loss: 0.578885142508\n",
      "Train loss: 0.5789028184349795\n",
      "Train loss: 0.578791843015077\n",
      "Train loss: 0.5789466900858957\n",
      "Train loss: 0.5791099737069921\n",
      "Train loss: 0.5790357276078418\n",
      "Train loss: 0.5792724213893445\n",
      "Train loss: 0.579091324890818\n",
      "Train loss: 0.5794376050555343\n",
      "Train loss: 0.5793438408102347\n",
      "Train loss: 0.5794935849960632\n",
      "Train loss: 0.5792486738701408\n",
      "Train loss: 0.5791047230371084\n",
      "Train loss: 0.5788399808882629\n",
      "Train loss: 0.5788398854125886\n",
      "Train loss: 0.5787846782760712\n",
      "Train loss: 0.5788751129039404\n",
      "Train loss: 0.5787709021767085\n",
      "Train loss: 0.5788737255436296\n",
      "Train loss: 0.5789306028696151\n",
      "Train loss: 0.5789654755182734\n",
      "Train loss: 0.579056104565931\n",
      "Train loss: 0.578755831942811\n",
      "Train loss: 0.5788144210847702\n",
      "Train loss: 0.5786470029468731\n",
      "Train loss: 0.5786023771263993\n",
      "Train loss: 0.5787338280092824\n",
      "Train loss: 0.578733739072616\n",
      "Train loss: 0.5788316937348953\n",
      "Train loss: 0.5788626053257085\n",
      "Train loss: 0.578917661887782\n",
      "Train loss: 0.5789672605229161\n",
      "Train loss: 0.5790703338833207\n",
      "Train loss: 0.5791611573881419\n",
      "Train loss: 0.5789047480684933\n",
      "Train loss: 0.5790534060531773\n",
      "Train loss: 0.5790539588576662\n",
      "Train loss: 0.5790869299709515\n",
      "Train loss: 0.5790646948356718\n",
      "Train loss: 0.578948099237218\n",
      "Train loss: 0.579082910530504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5790186946269819\n",
      "Train loss: 0.578937624815625\n",
      "Train loss: 0.5789166598009154\n",
      "Train loss: 0.5789513306290021\n",
      "Train loss: 0.5790012210018455\n",
      "Train loss: 0.5789446567668587\n",
      "Train loss: 0.5789800317763525\n",
      "Train loss: 0.5789682740839319\n",
      "Train loss: 0.5789063967918688\n",
      "Train loss: 0.5788145294458471\n",
      "Train loss: 0.5787669621141381\n",
      "Train loss: 0.5786939634194398\n",
      "Train loss: 0.578758939033478\n",
      "Train loss: 0.5786574365681121\n",
      "Train loss: 0.5785381562587885\n",
      "Train loss: 0.5783203475649333\n",
      "Train loss: 0.5784157892389743\n",
      "Train loss: 0.5784393163025924\n",
      "Train loss: 0.578440247162307\n",
      "Train loss: 0.5784985502982749\n",
      "Train loss: 0.5784884378095745\n",
      "Train loss: 0.5785155483977766\n",
      "Train loss: 0.5785055649704646\n",
      "Train loss: 0.5785547993157284\n",
      "Train loss: 0.578570303361256\n",
      "Train loss: 0.5786565386373119\n",
      "Train loss: 0.5786768527788918\n",
      "Train loss: 0.5787104261432549\n",
      "Train loss: 0.5785870192098472\n",
      "Train loss: 0.5785772246916688\n",
      "Train loss: 0.578522672997828\n",
      "Train loss: 0.5784028551956801\n",
      "Train loss: 0.5783576509128004\n",
      "Train loss: 0.5783851696175273\n",
      "Train loss: 0.5785256340427033\n",
      "Train loss: 0.5785200728566926\n",
      "Train loss: 0.5785676093390447\n",
      "Train loss: 0.5784849198371903\n",
      "Train loss: 0.5784577573447637\n",
      "Train loss: 0.5785023460563679\n",
      "Train loss: 0.578547748150994\n",
      "Train loss: 0.5784625984119606\n",
      "Train loss: 0.5786528766394471\n",
      "Train loss: 0.5786080811882575\n",
      "Train loss: 0.5786819054917541\n",
      "Train loss: 0.578833532888046\n",
      "Train loss: 0.5786667451168191\n",
      "Train loss: 0.5787402384382553\n",
      "Train loss: 0.5787260051939807\n",
      "Train loss: 0.5787097944261478\n",
      "Train loss: 0.5786614180505538\n",
      "Train loss: 0.5786584719697367\n",
      "Train loss: 0.5786793609021118\n",
      "Train loss: 0.5786595396841837\n",
      "Train loss: 0.5786508341893001\n",
      "Train loss: 0.578667458448113\n",
      "Train loss: 0.578618108514833\n",
      "Train loss: 0.5786255797648638\n",
      "Train loss: 0.5787176900815683\n",
      "Train loss: 0.5786038207155128\n",
      "Train loss: 0.5784813619531105\n",
      "Train loss: 0.5785203272629654\n",
      "Train loss: 0.5786582125628771\n",
      "Train loss: 0.578709463346327\n",
      "Train loss: 0.5787165661598879\n",
      "Train loss: 0.57868367197482\n",
      "Train loss: 0.5786506483538172\n",
      "Train loss: 0.578653383219934\n",
      "Train loss: 0.5786963574396039\n",
      "Train loss: 0.5787669374659503\n",
      "Train loss: 0.5788791011464881\n",
      "Train loss: 0.5789031462695593\n",
      "Train loss: 0.5789294253261364\n",
      "Train loss: 0.5789626510256342\n",
      "Train loss: 0.5790358661806254\n",
      "Train loss: 0.579010860901302\n",
      "Train loss: 0.5790469914895292\n",
      "Train loss: 0.5789186848196531\n",
      "Train loss: 0.5789809125816922\n",
      "Train loss: 0.5789199807695421\n",
      "Train loss: 0.578894468464533\n",
      "Train loss: 0.5788467839512252\n",
      "Train loss: 0.5789531268941261\n",
      "Train loss: 0.5789538405819108\n",
      "Train loss: 0.5791034055752742\n",
      "Train loss: 0.5792291512152543\n",
      "Train loss: 0.579312689278594\n",
      "Train loss: 0.5793070958256289\n",
      "Train loss: 0.579308751312177\n",
      "Train loss: 0.579388225355283\n",
      "Train loss: 0.5793643379302273\n",
      "Train loss: 0.5793647151983414\n",
      "Train loss: 0.579444806132572\n",
      "Train loss: 0.5794998940153003\n",
      "Train loss: 0.5794446672909057\n",
      "Train loss: 0.5794977921889422\n",
      "Train loss: 0.579429054238173\n",
      "Train loss: 0.5793817204232863\n",
      "Train loss: 0.5794463428534289\n",
      "Train loss: 0.5794770585657609\n",
      "Train loss: 0.5794756768115084\n",
      "Train loss: 0.5794111346532166\n",
      "Train loss: 0.5795300052654209\n",
      "Train loss: 0.5796008723737627\n",
      "Train loss: 0.5795882118948604\n",
      "Train loss: 0.579578650444183\n",
      "Train loss: 0.579608439099976\n",
      "Train loss: 0.5796360511970458\n",
      "Train loss: 0.5795369301874613\n",
      "Train loss: 0.5793883052552259\n",
      "Train loss: 0.5794156895471309\n",
      "Train loss: 0.5794611804398605\n",
      "Train loss: 0.5794490918398263\n",
      "Train loss: 0.5793792182576838\n",
      "Train loss: 0.5792963700777038\n",
      "Train loss: 0.5792065424776047\n",
      "Train loss: 0.5792685140869228\n",
      "Train loss: 0.5792368279966933\n",
      "Train loss: 0.579192404806877\n",
      "Train loss: 0.5792254142979244\n",
      "Train loss: 0.5792807463147437\n",
      "Train loss: 0.5793013219690887\n",
      "Train loss: 0.5793745208436927\n",
      "Train loss: 0.5793552012705072\n",
      "Train loss: 0.579311531943409\n",
      "Train loss: 0.5792104129062231\n",
      "Train loss: 0.5791942823657807\n",
      "Train loss: 0.5791911956244911\n",
      "Train loss: 0.5792485469530416\n",
      "Train loss: 0.5793348467237477\n",
      "Train loss: 0.5792935766632925\n",
      "Train loss: 0.5792457417891685\n",
      "Train loss: 0.5791544298208713\n",
      "Train loss: 0.5791049849070398\n",
      "Train loss: 0.5790947443928675\n",
      "Train loss: 0.5790633883785327\n",
      "Train loss: 0.5790078115139559\n",
      "Train loss: 0.578963286549833\n",
      "Train loss: 0.5790275535130136\n",
      "Train loss: 0.5791135155100416\n",
      "Train loss: 0.5791627301587855\n",
      "Train loss: 0.5791626340825796\n",
      "Train loss: 0.5792358834214691\n",
      "Train loss: 0.579238760622084\n",
      "Train loss: 0.5792454352182835\n",
      "Train loss: 0.5792914146150051\n",
      "Train loss: 0.5793028411156568\n",
      "Train loss: 0.5792643869497017\n",
      "Train loss: 0.5792339985484973\n",
      "Train loss: 0.579248402559263\n",
      "Train loss: 0.579189730388276\n",
      "Train loss: 0.5792453639246393\n",
      "Train loss: 0.579138757946255\n",
      "Train loss: 0.5790994712329868\n",
      "Train loss: 0.5791204177206244\n",
      "Train loss: 0.5790892988741452\n",
      "Train loss: 0.5790695619822888\n",
      "Train loss: 0.5791016331883755\n",
      "Train loss: 0.579052023373827\n",
      "Train loss: 0.5789628583569961\n",
      "Train loss: 0.5789720457746506\n",
      "Train loss: 0.5789624786362102\n",
      "Train loss: 0.5789186500196529\n",
      "Train loss: 0.5788912198941264\n",
      "Train loss: 0.5789379025401942\n",
      "Train loss: 0.5788923870117795\n",
      "Train loss: 0.5788503578751837\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6961116641759872\n",
      "Val loss: 0.6193466716342502\n",
      "Val loss: 0.5912760261978421\n",
      "Val loss: 0.593945994188911\n",
      "Val loss: 0.5939435822268327\n",
      "Val loss: 0.5955843134173031\n",
      "Val loss: 0.5900407179313547\n",
      "Val loss: 0.5852316839572711\n",
      "Val loss: 0.5796818889000199\n",
      "Val loss: 0.5803817261238488\n",
      "Val loss: 0.5806083629528681\n",
      "Val loss: 0.579757967742823\n",
      "Val loss: 0.5790690961293876\n",
      "Val loss: 0.5787095608918563\n",
      "Val loss: 0.5774092738692825\n",
      "Val loss: 0.5756158881549593\n",
      "Val loss: 0.5744186363049916\n",
      "Val loss: 0.5732948803499843\n",
      "Val loss: 0.5741533380239567\n",
      "Val loss: 0.5716354777716627\n",
      "Val loss: 0.5724097905823817\n",
      "Val loss: 0.5707641902866714\n",
      "Val loss: 0.5695654911953106\n",
      "Val loss: 0.5689617089864587\n",
      "Val loss: 0.5686845373242132\n",
      "Val loss: 0.5672073895617049\n",
      "Val loss: 0.5667993317344295\n",
      "Val loss: 0.5656550190002798\n",
      "Val loss: 0.564959278744128\n",
      "Val loss: 0.5651876848416041\n",
      "Val loss: 0.5648416456851092\n",
      "Val loss: 0.5648039701224873\n",
      "Val loss: 0.5646130437167679\n",
      "Val loss: 0.5639048432104686\n",
      "Val loss: 0.5636148252363863\n",
      "Val loss: 0.5635886410428159\n",
      "Val loss: 0.5618695192362951\n",
      "Val loss: 0.5617366445758355\n",
      "Val loss: 0.5625373693471102\n",
      "Val loss: 0.5618103607815115\n",
      "Val loss: 0.5622890347651407\n",
      "Val loss: 0.5618671402691654\n",
      "Val loss: 0.5613312820129306\n",
      "Val loss: 0.5614006006282214\n",
      "Val loss: 0.5609608626525316\n",
      "Val loss: 0.5610676958311073\n",
      "Val loss: 0.5607611067019976\n",
      "Val loss: 0.5608555769321809\n",
      "Val loss: 0.5609627686074523\n",
      "Val loss: 0.5611787948263697\n",
      "Val loss: 0.5605419968056866\n",
      "Val loss: 0.5598055062146721\n",
      "Val loss: 0.5597276696653077\n",
      "Val loss: 0.5592691854030226\n",
      "Val loss: 0.5593166478576451\n",
      "Val loss: 0.5586706392440317\n",
      "Val loss: 0.5587622931515667\n",
      "Val loss: 0.5589342326647683\n",
      "Val loss: 0.5589908896660318\n",
      "Val loss: 0.5588937583176986\n",
      "Val loss: 0.5590970803444323\n",
      "Val loss: 0.5591134443445113\n",
      "Val loss: 0.5585714862414985\n",
      "Val loss: 0.5582238309622558\n",
      "Val loss: 0.558100412565249\n",
      "Val loss: 0.5582373136807358\n",
      "Val loss: 0.5580418770184774\n",
      "Val loss: 0.5575777980606113\n",
      "Val loss: 0.5583927814177302\n",
      "Val loss: 0.5583130390220522\n",
      "Val loss: 0.5576221559849163\n",
      "Val loss: 0.5575131304914905\n",
      "Val loss: 0.5572489118510551\n",
      "Val loss: 0.5575401865369906\n",
      "Val loss: 0.5575991927939941\n",
      "Val loss: 0.557242334989885\n",
      "Val loss: 0.5573982259569069\n",
      "Val loss: 0.5571152320404592\n",
      "Val loss: 0.5567333337467939\n",
      "Val loss: 0.5568420228952154\n",
      "Val loss: 0.5571538732931165\n",
      "Val loss: 0.5567038513191754\n",
      "Val loss: 0.556836923899282\n",
      "Val loss: 0.5565170736045427\n",
      "Val loss: 0.5562653516963968\n",
      "Val loss: 0.5563627658607243\n",
      "Val loss: 0.5562311363522359\n",
      "Val loss: 0.5562061089046452\n",
      "Val loss: 0.5556882625898799\n",
      "Val loss: 0.5554715054470606\n",
      "Val loss: 0.5556755982962999\n",
      "Val loss: 0.5557601186964247\n",
      "Val loss: 0.555758167315146\n",
      "Val loss: 0.5556805798493977\n",
      "Val loss: 0.5553987101663517\n",
      "Val loss: 0.5556750499132033\n",
      "Val loss: 0.5558157454837452\n",
      "Val loss: 0.5558689826592833\n",
      "Val loss: 0.5558870089681525\n",
      "Val loss: 0.5564188965575728\n",
      "Val loss: 0.5562981148324315\n",
      "Val loss: 0.5561381854804888\n",
      "Val loss: 0.5565184723542358\n",
      "Val loss: 0.5562905890289299\n",
      "Val loss: 0.5562258387226184\n",
      "Val loss: 0.5561191760186662\n",
      "Val loss: 0.5557417462157846\n",
      "Val loss: 0.5557058195346803\n",
      "Val loss: 0.5554795706206385\n",
      "Val loss: 0.555614416445105\n",
      "Val loss: 0.5556348957094475\n",
      "Val loss: 0.5557400558630341\n",
      "Val loss: 0.555701697918963\n",
      "Val loss: 0.5559273440724727\n",
      "Val loss: 0.5557482615372861\n",
      "Val loss: 0.5558847595578854\n",
      "Val loss: 0.5560108991722538\n",
      "Val loss: 0.5560531781160973\n",
      "Val loss: 0.5562258015578042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5564987703237391\n",
      "Val loss: 0.556707360787897\n",
      "Val loss: 0.557051254708583\n",
      "Val loss: 0.557117220681731\n",
      "Val loss: 0.55711564854967\n",
      "Val loss: 0.55728560886704\n",
      "Val loss: 0.5574416715314164\n",
      "Val loss: 0.5573137856620344\n",
      "\n",
      "starting Epoch 10\n",
      "Training...\n",
      "Train loss: 0.6200687759800961\n",
      "Train loss: 0.6004974528765067\n",
      "Train loss: 0.5942170190609107\n",
      "Train loss: 0.591191451760787\n",
      "Train loss: 0.5919670703435185\n",
      "Train loss: 0.5922158753671566\n",
      "Train loss: 0.5899545728302688\n",
      "Train loss: 0.5903378749418559\n",
      "Train loss: 0.5912593388357642\n",
      "Train loss: 0.5880866452078124\n",
      "Train loss: 0.5890894752659209\n",
      "Train loss: 0.5884482227109965\n",
      "Train loss: 0.5874556007302406\n",
      "Train loss: 0.5862289290060706\n",
      "Train loss: 0.585579086805267\n",
      "Train loss: 0.5859543244853662\n",
      "Train loss: 0.5858502738771185\n",
      "Train loss: 0.5844303816972005\n",
      "Train loss: 0.5842599267223579\n",
      "Train loss: 0.5845245583762501\n",
      "Train loss: 0.5854292047478987\n",
      "Train loss: 0.5854777465648695\n",
      "Train loss: 0.5846245768023473\n",
      "Train loss: 0.5850597129734175\n",
      "Train loss: 0.5848746288396075\n",
      "Train loss: 0.5846761919859517\n",
      "Train loss: 0.5846427985378895\n",
      "Train loss: 0.5839714508150473\n",
      "Train loss: 0.5839846594860723\n",
      "Train loss: 0.5840855816469368\n",
      "Train loss: 0.5840274029575373\n",
      "Train loss: 0.5835257827396124\n",
      "Train loss: 0.583067818322927\n",
      "Train loss: 0.5826424502483699\n",
      "Train loss: 0.5828544247644994\n",
      "Train loss: 0.5826703178202824\n",
      "Train loss: 0.5828076128385387\n",
      "Train loss: 0.5827069897818157\n",
      "Train loss: 0.5823415918741973\n",
      "Train loss: 0.5826328078632211\n",
      "Train loss: 0.5824795741592426\n",
      "Train loss: 0.5823179830101022\n",
      "Train loss: 0.5825669282876016\n",
      "Train loss: 0.5824906886369837\n",
      "Train loss: 0.5824192666901364\n",
      "Train loss: 0.5822771011291313\n",
      "Train loss: 0.5819790469635282\n",
      "Train loss: 0.5816055970408248\n",
      "Train loss: 0.5817534567995627\n",
      "Train loss: 0.5814950376957864\n",
      "Train loss: 0.5812905365573763\n",
      "Train loss: 0.5812249007823952\n",
      "Train loss: 0.5812391659018\n",
      "Train loss: 0.5816137897846763\n",
      "Train loss: 0.5812403678460161\n",
      "Train loss: 0.5813431206614551\n",
      "Train loss: 0.5809721403000541\n",
      "Train loss: 0.5808938413575562\n",
      "Train loss: 0.5803579647880778\n",
      "Train loss: 0.5804114385482368\n",
      "Train loss: 0.5805354836878961\n",
      "Train loss: 0.5805432096761114\n",
      "Train loss: 0.5807047915941577\n",
      "Train loss: 0.5806428959725702\n",
      "Train loss: 0.5807036140086194\n",
      "Train loss: 0.5809827872394522\n",
      "Train loss: 0.5808006066764977\n",
      "Train loss: 0.5809923952295992\n",
      "Train loss: 0.5811887502108388\n",
      "Train loss: 0.5812618845143089\n",
      "Train loss: 0.5813250210480089\n",
      "Train loss: 0.5810937855374573\n",
      "Train loss: 0.5811910322874355\n",
      "Train loss: 0.5814742701048783\n",
      "Train loss: 0.5812581833041931\n",
      "Train loss: 0.5815954053182206\n",
      "Train loss: 0.5814619808130407\n",
      "Train loss: 0.5814186826090234\n",
      "Train loss: 0.5813200691782122\n",
      "Train loss: 0.5812956273481203\n",
      "Train loss: 0.5812936729306862\n",
      "Train loss: 0.5814009949410376\n",
      "Train loss: 0.5811362334444553\n",
      "Train loss: 0.5811898382275213\n",
      "Train loss: 0.5812251740269552\n",
      "Train loss: 0.5812123984774578\n",
      "Train loss: 0.5810777393135962\n",
      "Train loss: 0.581146761580175\n",
      "Train loss: 0.5811880736320189\n",
      "Train loss: 0.5812967628853529\n",
      "Train loss: 0.5814019611341865\n",
      "Train loss: 0.5814367897358843\n",
      "Train loss: 0.581483782844687\n",
      "Train loss: 0.5813390929975555\n",
      "Train loss: 0.5813674601270626\n",
      "Train loss: 0.5813429822767694\n",
      "Train loss: 0.5812047231652308\n",
      "Train loss: 0.5813960976872048\n",
      "Train loss: 0.5812061870110161\n",
      "Train loss: 0.5813603060224045\n",
      "Train loss: 0.5814913825536494\n",
      "Train loss: 0.5813143196615769\n",
      "Train loss: 0.5813904489893077\n",
      "Train loss: 0.5810999108972086\n",
      "Train loss: 0.5810257489906145\n",
      "Train loss: 0.5810505107024078\n",
      "Train loss: 0.5812742620107446\n",
      "Train loss: 0.5813318673467791\n",
      "Train loss: 0.581326505031647\n",
      "Train loss: 0.5812289593652359\n",
      "Train loss: 0.5814130274950786\n",
      "Train loss: 0.5815547031049698\n",
      "Train loss: 0.581659056869636\n",
      "Train loss: 0.5817741848309347\n",
      "Train loss: 0.5815668939481149\n",
      "Train loss: 0.58146507639009\n",
      "Train loss: 0.5814930778657743\n",
      "Train loss: 0.5814746537037753\n",
      "Train loss: 0.5812354418298386\n",
      "Train loss: 0.581143686912019\n",
      "Train loss: 0.5811340342183012\n",
      "Train loss: 0.5810162255562638\n",
      "Train loss: 0.5807358393004\n",
      "Train loss: 0.5807355097846285\n",
      "Train loss: 0.5806423615293056\n",
      "Train loss: 0.5806824822613246\n",
      "Train loss: 0.5807912935887608\n",
      "Train loss: 0.580696494202448\n",
      "Train loss: 0.5807555653111516\n",
      "Train loss: 0.580707112038397\n",
      "Train loss: 0.5806062957993806\n",
      "Train loss: 0.5806535676308047\n",
      "Train loss: 0.5806694327776214\n",
      "Train loss: 0.5808227640250229\n",
      "Train loss: 0.5808611762174725\n",
      "Train loss: 0.5808272270123368\n",
      "Train loss: 0.5807031703043175\n",
      "Train loss: 0.580568167816776\n",
      "Train loss: 0.5804643260248705\n",
      "Train loss: 0.5803616128465285\n",
      "Train loss: 0.5802818453662909\n",
      "Train loss: 0.5800769366022146\n",
      "Train loss: 0.580179747541954\n",
      "Train loss: 0.5800874037393144\n",
      "Train loss: 0.5800645330438288\n",
      "Train loss: 0.5801324279249511\n",
      "Train loss: 0.5801604186831465\n",
      "Train loss: 0.5800528027552856\n",
      "Train loss: 0.580112365777559\n",
      "Train loss: 0.5799945292273296\n",
      "Train loss: 0.5800515706371732\n",
      "Train loss: 0.5801269195902775\n",
      "Train loss: 0.5800918989923508\n",
      "Train loss: 0.5800207910811835\n",
      "Train loss: 0.5801712762120848\n",
      "Train loss: 0.5801845083025718\n",
      "Train loss: 0.5799773951631019\n",
      "Train loss: 0.5800543631715614\n",
      "Train loss: 0.5800910568923536\n",
      "Train loss: 0.5799876083784381\n",
      "Train loss: 0.5800350709208126\n",
      "Train loss: 0.5799775654314918\n",
      "Train loss: 0.5799267880296224\n",
      "Train loss: 0.5798409790591373\n",
      "Train loss: 0.5800074591406117\n",
      "Train loss: 0.5799843283276416\n",
      "Train loss: 0.5800304938108845\n",
      "Train loss: 0.5798508305835667\n",
      "Train loss: 0.5798253549401243\n",
      "Train loss: 0.5797705058579447\n",
      "Train loss: 0.5798568178614387\n",
      "Train loss: 0.5798292293577009\n",
      "Train loss: 0.5798451464949539\n",
      "Train loss: 0.5798077975223379\n",
      "Train loss: 0.5798662891896257\n",
      "Train loss: 0.5798426025985621\n",
      "Train loss: 0.57978492347862\n",
      "Train loss: 0.5796772445829171\n",
      "Train loss: 0.579770149415883\n",
      "Train loss: 0.5797460675819214\n",
      "Train loss: 0.5796992352454167\n",
      "Train loss: 0.5797000570377828\n",
      "Train loss: 0.5796150832790011\n",
      "Train loss: 0.5795445362813254\n",
      "Train loss: 0.579606821800316\n",
      "Train loss: 0.579469623096722\n",
      "Train loss: 0.5792950469005168\n",
      "Train loss: 0.5792617635084423\n",
      "Train loss: 0.5792299682862483\n",
      "Train loss: 0.5792759389712893\n",
      "Train loss: 0.5792111596704684\n",
      "Train loss: 0.5791205927694673\n",
      "Train loss: 0.5790961349480014\n",
      "Train loss: 0.5791136455784818\n",
      "Train loss: 0.5791256066682126\n",
      "Train loss: 0.5791235465302581\n",
      "Train loss: 0.5791862790930783\n",
      "Train loss: 0.5791829444447321\n",
      "Train loss: 0.5791327534428371\n",
      "Train loss: 0.5790910975102574\n",
      "Train loss: 0.5790792132646001\n",
      "Train loss: 0.5789161545269913\n",
      "Train loss: 0.5788346278194607\n",
      "Train loss: 0.578676604857834\n",
      "Train loss: 0.5787285824437408\n",
      "Train loss: 0.5786709473908136\n",
      "Train loss: 0.5786744965882542\n",
      "Train loss: 0.5787095141026743\n",
      "Train loss: 0.5787477549481945\n",
      "Train loss: 0.5787198583720325\n",
      "Train loss: 0.5786691348356047\n",
      "Train loss: 0.5786766132942261\n",
      "Train loss: 0.5786035851529294\n",
      "Train loss: 0.5786926410178153\n",
      "Train loss: 0.5786749466016254\n",
      "Train loss: 0.5786736441047745\n",
      "Train loss: 0.5786102656931832\n",
      "Train loss: 0.5785395500104235\n",
      "Train loss: 0.5784672698383327\n",
      "Train loss: 0.5784753324986046\n",
      "Train loss: 0.5784548774081729\n",
      "Train loss: 0.5785265708592702\n",
      "Train loss: 0.5785038734156072\n",
      "Train loss: 0.5785083334662601\n",
      "Train loss: 0.5785333196516433\n",
      "Train loss: 0.5786263649047911\n",
      "Train loss: 0.5786216820184552\n",
      "Train loss: 0.5787146804912698\n",
      "Train loss: 0.5786589091718132\n",
      "Train loss: 0.5786313188350779\n",
      "Train loss: 0.5787098042878219\n",
      "Train loss: 0.5787118364893997\n",
      "Train loss: 0.5787377776212256\n",
      "Train loss: 0.5787816263717779\n",
      "Train loss: 0.578786620918196\n",
      "Train loss: 0.5787302795659437\n",
      "Train loss: 0.5787375473691885\n",
      "Train loss: 0.5787499702184685\n",
      "Train loss: 0.5787338051967577\n",
      "Train loss: 0.5786730209833186\n",
      "Train loss: 0.5786993463248676\n",
      "Train loss: 0.5787410166335515\n",
      "Train loss: 0.5787290607823048\n",
      "Train loss: 0.5786852557032683\n",
      "Train loss: 0.5786281811945242\n",
      "Train loss: 0.5785661193747229\n",
      "Train loss: 0.5785769510785682\n",
      "Train loss: 0.5785536158106505\n",
      "Train loss: 0.5786193549261919\n",
      "Train loss: 0.5785783548001219\n",
      "Train loss: 0.5786156464048201\n",
      "Train loss: 0.5786734568289665\n",
      "Train loss: 0.5787124649930269\n",
      "Train loss: 0.5786734267256584\n",
      "Train loss: 0.5786342358011712\n",
      "Train loss: 0.5786669676449179\n",
      "Train loss: 0.5786446334555307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.578680371494112\n",
      "Train loss: 0.5786429393427278\n",
      "Train loss: 0.5786065263664706\n",
      "Train loss: 0.5786329129003067\n",
      "Train loss: 0.5785360829728686\n",
      "Train loss: 0.5785081655971449\n",
      "Train loss: 0.5785127121190904\n",
      "Train loss: 0.578500312973945\n",
      "Train loss: 0.5784605532453168\n",
      "Train loss: 0.5784486328747237\n",
      "Train loss: 0.5785209835242994\n",
      "Train loss: 0.5784677059084404\n",
      "Train loss: 0.5785125658884824\n",
      "Train loss: 0.5785056811941794\n",
      "Train loss: 0.5785715318072432\n",
      "Train loss: 0.5785704934404848\n",
      "Train loss: 0.5785626967276594\n",
      "Train loss: 0.5785453200643768\n",
      "Train loss: 0.5785462136047433\n",
      "Train loss: 0.5785770243870091\n",
      "Train loss: 0.5786200906598197\n",
      "Train loss: 0.5786003351724368\n",
      "Train loss: 0.5785509899676453\n",
      "Train loss: 0.5785839357586555\n",
      "Train loss: 0.5785978437209937\n",
      "Train loss: 0.5785398843442621\n",
      "Train loss: 0.5784922874106091\n",
      "Train loss: 0.5784846345914793\n",
      "Train loss: 0.5784782925088165\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.696301206946373\n",
      "Val loss: 0.6216366489728292\n",
      "Val loss: 0.593537660581725\n",
      "Val loss: 0.5936776917231711\n",
      "Val loss: 0.5936665870249271\n",
      "Val loss: 0.5962659870756084\n",
      "Val loss: 0.5894434390699163\n",
      "Val loss: 0.5838205974835616\n",
      "Val loss: 0.5786059593612497\n",
      "Val loss: 0.578386380964396\n",
      "Val loss: 0.5795570115248362\n",
      "Val loss: 0.5784215371487504\n",
      "Val loss: 0.5780032137408853\n",
      "Val loss: 0.5778561852116516\n",
      "Val loss: 0.5769297492665213\n",
      "Val loss: 0.5754912205134766\n",
      "Val loss: 0.574121932898249\n",
      "Val loss: 0.573593956366014\n",
      "Val loss: 0.5749935453876536\n",
      "Val loss: 0.5725643279576542\n",
      "Val loss: 0.5729504898190498\n",
      "Val loss: 0.5717754763200742\n",
      "Val loss: 0.5705504014826658\n",
      "Val loss: 0.5700138807296753\n",
      "Val loss: 0.5698150425188003\n",
      "Val loss: 0.5687278252701427\n",
      "Val loss: 0.5685610568790294\n",
      "Val loss: 0.567264412590068\n",
      "Val loss: 0.5667158992340168\n",
      "Val loss: 0.5670054852962494\n",
      "Val loss: 0.5668662262040299\n",
      "Val loss: 0.5668172253377782\n",
      "Val loss: 0.5664371333834601\n",
      "Val loss: 0.5658973428977312\n",
      "Val loss: 0.5656627891049988\n",
      "Val loss: 0.5655525215178229\n",
      "Val loss: 0.5639531884504401\n",
      "Val loss: 0.5638638517213246\n",
      "Val loss: 0.5645156917498284\n",
      "Val loss: 0.5637335073408769\n",
      "Val loss: 0.5641237985269696\n",
      "Val loss: 0.5636767832000861\n",
      "Val loss: 0.563085924659934\n",
      "Val loss: 0.563157627707747\n",
      "Val loss: 0.5625394403136202\n",
      "Val loss: 0.5628889105465735\n",
      "Val loss: 0.5627221736388329\n",
      "Val loss: 0.5627278619480931\n",
      "Val loss: 0.5629349743000797\n",
      "Val loss: 0.5630730757990994\n",
      "Val loss: 0.5625516509446572\n",
      "Val loss: 0.5619319485191213\n",
      "Val loss: 0.5617360557344827\n",
      "Val loss: 0.5614368650771429\n",
      "Val loss: 0.5614914983293436\n",
      "Val loss: 0.5609219597659231\n",
      "Val loss: 0.5610569308341389\n",
      "Val loss: 0.5611237875318033\n",
      "Val loss: 0.5611703420171932\n",
      "Val loss: 0.5610440512963362\n",
      "Val loss: 0.5612510307446906\n",
      "Val loss: 0.5612368608755587\n",
      "Val loss: 0.560704890614862\n",
      "Val loss: 0.5603926022037817\n",
      "Val loss: 0.5602837255523528\n",
      "Val loss: 0.5603982625942462\n",
      "Val loss: 0.5602709525181148\n",
      "Val loss: 0.5598123042808522\n",
      "Val loss: 0.5606647129495477\n",
      "Val loss: 0.5605019318169372\n",
      "Val loss: 0.5598703179992525\n",
      "Val loss: 0.5598358525207116\n",
      "Val loss: 0.5595910855374493\n",
      "Val loss: 0.5598559251968777\n",
      "Val loss: 0.5599297387077209\n",
      "Val loss: 0.5596542473204218\n",
      "Val loss: 0.5598355679928014\n",
      "Val loss: 0.5596104587564739\n",
      "Val loss: 0.5591370998905395\n",
      "Val loss: 0.5594002002462707\n",
      "Val loss: 0.5597239059977012\n",
      "Val loss: 0.5592450667127129\n",
      "Val loss: 0.5593605579961325\n",
      "Val loss: 0.5590592097837771\n",
      "Val loss: 0.5587305718294855\n",
      "Val loss: 0.558902551035781\n",
      "Val loss: 0.55872357873598\n",
      "Val loss: 0.5587576094140767\n",
      "Val loss: 0.5582146576798714\n",
      "Val loss: 0.557991866876925\n",
      "Val loss: 0.5581740224282647\n",
      "Val loss: 0.5581575218498837\n",
      "Val loss: 0.5582569844635397\n",
      "Val loss: 0.5582028797059171\n",
      "Val loss: 0.557932724240963\n",
      "Val loss: 0.5581938654868737\n",
      "Val loss: 0.5582651542114817\n",
      "Val loss: 0.5582886292524865\n",
      "Val loss: 0.5583566996732704\n",
      "Val loss: 0.5588421413081442\n",
      "Val loss: 0.5586993291027962\n",
      "Val loss: 0.5585626378977463\n",
      "Val loss: 0.5589165408091786\n",
      "Val loss: 0.5586724899178066\n",
      "Val loss: 0.558571323297406\n",
      "Val loss: 0.5584576064562302\n",
      "Val loss: 0.5581583721695768\n",
      "Val loss: 0.5581206549408263\n",
      "Val loss: 0.557899275849409\n",
      "Val loss: 0.5580782492412678\n",
      "Val loss: 0.558158365720446\n",
      "Val loss: 0.5582057733441934\n",
      "Val loss: 0.5581998374339536\n",
      "Val loss: 0.5583861398466442\n",
      "Val loss: 0.5582357997055253\n",
      "Val loss: 0.5583441502483809\n",
      "Val loss: 0.5584786684341627\n",
      "Val loss: 0.5585148988231536\n",
      "Val loss: 0.5586130519707998\n",
      "Val loss: 0.5588744252870397\n",
      "Val loss: 0.5590653153050025\n",
      "Val loss: 0.5593505372554798\n",
      "Val loss: 0.5594313516484798\n",
      "Val loss: 0.5594396325421064\n",
      "Val loss: 0.559560693275088\n",
      "Val loss: 0.5597460877819547\n",
      "Val loss: 0.5596401470974793\n",
      "\n",
      "starting Epoch 11\n",
      "Training...\n",
      "Train loss: 0.5886903138537156\n",
      "Train loss: 0.5841920796113137\n",
      "Train loss: 0.5866387814788495\n",
      "Train loss: 0.5856989316547974\n",
      "Train loss: 0.5799977993122255\n",
      "Train loss: 0.5777005721040133\n",
      "Train loss: 0.5807636931216974\n",
      "Train loss: 0.5816535676050486\n",
      "Train loss: 0.5824214333262523\n",
      "Train loss: 0.5798733471026972\n",
      "Train loss: 0.5819695556544822\n",
      "Train loss: 0.5802651739768903\n",
      "Train loss: 0.5788022968990002\n",
      "Train loss: 0.5776181411144982\n",
      "Train loss: 0.577622849805698\n",
      "Train loss: 0.577118332483178\n",
      "Train loss: 0.5784739345575856\n",
      "Train loss: 0.5789145862159623\n",
      "Train loss: 0.5781846697538069\n",
      "Train loss: 0.5800779971263761\n",
      "Train loss: 0.5807127227077621\n",
      "Train loss: 0.5801939953432539\n",
      "Train loss: 0.5801356630257791\n",
      "Train loss: 0.5797091897320399\n",
      "Train loss: 0.5798643714559819\n",
      "Train loss: 0.5795995330649764\n",
      "Train loss: 0.5801778029864706\n",
      "Train loss: 0.5805321263285997\n",
      "Train loss: 0.5806247882274767\n",
      "Train loss: 0.5799126982788411\n",
      "Train loss: 0.579458337126718\n",
      "Train loss: 0.5788080527189192\n",
      "Train loss: 0.5786284957199791\n",
      "Train loss: 0.5790388728539968\n",
      "Train loss: 0.579298295304499\n",
      "Train loss: 0.578851838561192\n",
      "Train loss: 0.5790422908427428\n",
      "Train loss: 0.5794667623181274\n",
      "Train loss: 0.5797095477427996\n",
      "Train loss: 0.5796652325476812\n",
      "Train loss: 0.5792771998488132\n",
      "Train loss: 0.5792780880138048\n",
      "Train loss: 0.5791651658254119\n",
      "Train loss: 0.578892513245039\n",
      "Train loss: 0.5791282917355272\n",
      "Train loss: 0.5795247830101922\n",
      "Train loss: 0.579435748001511\n",
      "Train loss: 0.5790221629910971\n",
      "Train loss: 0.5791035400165113\n",
      "Train loss: 0.5789261945673415\n",
      "Train loss: 0.5792002269926904\n",
      "Train loss: 0.5792571392279616\n",
      "Train loss: 0.5794326397638708\n",
      "Train loss: 0.5790249038283973\n",
      "Train loss: 0.578846231387679\n",
      "Train loss: 0.578556064572901\n",
      "Train loss: 0.5790235172561431\n",
      "Train loss: 0.5793134264127255\n",
      "Train loss: 0.5790964638762195\n",
      "Train loss: 0.5788016016529042\n",
      "Train loss: 0.5787552312762156\n",
      "Train loss: 0.578807128808304\n",
      "Train loss: 0.5787966485010243\n",
      "Train loss: 0.5787838463255723\n",
      "Train loss: 0.5789790880092756\n",
      "Train loss: 0.5789713538488355\n",
      "Train loss: 0.5790782486929477\n",
      "Train loss: 0.5792827284932575\n",
      "Train loss: 0.579467574935447\n",
      "Train loss: 0.5796176218364476\n",
      "Train loss: 0.5795754620817196\n",
      "Train loss: 0.5795325338426276\n",
      "Train loss: 0.5792295604515272\n",
      "Train loss: 0.5793357108032647\n",
      "Train loss: 0.5796038764289732\n",
      "Train loss: 0.5794405392551987\n",
      "Train loss: 0.5794365291978106\n",
      "Train loss: 0.5794740434141018\n",
      "Train loss: 0.5796712187176041\n",
      "Train loss: 0.5797587981479029\n",
      "Train loss: 0.5796664379642951\n",
      "Train loss: 0.5799271278151512\n",
      "Train loss: 0.5800021324192206\n",
      "Train loss: 0.5800710609443419\n",
      "Train loss: 0.5799705266356118\n",
      "Train loss: 0.5800530987294903\n",
      "Train loss: 0.5801708849597896\n",
      "Train loss: 0.580183785686173\n",
      "Train loss: 0.5802267832391512\n",
      "Train loss: 0.5801407300213299\n",
      "Train loss: 0.5800066759391038\n",
      "Train loss: 0.5797205739950861\n",
      "Train loss: 0.5797097668845272\n",
      "Train loss: 0.5795488926627143\n",
      "Train loss: 0.5797249059604306\n",
      "Train loss: 0.5796905174859183\n",
      "Train loss: 0.5796503383453019\n",
      "Train loss: 0.5797133518914412\n",
      "Train loss: 0.579525353226052\n",
      "Train loss: 0.5794123139841787\n",
      "Train loss: 0.5795328553328719\n",
      "Train loss: 0.5798042068415965\n",
      "Train loss: 0.5796894764164919\n",
      "Train loss: 0.5798384975131284\n",
      "Train loss: 0.5796806213826211\n",
      "Train loss: 0.5796749205761442\n",
      "Train loss: 0.5796746899726068\n",
      "Train loss: 0.5797629652336936\n",
      "Train loss: 0.579742061101608\n",
      "Train loss: 0.5796669335894392\n",
      "Train loss: 0.5796811648079166\n",
      "Train loss: 0.5797745073384715\n",
      "Train loss: 0.5796837802611922\n",
      "Train loss: 0.5799055540049061\n",
      "Train loss: 0.5799096791756677\n",
      "Train loss: 0.5797323514773651\n",
      "Train loss: 0.579724099377171\n",
      "Train loss: 0.5799608722537591\n",
      "Train loss: 0.5800229484115944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5798989516116521\n",
      "Train loss: 0.5798829149958417\n",
      "Train loss: 0.5799241594942733\n",
      "Train loss: 0.5798768919747551\n",
      "Train loss: 0.5798549789029199\n",
      "Train loss: 0.5798326279698204\n",
      "Train loss: 0.5797147689333223\n",
      "Train loss: 0.5797801963125171\n",
      "Train loss: 0.5796386970584865\n",
      "Train loss: 0.579594166380992\n",
      "Train loss: 0.579626423010417\n",
      "Train loss: 0.5795579181686072\n",
      "Train loss: 0.5795662017110714\n",
      "Train loss: 0.5797046372073626\n",
      "Train loss: 0.5794467667048883\n",
      "Train loss: 0.5794475233280999\n",
      "Train loss: 0.5793894386304657\n",
      "Train loss: 0.5793721268280617\n",
      "Train loss: 0.579419104430329\n",
      "Train loss: 0.579250665470695\n",
      "Train loss: 0.5792288821311711\n",
      "Train loss: 0.5791187230451347\n",
      "Train loss: 0.5789358882825951\n",
      "Train loss: 0.5788974629264897\n",
      "Train loss: 0.5789344293783837\n",
      "Train loss: 0.5790316603972444\n",
      "Train loss: 0.5791470349427158\n",
      "Train loss: 0.5791001313192661\n",
      "Train loss: 0.5792271448132476\n",
      "Train loss: 0.5791979081661119\n",
      "Train loss: 0.5792529287736549\n",
      "Train loss: 0.5792895779087671\n",
      "Train loss: 0.5793640057422409\n",
      "Train loss: 0.5793018531335573\n",
      "Train loss: 0.5794109008497292\n",
      "Train loss: 0.5794787147076679\n",
      "Train loss: 0.5793284805358393\n",
      "Train loss: 0.5793473008674739\n",
      "Train loss: 0.579368544987162\n",
      "Train loss: 0.579490086442834\n",
      "Train loss: 0.5793395005713854\n",
      "Train loss: 0.5794055036096404\n",
      "Train loss: 0.5793566026320315\n",
      "Train loss: 0.5792171679215403\n",
      "Train loss: 0.5792519508083626\n",
      "Train loss: 0.5791861852413598\n",
      "Train loss: 0.5791607972332138\n",
      "Train loss: 0.579094163732066\n",
      "Train loss: 0.5790555802847381\n",
      "Train loss: 0.5792192531940887\n",
      "Train loss: 0.5792645733561296\n",
      "Train loss: 0.5792949966918519\n",
      "Train loss: 0.5793361303833343\n",
      "Train loss: 0.5793800908349503\n",
      "Train loss: 0.5794671162444649\n",
      "Train loss: 0.5794254212881641\n",
      "Train loss: 0.579407094782745\n",
      "Train loss: 0.5794190575288966\n",
      "Train loss: 0.579349412764683\n",
      "Train loss: 0.5792533431715324\n",
      "Train loss: 0.5793265678479163\n",
      "Train loss: 0.5793786537304673\n",
      "Train loss: 0.5792632487145188\n",
      "Train loss: 0.5792923533271395\n",
      "Train loss: 0.5792574769827166\n",
      "Train loss: 0.5792091907150329\n",
      "Train loss: 0.5792853744416572\n",
      "Train loss: 0.5791533474904326\n",
      "Train loss: 0.5791413739311724\n",
      "Train loss: 0.5791301397805366\n",
      "Train loss: 0.579055925534568\n",
      "Train loss: 0.5790409741991007\n",
      "Train loss: 0.5790069191894919\n",
      "Train loss: 0.5789737357519302\n",
      "Train loss: 0.5790108760459179\n",
      "Train loss: 0.5790914005195523\n",
      "Train loss: 0.5791087500916418\n",
      "Train loss: 0.5791536165799971\n",
      "Train loss: 0.5791983825671428\n",
      "Train loss: 0.5791543186774473\n",
      "Train loss: 0.5792584056614577\n",
      "Train loss: 0.5791936632737973\n",
      "Train loss: 0.5791022109347838\n",
      "Train loss: 0.5790542029312427\n",
      "Train loss: 0.5790392770051781\n",
      "Train loss: 0.5789934321928967\n",
      "Train loss: 0.5790126104831232\n",
      "Train loss: 0.5790352922083244\n",
      "Train loss: 0.5790942595838554\n",
      "Train loss: 0.5791610510947174\n",
      "Train loss: 0.5791709790856647\n",
      "Train loss: 0.5791979649672154\n",
      "Train loss: 0.5792545707938974\n",
      "Train loss: 0.5792544940864852\n",
      "Train loss: 0.5793100322384955\n",
      "Train loss: 0.5792945762530791\n",
      "Train loss: 0.579156065510224\n",
      "Train loss: 0.5792333998144923\n",
      "Train loss: 0.5791662797164304\n",
      "Train loss: 0.5791527672455338\n",
      "Train loss: 0.5791724304240735\n",
      "Train loss: 0.5791279184761617\n",
      "Train loss: 0.5791166188897354\n",
      "Train loss: 0.5791944440543638\n",
      "Train loss: 0.5791758669157278\n",
      "Train loss: 0.5791381023001156\n",
      "Train loss: 0.5791930972830757\n",
      "Train loss: 0.579227575451615\n",
      "Train loss: 0.5792622933604262\n",
      "Train loss: 0.5791185501036256\n",
      "Train loss: 0.5790508003319365\n",
      "Train loss: 0.5789948562750906\n",
      "Train loss: 0.5789740684217974\n",
      "Train loss: 0.5789037590235133\n",
      "Train loss: 0.5788641106142247\n",
      "Train loss: 0.5789209553017771\n",
      "Train loss: 0.5788879595297478\n",
      "Train loss: 0.5789419185652414\n",
      "Train loss: 0.5789189495993252\n",
      "Train loss: 0.5789864018986629\n",
      "Train loss: 0.5789759506109134\n",
      "Train loss: 0.5789881733275717\n",
      "Train loss: 0.5789380758957666\n",
      "Train loss: 0.5788820649670289\n",
      "Train loss: 0.5789529633438961\n",
      "Train loss: 0.5789341570881343\n",
      "Train loss: 0.578817677675987\n",
      "Train loss: 0.5787196032707448\n",
      "Train loss: 0.5786270373505579\n",
      "Train loss: 0.5785494197394192\n",
      "Train loss: 0.5785697602133533\n",
      "Train loss: 0.5785539128861595\n",
      "Train loss: 0.5785786685110392\n",
      "Train loss: 0.5785135803109078\n",
      "Train loss: 0.578432693880922\n",
      "Train loss: 0.5784130713756843\n",
      "Train loss: 0.5784559022707267\n",
      "Train loss: 0.5784179877448485\n",
      "Train loss: 0.5784368167022421\n",
      "Train loss: 0.578429942714314\n",
      "Train loss: 0.5783596853839363\n",
      "Train loss: 0.578289373141638\n",
      "Train loss: 0.5783316321581414\n",
      "Train loss: 0.578302485195034\n",
      "Train loss: 0.5782906139217604\n",
      "Train loss: 0.5782616191718866\n",
      "Train loss: 0.578249293417831\n",
      "Train loss: 0.5783488101214812\n",
      "Train loss: 0.5783764498628416\n",
      "Train loss: 0.5783041431782392\n",
      "Train loss: 0.5782554523274509\n",
      "Train loss: 0.57822537356185\n",
      "Train loss: 0.5781987262881181\n",
      "Train loss: 0.5781382730721685\n",
      "Train loss: 0.5781409583358917\n",
      "Train loss: 0.5781210127755239\n",
      "Train loss: 0.5781083836635832\n",
      "Train loss: 0.5780898288045685\n",
      "Train loss: 0.5780848133669073\n",
      "Train loss: 0.5781207996513879\n",
      "Train loss: 0.5781100870062356\n",
      "Train loss: 0.5780693296807838\n",
      "Train loss: 0.5781399335214741\n",
      "Train loss: 0.5781268337881382\n",
      "Train loss: 0.5781393447724588\n",
      "Train loss: 0.5781551599031918\n",
      "Train loss: 0.5782192619577698\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6979076862335205\n",
      "Val loss: 0.6271676156255934\n",
      "Val loss: 0.6009257776396615\n",
      "Val loss: 0.5986575296050624\n",
      "Val loss: 0.5976205095648766\n",
      "Val loss: 0.6007151418718798\n",
      "Val loss: 0.595242456478231\n",
      "Val loss: 0.5900671650201846\n",
      "Val loss: 0.5863635959950361\n",
      "Val loss: 0.5862688020784028\n",
      "Val loss: 0.5865325442066899\n",
      "Val loss: 0.5856640551049831\n",
      "Val loss: 0.5844336785376072\n",
      "Val loss: 0.5842362720033397\n",
      "Val loss: 0.5834391624540896\n",
      "Val loss: 0.58216425889655\n",
      "Val loss: 0.5809428337074461\n",
      "Val loss: 0.579799283756299\n",
      "Val loss: 0.5803973934751876\n",
      "Val loss: 0.5785105710077767\n",
      "Val loss: 0.5792002746692071\n",
      "Val loss: 0.5779266037525387\n",
      "Val loss: 0.5771607978824984\n",
      "Val loss: 0.5769271605155047\n",
      "Val loss: 0.5767959589439053\n",
      "Val loss: 0.5757494561893995\n",
      "Val loss: 0.5755777623671204\n",
      "Val loss: 0.5745308590021064\n",
      "Val loss: 0.5736393156564898\n",
      "Val loss: 0.573795369207459\n",
      "Val loss: 0.5733458904863952\n",
      "Val loss: 0.5733443208085666\n",
      "Val loss: 0.5734028134767603\n",
      "Val loss: 0.5726708302483756\n",
      "Val loss: 0.5722980944589636\n",
      "Val loss: 0.5721309707817419\n",
      "Val loss: 0.57077223677998\n",
      "Val loss: 0.570629407804479\n",
      "Val loss: 0.5711753475911838\n",
      "Val loss: 0.5705312286789094\n",
      "Val loss: 0.5707611393402604\n",
      "Val loss: 0.5703326636239102\n",
      "Val loss: 0.5697358518003304\n",
      "Val loss: 0.5699311666292687\n",
      "Val loss: 0.5695618486830166\n",
      "Val loss: 0.5699333199767567\n",
      "Val loss: 0.5697289073569143\n",
      "Val loss: 0.5696134038550086\n",
      "Val loss: 0.5696187642265539\n",
      "Val loss: 0.5696763604520315\n",
      "Val loss: 0.5691395893575638\n",
      "Val loss: 0.5684467213494437\n",
      "Val loss: 0.5683997067989726\n",
      "Val loss: 0.5680492174669712\n",
      "Val loss: 0.5680573660961903\n",
      "Val loss: 0.5675599963861554\n",
      "Val loss: 0.5676595896062716\n",
      "Val loss: 0.5677968748300546\n",
      "Val loss: 0.5678882023104194\n",
      "Val loss: 0.5677224279048052\n",
      "Val loss: 0.5679219424920646\n",
      "Val loss: 0.5678179484933711\n",
      "Val loss: 0.5674110200184925\n",
      "Val loss: 0.5671871520321945\n",
      "Val loss: 0.5670002867226247\n",
      "Val loss: 0.5669905243492417\n",
      "Val loss: 0.5668850841814886\n",
      "Val loss: 0.5665287140723878\n",
      "Val loss: 0.567389558134384\n",
      "Val loss: 0.5672827869397521\n",
      "Val loss: 0.5667592894054402\n",
      "Val loss: 0.5667816157294515\n",
      "Val loss: 0.5664108226096237\n",
      "Val loss: 0.5665983182143389\n",
      "Val loss: 0.5666535821031121\n",
      "Val loss: 0.5662620183188557\n",
      "Val loss: 0.5663592763400326\n",
      "Val loss: 0.5661212120846795\n",
      "Val loss: 0.5657323861333925\n",
      "Val loss: 0.5657771968453151\n",
      "Val loss: 0.5660413138494633\n",
      "Val loss: 0.5656538158290835\n",
      "Val loss: 0.565867205892784\n",
      "Val loss: 0.5656040305880772\n",
      "Val loss: 0.5654277038883488\n",
      "Val loss: 0.5655577748944431\n",
      "Val loss: 0.5653613421911469\n",
      "Val loss: 0.5652974445483137\n",
      "Val loss: 0.5649462043567821\n",
      "Val loss: 0.5647735192807586\n",
      "Val loss: 0.564886458579139\n",
      "Val loss: 0.5649178239125312\n",
      "Val loss: 0.5649993913569327\n",
      "Val loss: 0.564967431429861\n",
      "Val loss: 0.5648137661097925\n",
      "Val loss: 0.5649339911957624\n",
      "Val loss: 0.5649767713236414\n",
      "Val loss: 0.5649401197526108\n",
      "Val loss: 0.5648986009451059\n",
      "Val loss: 0.565316397704199\n",
      "Val loss: 0.5652776516619182\n",
      "Val loss: 0.5650821885566112\n",
      "Val loss: 0.5653189921657399\n",
      "Val loss: 0.5651203113713935\n",
      "Val loss: 0.5650274158661602\n",
      "Val loss: 0.5649925979639496\n",
      "Val loss: 0.5647451439488693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5646919540107139\n",
      "Val loss: 0.5645162244053448\n",
      "Val loss: 0.5646284105347805\n",
      "Val loss: 0.564643437036108\n",
      "Val loss: 0.5646922521173208\n",
      "Val loss: 0.5646390817161148\n",
      "Val loss: 0.5648110833130319\n",
      "Val loss: 0.5646455864769241\n",
      "Val loss: 0.5647687089978516\n",
      "Val loss: 0.5648748913958465\n",
      "Val loss: 0.5649084370387228\n",
      "Val loss: 0.5650376204388712\n",
      "Val loss: 0.5651821771229248\n",
      "Val loss: 0.5653431850257299\n",
      "Val loss: 0.5655638631830857\n",
      "Val loss: 0.5657147795738537\n",
      "Val loss: 0.5657478130402973\n",
      "Val loss: 0.5659186131774615\n",
      "Val loss: 0.5660323214928941\n",
      "Val loss: 0.5659400455489143\n",
      "\n",
      "starting Epoch 12\n",
      "Training...\n",
      "Train loss: 0.5944328010082245\n",
      "Train loss: 0.5842945575714111\n",
      "Train loss: 0.590156577401242\n",
      "Train loss: 0.586635072774525\n",
      "Train loss: 0.5832808607756489\n",
      "Train loss: 0.5852487367742202\n",
      "Train loss: 0.584289727022322\n",
      "Train loss: 0.5846430096986159\n",
      "Train loss: 0.5839103540228732\n",
      "Train loss: 0.5827409067944666\n",
      "Train loss: 0.5814842290529921\n",
      "Train loss: 0.5809381883513478\n",
      "Train loss: 0.5817466121612829\n",
      "Train loss: 0.581624545397297\n",
      "Train loss: 0.5826516594974492\n",
      "Train loss: 0.5812912095676769\n",
      "Train loss: 0.5823066729535747\n",
      "Train loss: 0.5819413657806046\n",
      "Train loss: 0.5819927620699035\n",
      "Train loss: 0.5809766556834218\n",
      "Train loss: 0.5801841933146866\n",
      "Train loss: 0.5802757289931009\n",
      "Train loss: 0.5796488215071444\n",
      "Train loss: 0.578604086455821\n",
      "Train loss: 0.5783723046282728\n",
      "Train loss: 0.5782410186609551\n",
      "Train loss: 0.5785118538285009\n",
      "Train loss: 0.5784535248079112\n",
      "Train loss: 0.5785473399829371\n",
      "Train loss: 0.577924485795685\n",
      "Train loss: 0.5773250027792904\n",
      "Train loss: 0.5769725465252179\n",
      "Train loss: 0.5770856527800987\n",
      "Train loss: 0.5773168585876302\n",
      "Train loss: 0.576986306684041\n",
      "Train loss: 0.5770710789195685\n",
      "Train loss: 0.5771380017671598\n",
      "Train loss: 0.5771446530762397\n",
      "Train loss: 0.5774766347145085\n",
      "Train loss: 0.5771527151962992\n",
      "Train loss: 0.5773311932372232\n",
      "Train loss: 0.5771073043630575\n",
      "Train loss: 0.5771664991992178\n",
      "Train loss: 0.5764098611572361\n",
      "Train loss: 0.5767779073142368\n",
      "Train loss: 0.5770012117897985\n",
      "Train loss: 0.5773851504277624\n",
      "Train loss: 0.5772642566899189\n",
      "Train loss: 0.5774781247020621\n",
      "Train loss: 0.5776361784001848\n",
      "Train loss: 0.5775325539301609\n",
      "Train loss: 0.577191510719101\n",
      "Train loss: 0.5773629626088597\n",
      "Train loss: 0.5772752836415677\n",
      "Train loss: 0.5774307849734778\n",
      "Train loss: 0.5776442510938943\n",
      "Train loss: 0.5775921373538955\n",
      "Train loss: 0.5775258795159762\n",
      "Train loss: 0.5775586355025734\n",
      "Train loss: 0.5777592449709213\n",
      "Train loss: 0.5780025273194755\n",
      "Train loss: 0.5775362391256343\n",
      "Train loss: 0.5772688850417225\n",
      "Train loss: 0.5773687770909123\n",
      "Train loss: 0.5772683775773684\n",
      "Train loss: 0.5771312046403502\n",
      "Train loss: 0.5769153976057607\n",
      "Train loss: 0.5769125311974658\n",
      "Train loss: 0.5766741315077145\n",
      "Train loss: 0.5768429634611977\n",
      "Train loss: 0.5768318095200493\n",
      "Train loss: 0.5768867097769121\n",
      "Train loss: 0.5767619644557882\n",
      "Train loss: 0.5769408513074634\n",
      "Train loss: 0.5769160481354966\n",
      "Train loss: 0.5769316975500648\n",
      "Train loss: 0.5770612285848249\n",
      "Train loss: 0.5770454119802515\n",
      "Train loss: 0.5770808207490461\n",
      "Train loss: 0.57729568378563\n",
      "Train loss: 0.5773116281512639\n",
      "Train loss: 0.577561525344994\n",
      "Train loss: 0.5774459251623113\n",
      "Train loss: 0.5774791561790419\n",
      "Train loss: 0.577422316860914\n",
      "Train loss: 0.5773666576114486\n",
      "Train loss: 0.5776339030560576\n",
      "Train loss: 0.5776712975934394\n",
      "Train loss: 0.5776220589892928\n",
      "Train loss: 0.5777176652039469\n",
      "Train loss: 0.5777442259667928\n",
      "Train loss: 0.577405923893427\n",
      "Train loss: 0.5770785728406368\n",
      "Train loss: 0.5771205348179276\n",
      "Train loss: 0.577298623941772\n",
      "Train loss: 0.577395605848381\n",
      "Train loss: 0.5775923711728533\n",
      "Train loss: 0.5773587512111956\n",
      "Train loss: 0.5774789715036109\n",
      "Train loss: 0.5775392934672531\n",
      "Train loss: 0.5777245184582847\n",
      "Train loss: 0.5776397334324956\n",
      "Train loss: 0.5778624863662785\n",
      "Train loss: 0.5779550965586897\n",
      "Train loss: 0.5779223464085068\n",
      "Train loss: 0.5777387996786548\n",
      "Train loss: 0.5777239827504499\n",
      "Train loss: 0.5778244925130568\n",
      "Train loss: 0.5779451773822062\n",
      "Train loss: 0.5779094576916949\n",
      "Train loss: 0.5778105953836721\n",
      "Train loss: 0.5779262079810287\n",
      "Train loss: 0.5780995654790684\n",
      "Train loss: 0.5779795524005882\n",
      "Train loss: 0.5778124306372011\n",
      "Train loss: 0.5777711207211763\n",
      "Train loss: 0.5778018621717472\n",
      "Train loss: 0.5776060081579363\n",
      "Train loss: 0.5777852157676555\n",
      "Train loss: 0.5777475923585514\n",
      "Train loss: 0.5777935974851233\n",
      "Train loss: 0.5778621213511396\n",
      "Train loss: 0.5779233952274454\n",
      "Train loss: 0.5779236675318618\n",
      "Train loss: 0.5779148044110107\n",
      "Train loss: 0.5780355495065391\n",
      "Train loss: 0.5780475250775066\n",
      "Train loss: 0.5779486274104325\n",
      "Train loss: 0.5781085404049939\n",
      "Train loss: 0.5780106182738697\n",
      "Train loss: 0.5780524077329894\n",
      "Train loss: 0.5780035476424379\n",
      "Train loss: 0.5781146016682212\n",
      "Train loss: 0.578112251247297\n",
      "Train loss: 0.5781143606592435\n",
      "Train loss: 0.5781355494960084\n",
      "Train loss: 0.5780878535540881\n",
      "Train loss: 0.5780373863138639\n",
      "Train loss: 0.5780447715154095\n",
      "Train loss: 0.5779375734713385\n",
      "Train loss: 0.5778508804905029\n",
      "Train loss: 0.5778986006796717\n",
      "Train loss: 0.5778126330820152\n",
      "Train loss: 0.5778320564934677\n",
      "Train loss: 0.5778141187548267\n",
      "Train loss: 0.5777260921780644\n",
      "Train loss: 0.5777124960068011\n",
      "Train loss: 0.5777749275689835\n",
      "Train loss: 0.5777881479315328\n",
      "Train loss: 0.5777266964132526\n",
      "Train loss: 0.5776520930366352\n",
      "Train loss: 0.5776506507620289\n",
      "Train loss: 0.5776715619418309\n",
      "Train loss: 0.5776116051004861\n",
      "Train loss: 0.5775809331107963\n",
      "Train loss: 0.577556593252245\n",
      "Train loss: 0.5775890299452836\n",
      "Train loss: 0.5775705992741538\n",
      "Train loss: 0.5776495073868366\n",
      "Train loss: 0.5775925075031512\n",
      "Train loss: 0.577598460224631\n",
      "Train loss: 0.5776796846358119\n",
      "Train loss: 0.577784104585355\n",
      "Train loss: 0.5777541594684905\n",
      "Train loss: 0.5777939713590251\n",
      "Train loss: 0.5778122007954584\n",
      "Train loss: 0.5777768660855529\n",
      "Train loss: 0.577744156060101\n",
      "Train loss: 0.577593514034825\n",
      "Train loss: 0.5776064422646843\n",
      "Train loss: 0.5774657909338081\n",
      "Train loss: 0.5775035627188465\n",
      "Train loss: 0.5774580384307115\n",
      "Train loss: 0.5775010834465976\n",
      "Train loss: 0.5774369913021065\n",
      "Train loss: 0.5775437119604004\n",
      "Train loss: 0.5775108132432429\n",
      "Train loss: 0.5775572955189678\n",
      "Train loss: 0.5776183056278448\n",
      "Train loss: 0.5776282498309201\n",
      "Train loss: 0.577671741189203\n",
      "Train loss: 0.5777393722720905\n",
      "Train loss: 0.5778115325838726\n",
      "Train loss: 0.5778407248328904\n",
      "Train loss: 0.5778248309041719\n",
      "Train loss: 0.5777864795549803\n",
      "Train loss: 0.5776713167992975\n",
      "Train loss: 0.5776394867427681\n",
      "Train loss: 0.5777237078095593\n",
      "Train loss: 0.577765025804469\n",
      "Train loss: 0.5777700634706892\n",
      "Train loss: 0.5777624467522193\n",
      "Train loss: 0.5777787757556221\n",
      "Train loss: 0.5776990711243382\n",
      "Train loss: 0.5777439470320489\n",
      "Train loss: 0.5778411237157465\n",
      "Train loss: 0.5778173562021055\n",
      "Train loss: 0.5778672284596976\n",
      "Train loss: 0.5779444974371764\n",
      "Train loss: 0.5779561794871358\n",
      "Train loss: 0.5778536516269305\n",
      "Train loss: 0.5779240200884952\n",
      "Train loss: 0.5779101994273168\n",
      "Train loss: 0.5778979038704141\n",
      "Train loss: 0.57797161307414\n",
      "Train loss: 0.577968580322261\n",
      "Train loss: 0.5779533757913455\n",
      "Train loss: 0.5778872907434011\n",
      "Train loss: 0.5778424096937583\n",
      "Train loss: 0.5779446440030915\n",
      "Train loss: 0.5779781953634089\n",
      "Train loss: 0.5780215194608334\n",
      "Train loss: 0.5781070238745081\n",
      "Train loss: 0.5780615470250039\n",
      "Train loss: 0.5780460028405465\n",
      "Train loss: 0.5781500756423157\n",
      "Train loss: 0.5781676148071517\n",
      "Train loss: 0.5781896515405623\n",
      "Train loss: 0.5782179143173451\n",
      "Train loss: 0.5782335941209986\n",
      "Train loss: 0.5782524074235365\n",
      "Train loss: 0.5782305136621737\n",
      "Train loss: 0.5782214653521381\n",
      "Train loss: 0.5781436942136828\n",
      "Train loss: 0.5781607278905251\n",
      "Train loss: 0.5781319775948563\n",
      "Train loss: 0.5782051515700035\n",
      "Train loss: 0.5781824843131494\n",
      "Train loss: 0.5781798378417367\n",
      "Train loss: 0.5782147764343105\n",
      "Train loss: 0.5782326880369023\n",
      "Train loss: 0.5782028969749086\n",
      "Train loss: 0.5782602668115575\n",
      "Train loss: 0.5782995618773108\n",
      "Train loss: 0.5782676321137431\n",
      "Train loss: 0.5782483541616875\n",
      "Train loss: 0.5781424400624808\n",
      "Train loss: 0.5781204846319117\n",
      "Train loss: 0.5781706527863918\n",
      "Train loss: 0.578103588854032\n",
      "Train loss: 0.5780378809470461\n",
      "Train loss: 0.5780817454672718\n",
      "Train loss: 0.5781342060410405\n",
      "Train loss: 0.5780995932239127\n",
      "Train loss: 0.5780682972156896\n",
      "Train loss: 0.5781166791661432\n",
      "Train loss: 0.5780935317344283\n",
      "Train loss: 0.5780260533448981\n",
      "Train loss: 0.5780945055502489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5781460207422916\n",
      "Train loss: 0.578133017020673\n",
      "Train loss: 0.5782125331409679\n",
      "Train loss: 0.5782407080439104\n",
      "Train loss: 0.5782881766575296\n",
      "Train loss: 0.5782488805683343\n",
      "Train loss: 0.578205539073346\n",
      "Train loss: 0.5782190716076416\n",
      "Train loss: 0.5782033052699098\n",
      "Train loss: 0.5782009986554547\n",
      "Train loss: 0.5781828649492533\n",
      "Train loss: 0.5781663970475363\n",
      "Train loss: 0.5782084628515831\n",
      "Train loss: 0.5781787286173778\n",
      "Train loss: 0.5781810398150583\n",
      "Train loss: 0.5782219326300404\n",
      "Train loss: 0.5782355983177956\n",
      "Train loss: 0.5782568279467488\n",
      "Train loss: 0.578282501080695\n",
      "Train loss: 0.5783499810837042\n",
      "Train loss: 0.5783335511044985\n",
      "Train loss: 0.5783368321632323\n",
      "Train loss: 0.5783801057372029\n",
      "Train loss: 0.5783988078460739\n",
      "Train loss: 0.5783982920775037\n",
      "Train loss: 0.5783642771071056\n",
      "Train loss: 0.5783628586691172\n",
      "Train loss: 0.578390696382755\n",
      "Train loss: 0.5784392674302943\n",
      "Train loss: 0.5783952720266962\n",
      "Train loss: 0.5783958521021714\n",
      "Train loss: 0.578411843322057\n",
      "Train loss: 0.5784436260888627\n",
      "Train loss: 0.5785478660423736\n",
      "Train loss: 0.5785797364717219\n",
      "Train loss: 0.5785483351546978\n",
      "Train loss: 0.5785044515533118\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.698738344013691\n",
      "Val loss: 0.6216880182425181\n",
      "Val loss: 0.5933266792978559\n",
      "Val loss: 0.593359112739563\n",
      "Val loss: 0.59401307006677\n",
      "Val loss: 0.5962833844382187\n",
      "Val loss: 0.5896993016495424\n",
      "Val loss: 0.5846778200222895\n",
      "Val loss: 0.5791745727712457\n",
      "Val loss: 0.5792627699521123\n",
      "Val loss: 0.5800829540800165\n",
      "Val loss: 0.5790789097042407\n",
      "Val loss: 0.5789469927549362\n",
      "Val loss: 0.5785304269065028\n",
      "Val loss: 0.577604586610923\n",
      "Val loss: 0.5758165207844747\n",
      "Val loss: 0.5745906368607566\n",
      "Val loss: 0.5738229554020957\n",
      "Val loss: 0.5751935861846234\n",
      "Val loss: 0.5726779678253212\n",
      "Val loss: 0.5733059624639841\n",
      "Val loss: 0.5721370208700862\n",
      "Val loss: 0.5709352067165208\n",
      "Val loss: 0.5702039849858324\n",
      "Val loss: 0.5699580999151352\n",
      "Val loss: 0.5687048693960027\n",
      "Val loss: 0.5684562885939185\n",
      "Val loss: 0.5672046960257798\n",
      "Val loss: 0.5665717754099104\n",
      "Val loss: 0.5667926297091798\n",
      "Val loss: 0.5666064020101126\n",
      "Val loss: 0.5665420758649238\n",
      "Val loss: 0.5661116213333316\n",
      "Val loss: 0.5655754928052779\n",
      "Val loss: 0.5654165508418247\n",
      "Val loss: 0.565400661036955\n",
      "Val loss: 0.5636289155353671\n",
      "Val loss: 0.5636097252684296\n",
      "Val loss: 0.5643920035091872\n",
      "Val loss: 0.5636494700333581\n",
      "Val loss: 0.5640706343978059\n",
      "Val loss: 0.5635788038586885\n",
      "Val loss: 0.563126883634897\n",
      "Val loss: 0.5631813696802479\n",
      "Val loss: 0.5625660487317613\n",
      "Val loss: 0.5627928861624288\n",
      "Val loss: 0.5625907680672458\n",
      "Val loss: 0.5626982916847931\n",
      "Val loss: 0.5629265621060231\n",
      "Val loss: 0.5631299938064024\n",
      "Val loss: 0.562592227158584\n",
      "Val loss: 0.561988929178724\n",
      "Val loss: 0.5617875916262468\n",
      "Val loss: 0.5613260797630012\n",
      "Val loss: 0.5612852201409584\n",
      "Val loss: 0.5606098952686488\n",
      "Val loss: 0.5607388875853847\n",
      "Val loss: 0.5608037965932932\n",
      "Val loss: 0.5608712965939321\n",
      "Val loss: 0.5607813170123659\n",
      "Val loss: 0.5610430875891134\n",
      "Val loss: 0.5611329111466515\n",
      "Val loss: 0.5605914290923222\n",
      "Val loss: 0.560236633664762\n",
      "Val loss: 0.560121047919915\n",
      "Val loss: 0.5602204633882343\n",
      "Val loss: 0.5600751908774861\n",
      "Val loss: 0.5595929156714132\n",
      "Val loss: 0.5604681280809779\n",
      "Val loss: 0.5602950023375132\n",
      "Val loss: 0.5596023789570157\n",
      "Val loss: 0.5595441694711244\n",
      "Val loss: 0.5592369473242498\n",
      "Val loss: 0.5595028696021414\n",
      "Val loss: 0.5595788026557249\n",
      "Val loss: 0.5592790332507332\n",
      "Val loss: 0.5595267448419085\n",
      "Val loss: 0.5593439692243513\n",
      "Val loss: 0.5588948034544281\n",
      "Val loss: 0.5591007854257312\n",
      "Val loss: 0.5593695861130658\n",
      "Val loss: 0.5589189044129295\n",
      "Val loss: 0.5590006714857719\n",
      "Val loss: 0.5586827797081704\n",
      "Val loss: 0.5583732487317526\n",
      "Val loss: 0.5585234497949516\n",
      "Val loss: 0.5583606629465033\n",
      "Val loss: 0.5583928093687548\n",
      "Val loss: 0.557856150709831\n",
      "Val loss: 0.5576220897497207\n",
      "Val loss: 0.5578501276376489\n",
      "Val loss: 0.5578772892489672\n",
      "Val loss: 0.5579475186893652\n",
      "Val loss: 0.5579298223132518\n",
      "Val loss: 0.5576298182146459\n",
      "Val loss: 0.5579220488947468\n",
      "Val loss: 0.5580571842464533\n",
      "Val loss: 0.5581035957136524\n",
      "Val loss: 0.5581834915919826\n",
      "Val loss: 0.5586810902746503\n",
      "Val loss: 0.5585351763736635\n",
      "Val loss: 0.5584025726102892\n",
      "Val loss: 0.5588079184177784\n",
      "Val loss: 0.5585583044155026\n",
      "Val loss: 0.5584342078170703\n",
      "Val loss: 0.558312709254894\n",
      "Val loss: 0.5579806464106849\n",
      "Val loss: 0.5579517469888273\n",
      "Val loss: 0.5577613840098766\n",
      "Val loss: 0.5579199808868555\n",
      "Val loss: 0.5579825342777404\n",
      "Val loss: 0.5580657127718166\n",
      "Val loss: 0.5580269734487466\n",
      "Val loss: 0.558228244052411\n",
      "Val loss: 0.5580558209884457\n",
      "Val loss: 0.5581686116870821\n",
      "Val loss: 0.5582966964742909\n",
      "Val loss: 0.5583420468206519\n",
      "Val loss: 0.5584643610097744\n",
      "Val loss: 0.5587518211199166\n",
      "Val loss: 0.5589322564222955\n",
      "Val loss: 0.5592288428730957\n",
      "Val loss: 0.5592763518083368\n",
      "Val loss: 0.5592797468860238\n",
      "Val loss: 0.5594214022828218\n",
      "Val loss: 0.5596036168558609\n",
      "Val loss: 0.559485914391298\n",
      "\n",
      "starting Epoch 13\n",
      "Training...\n",
      "Train loss: 0.6146616841617384\n",
      "Train loss: 0.6004478480571356\n",
      "Train loss: 0.5968332214880798\n",
      "Train loss: 0.5853543485267253\n",
      "Train loss: 0.5799711451988028\n",
      "Train loss: 0.5827581927555949\n",
      "Train loss: 0.5820763132555021\n",
      "Train loss: 0.5839685552150199\n",
      "Train loss: 0.582242962035387\n",
      "Train loss: 0.5824374808138938\n",
      "Train loss: 0.5826638430765231\n",
      "Train loss: 0.5814443036103348\n",
      "Train loss: 0.5808327878073836\n",
      "Train loss: 0.5796452962583111\n",
      "Train loss: 0.5803429939874438\n",
      "Train loss: 0.5803150568262536\n",
      "Train loss: 0.5815136265614039\n",
      "Train loss: 0.5806423302812497\n",
      "Train loss: 0.580671248461137\n",
      "Train loss: 0.5801617401584348\n",
      "Train loss: 0.580720308430155\n",
      "Train loss: 0.5802302821486306\n",
      "Train loss: 0.5807383220180188\n",
      "Train loss: 0.579783509195722\n",
      "Train loss: 0.5797993599293466\n",
      "Train loss: 0.5789177254674981\n",
      "Train loss: 0.5792541623336707\n",
      "Train loss: 0.5784883023587877\n",
      "Train loss: 0.5789350130191533\n",
      "Train loss: 0.5793956231593289\n",
      "Train loss: 0.5799698655362661\n",
      "Train loss: 0.5803524978657097\n",
      "Train loss: 0.5801652820501052\n",
      "Train loss: 0.5802937399744109\n",
      "Train loss: 0.5801276122466348\n",
      "Train loss: 0.5802531438013111\n",
      "Train loss: 0.5796374289044185\n",
      "Train loss: 0.5789079751735934\n",
      "Train loss: 0.5782963823292773\n",
      "Train loss: 0.5786304533929192\n",
      "Train loss: 0.5783236228357159\n",
      "Train loss: 0.5781112658537045\n",
      "Train loss: 0.5783203902705307\n",
      "Train loss: 0.5783981519144142\n",
      "Train loss: 0.5781857457190123\n",
      "Train loss: 0.5779481597317705\n",
      "Train loss: 0.5776965421673081\n",
      "Train loss: 0.5772486886590316\n",
      "Train loss: 0.5779321381824131\n",
      "Train loss: 0.5777742929525442\n",
      "Train loss: 0.5778322803903959\n",
      "Train loss: 0.5785512843593252\n",
      "Train loss: 0.5787575374637942\n",
      "Train loss: 0.578460999917277\n",
      "Train loss: 0.5781485238760791\n",
      "Train loss: 0.5779298009405827\n",
      "Train loss: 0.577934834442189\n",
      "Train loss: 0.5783194323569356\n",
      "Train loss: 0.5783072926666496\n",
      "Train loss: 0.5785657865142106\n",
      "Train loss: 0.5785294845516903\n",
      "Train loss: 0.578449780153016\n",
      "Train loss: 0.5786953808250079\n",
      "Train loss: 0.5788273170788462\n",
      "Train loss: 0.5786596758490439\n",
      "Train loss: 0.5790524196001144\n",
      "Train loss: 0.57915949721315\n",
      "Train loss: 0.5790565111033613\n",
      "Train loss: 0.5790802260038211\n",
      "Train loss: 0.5791484996665793\n",
      "Train loss: 0.5792972345726854\n",
      "Train loss: 0.5791966770985957\n",
      "Train loss: 0.5790351044853882\n",
      "Train loss: 0.5790392508008659\n",
      "Train loss: 0.5791211084495631\n",
      "Train loss: 0.5788331626668582\n",
      "Train loss: 0.5787246906772846\n",
      "Train loss: 0.5789029485641036\n",
      "Train loss: 0.579053090164524\n",
      "Train loss: 0.5791201685614106\n",
      "Train loss: 0.5790910191047037\n",
      "Train loss: 0.5790788281465755\n",
      "Train loss: 0.5792030213898101\n",
      "Train loss: 0.579197971681671\n",
      "Train loss: 0.5791208700722564\n",
      "Train loss: 0.5793874225101893\n",
      "Train loss: 0.5794865779488714\n",
      "Train loss: 0.5795262218572119\n",
      "Train loss: 0.5794342095728602\n",
      "Train loss: 0.5795155209491755\n",
      "Train loss: 0.579692150666335\n",
      "Train loss: 0.5796379178454527\n",
      "Train loss: 0.5796631611983837\n",
      "Train loss: 0.5798077359835222\n",
      "Train loss: 0.5797718597155235\n",
      "Train loss: 0.5796157860581983\n",
      "Train loss: 0.5797037070039747\n",
      "Train loss: 0.5795714142615848\n",
      "Train loss: 0.5795083406026945\n",
      "Train loss: 0.5796308595129941\n",
      "Train loss: 0.5797392719302099\n",
      "Train loss: 0.5797335804824212\n",
      "Train loss: 0.5796898402393067\n",
      "Train loss: 0.5797067870452692\n",
      "Train loss: 0.579841919190092\n",
      "Train loss: 0.5798503288180832\n",
      "Train loss: 0.5799231236646419\n",
      "Train loss: 0.5798779746181935\n",
      "Train loss: 0.5798249619304724\n",
      "Train loss: 0.5797632166937081\n",
      "Train loss: 0.5796568721009031\n",
      "Train loss: 0.57946703925416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5796453447508675\n",
      "Train loss: 0.5795352137784469\n",
      "Train loss: 0.5794779402813531\n",
      "Train loss: 0.5795021629014783\n",
      "Train loss: 0.5796205556994067\n",
      "Train loss: 0.5796029201474621\n",
      "Train loss: 0.5794331628567735\n",
      "Train loss: 0.5793942986129968\n",
      "Train loss: 0.5794074711177702\n",
      "Train loss: 0.5793011040656281\n",
      "Train loss: 0.5792757498389001\n",
      "Train loss: 0.5791241955425336\n",
      "Train loss: 0.5791267046049721\n",
      "Train loss: 0.579322553156576\n",
      "Train loss: 0.5792937450706607\n",
      "Train loss: 0.579229769959679\n",
      "Train loss: 0.5792589274636993\n",
      "Train loss: 0.5792330604257286\n",
      "Train loss: 0.5792624038132423\n",
      "Train loss: 0.5792686977748756\n",
      "Train loss: 0.5791165140415772\n",
      "Train loss: 0.5792007698813407\n",
      "Train loss: 0.5791486588373322\n",
      "Train loss: 0.5791840162970819\n",
      "Train loss: 0.5791038581275209\n",
      "Train loss: 0.5790476587834934\n",
      "Train loss: 0.5789974930368769\n",
      "Train loss: 0.5790912331683843\n",
      "Train loss: 0.5789580029475431\n",
      "Train loss: 0.5787728626403056\n",
      "Train loss: 0.5788240812695414\n",
      "Train loss: 0.578919232057258\n",
      "Train loss: 0.5787295711764223\n",
      "Train loss: 0.578721042425104\n",
      "Train loss: 0.578718383728383\n",
      "Train loss: 0.5787616421057188\n",
      "Train loss: 0.5787552680373792\n",
      "Train loss: 0.5788504620936363\n",
      "Train loss: 0.5788532543968151\n",
      "Train loss: 0.578897583765119\n",
      "Train loss: 0.5789706655268032\n",
      "Train loss: 0.5788758213390736\n",
      "Train loss: 0.5787095124585354\n",
      "Train loss: 0.5785983300965808\n",
      "Train loss: 0.5785682741605996\n",
      "Train loss: 0.5785600918499342\n",
      "Train loss: 0.5785997975506322\n",
      "Train loss: 0.5785215047140649\n",
      "Train loss: 0.5786149420698224\n",
      "Train loss: 0.5786320631906698\n",
      "Train loss: 0.5785781724590507\n",
      "Train loss: 0.5786662544826794\n",
      "Train loss: 0.5786232970255222\n",
      "Train loss: 0.5787394220802431\n",
      "Train loss: 0.5787156716047128\n",
      "Train loss: 0.5786551971295718\n",
      "Train loss: 0.5787568203741081\n",
      "Train loss: 0.5787949170758353\n",
      "Train loss: 0.5788273853354721\n",
      "Train loss: 0.5788513544029398\n",
      "Train loss: 0.5788453304550488\n",
      "Train loss: 0.5787226522609604\n",
      "Train loss: 0.5786502046154444\n",
      "Train loss: 0.5787383605384393\n",
      "Train loss: 0.578746387331322\n",
      "Train loss: 0.5787135821341396\n",
      "Train loss: 0.5786104829459925\n",
      "Train loss: 0.5786567625568191\n",
      "Train loss: 0.5787321485828122\n",
      "Train loss: 0.5788129934589101\n",
      "Train loss: 0.5788704909467737\n",
      "Train loss: 0.578980659152153\n",
      "Train loss: 0.5789138488139163\n",
      "Train loss: 0.5788149834400417\n",
      "Train loss: 0.5788038255744964\n",
      "Train loss: 0.5788718924128524\n",
      "Train loss: 0.578757165230024\n",
      "Train loss: 0.5787212650190375\n",
      "Train loss: 0.5787281048457078\n",
      "Train loss: 0.5787038655835659\n",
      "Train loss: 0.5787613763652276\n",
      "Train loss: 0.5787354664616315\n",
      "Train loss: 0.5787848864084147\n",
      "Train loss: 0.5789233335322584\n",
      "Train loss: 0.5789488766680032\n",
      "Train loss: 0.5789402362808913\n",
      "Train loss: 0.5788363448472201\n",
      "Train loss: 0.578854369719227\n",
      "Train loss: 0.5787777522660867\n",
      "Train loss: 0.5787855654652386\n",
      "Train loss: 0.5786559554736994\n",
      "Train loss: 0.5786420836785343\n",
      "Train loss: 0.5786461581992475\n",
      "Train loss: 0.5787717773298466\n",
      "Train loss: 0.5787635828150329\n",
      "Train loss: 0.578838773582325\n",
      "Train loss: 0.5788826592646309\n",
      "Train loss: 0.5790097561939241\n",
      "Train loss: 0.5790158418025075\n",
      "Train loss: 0.5790204758786401\n",
      "Train loss: 0.5790347081936422\n",
      "Train loss: 0.57905314907651\n",
      "Train loss: 0.5789018811098668\n",
      "Train loss: 0.578934788910927\n",
      "Train loss: 0.5788075231233757\n",
      "Train loss: 0.5788102208556153\n",
      "Train loss: 0.5787873124049989\n",
      "Train loss: 0.5787788312742455\n",
      "Train loss: 0.5787859614495914\n",
      "Train loss: 0.5787951636217714\n",
      "Train loss: 0.5788049328137364\n",
      "Train loss: 0.5788275757041407\n",
      "Train loss: 0.5788019148541387\n",
      "Train loss: 0.5787054581052832\n",
      "Train loss: 0.5787730235758267\n",
      "Train loss: 0.5787603005972785\n",
      "Train loss: 0.5787337219371471\n",
      "Train loss: 0.5787325055800357\n",
      "Train loss: 0.5787741945313282\n",
      "Train loss: 0.5787652486175986\n",
      "Train loss: 0.5787666939657935\n",
      "Train loss: 0.5788267823232751\n",
      "Train loss: 0.5788435088266436\n",
      "Train loss: 0.578900218710957\n",
      "Train loss: 0.578870582886199\n",
      "Train loss: 0.578796905150156\n",
      "Train loss: 0.5788136346553704\n",
      "Train loss: 0.5788289425049655\n",
      "Train loss: 0.5789147883956673\n",
      "Train loss: 0.5789665353768718\n",
      "Train loss: 0.5789203203418761\n",
      "Train loss: 0.578913326438369\n",
      "Train loss: 0.5788969795578328\n",
      "Train loss: 0.5788782318509869\n",
      "Train loss: 0.5787892529877073\n",
      "Train loss: 0.5789071620171723\n",
      "Train loss: 0.5789126844488969\n",
      "Train loss: 0.5790293144748124\n",
      "Train loss: 0.5789633124716421\n",
      "Train loss: 0.5789425064053509\n",
      "Train loss: 0.5790119674877819\n",
      "Train loss: 0.5789354104511286\n",
      "Train loss: 0.5789747853493264\n",
      "Train loss: 0.5790004641382367\n",
      "Train loss: 0.5790472668672634\n",
      "Train loss: 0.5790104022620376\n",
      "Train loss: 0.5791086124094049\n",
      "Train loss: 0.5791126366738379\n",
      "Train loss: 0.5790944549287729\n",
      "Train loss: 0.5790325119525428\n",
      "Train loss: 0.5790054109085646\n",
      "Train loss: 0.5790752962776422\n",
      "Train loss: 0.579072982663365\n",
      "Train loss: 0.5790003491346718\n",
      "Train loss: 0.5789791959058616\n",
      "Train loss: 0.5789705535039281\n",
      "Train loss: 0.5789468947241887\n",
      "Train loss: 0.5789312607194566\n",
      "Train loss: 0.5788868320155438\n",
      "Train loss: 0.5789013658130535\n",
      "Train loss: 0.5788914539434464\n",
      "Train loss: 0.5790089857167061\n",
      "Train loss: 0.5790549097418417\n",
      "Train loss: 0.5790709063753656\n",
      "Train loss: 0.57915626475152\n",
      "Train loss: 0.5791783460901246\n",
      "Train loss: 0.5792269292570481\n",
      "Train loss: 0.5792932809996039\n",
      "Train loss: 0.5792378286149371\n",
      "Train loss: 0.5791958541676896\n",
      "Train loss: 0.5792633946349752\n",
      "Train loss: 0.5792669913711672\n",
      "Train loss: 0.5792541553480915\n",
      "Train loss: 0.5793065439729262\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6925750076770782\n",
      "Val loss: 0.6193785998556349\n",
      "Val loss: 0.5915561233248029\n",
      "Val loss: 0.5929551532394007\n",
      "Val loss: 0.5933573494354883\n",
      "Val loss: 0.5951369589772718\n",
      "Val loss: 0.589188805397819\n",
      "Val loss: 0.5840224394431481\n",
      "Val loss: 0.5791486935182051\n",
      "Val loss: 0.5788462563436858\n",
      "Val loss: 0.5791646794036582\n",
      "Val loss: 0.5779769208471653\n",
      "Val loss: 0.5779067715629935\n",
      "Val loss: 0.5776230338690937\n",
      "Val loss: 0.5765252572459143\n",
      "Val loss: 0.5751733002783377\n",
      "Val loss: 0.5737275956642061\n",
      "Val loss: 0.5733700217155928\n",
      "Val loss: 0.5746111632027524\n",
      "Val loss: 0.5724486287194069\n",
      "Val loss: 0.5727155346136826\n",
      "Val loss: 0.5716189840518007\n",
      "Val loss: 0.5706608598692375\n",
      "Val loss: 0.5701128179285707\n",
      "Val loss: 0.569835169180747\n",
      "Val loss: 0.5684949413750523\n",
      "Val loss: 0.5682763212652349\n",
      "Val loss: 0.5671524439784262\n",
      "Val loss: 0.5665205158293247\n",
      "Val loss: 0.566687709133097\n",
      "Val loss: 0.5665928864633882\n",
      "Val loss: 0.5665913981461675\n",
      "Val loss: 0.5662801527395481\n",
      "Val loss: 0.5658668463046734\n",
      "Val loss: 0.5656842411249533\n",
      "Val loss: 0.5656028010325724\n",
      "Val loss: 0.5640157049764758\n",
      "Val loss: 0.5640451955416846\n",
      "Val loss: 0.5648104275010296\n",
      "Val loss: 0.5640961467920236\n",
      "Val loss: 0.5645239627828785\n",
      "Val loss: 0.564061522198636\n",
      "Val loss: 0.5636086512670339\n",
      "Val loss: 0.5636187028394987\n",
      "Val loss: 0.5629630760688867\n",
      "Val loss: 0.5632419735843959\n",
      "Val loss: 0.5630673214665844\n",
      "Val loss: 0.5631897732553123\n",
      "Val loss: 0.5633602568604907\n",
      "Val loss: 0.5635810153311994\n",
      "Val loss: 0.563043862463921\n",
      "Val loss: 0.5625204943321847\n",
      "Val loss: 0.5622458306677414\n",
      "Val loss: 0.5618919256008248\n",
      "Val loss: 0.5618452423245367\n",
      "Val loss: 0.5613506225915793\n",
      "Val loss: 0.5614449454235358\n",
      "Val loss: 0.5614838460943691\n",
      "Val loss: 0.5615174070912965\n",
      "Val loss: 0.5614946258307301\n",
      "Val loss: 0.561680314278132\n",
      "Val loss: 0.5618302485510756\n",
      "Val loss: 0.5613123407219626\n",
      "Val loss: 0.5609821494096499\n",
      "Val loss: 0.5608353565136591\n",
      "Val loss: 0.5609584133675758\n",
      "Val loss: 0.5608278748517979\n",
      "Val loss: 0.5603842635949453\n",
      "Val loss: 0.5611453395076963\n",
      "Val loss: 0.5609541655782301\n",
      "Val loss: 0.5603485696733335\n",
      "Val loss: 0.5603067562772703\n",
      "Val loss: 0.5601017317929111\n",
      "Val loss: 0.560439605538438\n",
      "Val loss: 0.5604918548130097\n",
      "Val loss: 0.5602411323455518\n",
      "Val loss: 0.5604382997844368\n",
      "Val loss: 0.5602349133295388\n",
      "Val loss: 0.5597793121023227\n",
      "Val loss: 0.5600047647803649\n",
      "Val loss: 0.5602558619610154\n",
      "Val loss: 0.5597876368641562\n",
      "Val loss: 0.5598544646144489\n",
      "Val loss: 0.5595180235785345\n",
      "Val loss: 0.5592171074646823\n",
      "Val loss: 0.5593805465942774\n",
      "Val loss: 0.5592507656818161\n",
      "Val loss: 0.5592909570707004\n",
      "Val loss: 0.5588028012766494\n",
      "Val loss: 0.5586178195901331\n",
      "Val loss: 0.5587921027701331\n",
      "Val loss: 0.5588308931825468\n",
      "Val loss: 0.5589010824031871\n",
      "Val loss: 0.5588418304411842\n",
      "Val loss: 0.5586154441416012\n",
      "Val loss: 0.5588988310235528\n",
      "Val loss: 0.5590172265309933\n",
      "Val loss: 0.5590181676766136\n",
      "Val loss: 0.5590869390409485\n",
      "Val loss: 0.559570653703743\n",
      "Val loss: 0.5594341092048183\n",
      "Val loss: 0.5593163036761443\n",
      "Val loss: 0.559648106708137\n",
      "Val loss: 0.5594185795742652\n",
      "Val loss: 0.5593163160081128\n",
      "Val loss: 0.5592255665021042\n",
      "Val loss: 0.5589038705111443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5588277113902105\n",
      "Val loss: 0.5586271267384291\n",
      "Val loss: 0.5588088478546976\n",
      "Val loss: 0.5588912797641238\n",
      "Val loss: 0.5589626408540286\n",
      "Val loss: 0.5589078515768051\n",
      "Val loss: 0.5591008927155882\n",
      "Val loss: 0.5589606783219746\n",
      "Val loss: 0.5590941248677144\n",
      "Val loss: 0.5592372817025609\n",
      "Val loss: 0.5592641399347924\n",
      "Val loss: 0.559344799028904\n",
      "Val loss: 0.5596187957539184\n",
      "Val loss: 0.5597741605824982\n",
      "Val loss: 0.5600508286056456\n",
      "Val loss: 0.5600610200182234\n",
      "Val loss: 0.5600527771936672\n",
      "Val loss: 0.5601988110500269\n",
      "Val loss: 0.5603474157318971\n",
      "Val loss: 0.5602399790512651\n",
      "\n",
      "starting Epoch 14\n",
      "Training...\n",
      "Train loss: 0.6107165687962582\n",
      "Train loss: 0.6031591372612195\n",
      "Train loss: 0.5921378661010225\n",
      "Train loss: 0.5927142614050757\n",
      "Train loss: 0.5901402542085359\n",
      "Train loss: 0.5887339748755223\n",
      "Train loss: 0.5887502571232885\n",
      "Train loss: 0.5857090948137847\n",
      "Train loss: 0.5838102024027755\n",
      "Train loss: 0.5818399837867698\n",
      "Train loss: 0.5797226343252887\n",
      "Train loss: 0.5800403206917033\n",
      "Train loss: 0.5788603186837494\n",
      "Train loss: 0.5777210317632203\n",
      "Train loss: 0.578661030152171\n",
      "Train loss: 0.5785257598636292\n",
      "Train loss: 0.5766783230951754\n",
      "Train loss: 0.5772494942696977\n",
      "Train loss: 0.5786335968405087\n",
      "Train loss: 0.5790692464749616\n",
      "Train loss: 0.5786038854099401\n",
      "Train loss: 0.5779759825497933\n",
      "Train loss: 0.5784303725155351\n",
      "Train loss: 0.5791392660464524\n",
      "Train loss: 0.5793987232482505\n",
      "Train loss: 0.5788043818377346\n",
      "Train loss: 0.5783980823845943\n",
      "Train loss: 0.578910347067606\n",
      "Train loss: 0.5790353342247339\n",
      "Train loss: 0.579164534359424\n",
      "Train loss: 0.5789510912001614\n",
      "Train loss: 0.5791578201434235\n",
      "Train loss: 0.5787878658322897\n",
      "Train loss: 0.5784778010774958\n",
      "Train loss: 0.5790700298095807\n",
      "Train loss: 0.5792706229507675\n",
      "Train loss: 0.57943635126412\n",
      "Train loss: 0.579766789404614\n",
      "Train loss: 0.5795963995340394\n",
      "Train loss: 0.5800148553036629\n",
      "Train loss: 0.5796993326791476\n",
      "Train loss: 0.5798309039018151\n",
      "Train loss: 0.5799501537582789\n",
      "Train loss: 0.5798089349636039\n",
      "Train loss: 0.5799526864615112\n",
      "Train loss: 0.5802580265407334\n",
      "Train loss: 0.5799099180581597\n",
      "Train loss: 0.5793428624571302\n",
      "Train loss: 0.5795054350493511\n",
      "Train loss: 0.5797344073280319\n",
      "Train loss: 0.5797702413311885\n",
      "Train loss: 0.5796039145192466\n",
      "Train loss: 0.5795177820440279\n",
      "Train loss: 0.5794954086697908\n",
      "Train loss: 0.5794735032390529\n",
      "Train loss: 0.5794688631531594\n",
      "Train loss: 0.5794001357716568\n",
      "Train loss: 0.5794954489180505\n",
      "Train loss: 0.579557889720158\n",
      "Train loss: 0.5796174496735405\n",
      "Train loss: 0.5794462420052442\n",
      "Train loss: 0.5798087043806466\n",
      "Train loss: 0.5793576835639141\n",
      "Train loss: 0.5790381727505699\n",
      "Train loss: 0.5793512098783709\n",
      "Train loss: 0.5794589610783054\n",
      "Train loss: 0.5794390588664937\n",
      "Train loss: 0.5789925772271848\n",
      "Train loss: 0.5791571671933347\n",
      "Train loss: 0.5796330439950672\n",
      "Train loss: 0.5799231622486235\n",
      "Train loss: 0.5800966589172488\n",
      "Train loss: 0.5799368989434939\n",
      "Train loss: 0.5800544491807217\n",
      "Train loss: 0.5800679443358102\n",
      "Train loss: 0.5799270221636121\n",
      "Train loss: 0.5796882425561364\n",
      "Train loss: 0.5796031721552799\n",
      "Train loss: 0.5795744686393968\n",
      "Train loss: 0.5796644457099287\n",
      "Train loss: 0.5798883707417317\n",
      "Train loss: 0.5800365405412911\n",
      "Train loss: 0.5801507811757582\n",
      "Train loss: 0.5799364582671517\n",
      "Train loss: 0.5798374747023153\n",
      "Train loss: 0.5798310890421055\n",
      "Train loss: 0.5797738900912638\n",
      "Train loss: 0.579608740709667\n",
      "Train loss: 0.5795779606477138\n",
      "Train loss: 0.5796335506863299\n",
      "Train loss: 0.5795066650586184\n",
      "Train loss: 0.5793367718600138\n",
      "Train loss: 0.5793696055334059\n",
      "Train loss: 0.579393030275621\n",
      "Train loss: 0.579365719535465\n",
      "Train loss: 0.5791623531809196\n",
      "Train loss: 0.5793098828238025\n",
      "Train loss: 0.5792899786087249\n",
      "Train loss: 0.5791665314122123\n",
      "Train loss: 0.5790718774785991\n",
      "Train loss: 0.5788901778009046\n",
      "Train loss: 0.5789134285507277\n",
      "Train loss: 0.5789629788784335\n",
      "Train loss: 0.5790884680275507\n",
      "Train loss: 0.5792087646482558\n",
      "Train loss: 0.579188155531602\n",
      "Train loss: 0.5792780971766744\n",
      "Train loss: 0.5791560766071896\n",
      "Train loss: 0.5791301876785231\n",
      "Train loss: 0.5790816226727641\n",
      "Train loss: 0.5791056421679814\n",
      "Train loss: 0.5787818103216759\n",
      "Train loss: 0.5788840725523225\n",
      "Train loss: 0.5788574909604187\n",
      "Train loss: 0.5787648663877559\n",
      "Train loss: 0.5787222333615482\n",
      "Train loss: 0.5787920025919889\n",
      "Train loss: 0.5789744071078129\n",
      "Train loss: 0.5790454632171617\n",
      "Train loss: 0.5790079556663318\n",
      "Train loss: 0.5791079736707032\n",
      "Train loss: 0.5790680358526207\n",
      "Train loss: 0.579273622566338\n",
      "Train loss: 0.579211963297812\n",
      "Train loss: 0.5791757083883663\n",
      "Train loss: 0.5791957851627981\n",
      "Train loss: 0.5792040742608026\n",
      "Train loss: 0.5791607589729133\n",
      "Train loss: 0.5791376588094045\n",
      "Train loss: 0.5792189278066869\n",
      "Train loss: 0.5792311358401775\n",
      "Train loss: 0.5791036901983541\n",
      "Train loss: 0.5789284574196454\n",
      "Train loss: 0.5789582448447983\n",
      "Train loss: 0.5789975393321966\n",
      "Train loss: 0.5789975557927044\n",
      "Train loss: 0.5788639445359709\n",
      "Train loss: 0.5786978663871927\n",
      "Train loss: 0.5787443171177206\n",
      "Train loss: 0.5786902288766195\n",
      "Train loss: 0.5788408159765633\n",
      "Train loss: 0.5787817026137801\n",
      "Train loss: 0.5787661344740682\n",
      "Train loss: 0.5786866759852263\n",
      "Train loss: 0.5787257477306998\n",
      "Train loss: 0.5785855130918143\n",
      "Train loss: 0.5787295305270571\n",
      "Train loss: 0.5787198473449171\n",
      "Train loss: 0.5787041928592885\n",
      "Train loss: 0.5785934021370377\n",
      "Train loss: 0.5786326217134096\n",
      "Train loss: 0.5787034950373242\n",
      "Train loss: 0.5787790386202913\n",
      "Train loss: 0.5787705631273913\n",
      "Train loss: 0.5788512995797767\n",
      "Train loss: 0.5788417519017953\n",
      "Train loss: 0.5787507165917624\n",
      "Train loss: 0.5788107734573307\n",
      "Train loss: 0.5788468820526\n",
      "Train loss: 0.57871966288961\n",
      "Train loss: 0.5787799089137868\n",
      "Train loss: 0.5786952307541527\n",
      "Train loss: 0.5785980434752783\n",
      "Train loss: 0.5786934753277201\n",
      "Train loss: 0.5787690667970211\n",
      "Train loss: 0.5787576352770554\n",
      "Train loss: 0.5787536171640097\n",
      "Train loss: 0.5786553071204592\n",
      "Train loss: 0.5784856126607612\n",
      "Train loss: 0.5784689069285257\n",
      "Train loss: 0.5784589493961563\n",
      "Train loss: 0.5784763807680552\n",
      "Train loss: 0.5785281372211613\n",
      "Train loss: 0.5785689691713947\n",
      "Train loss: 0.5785735358286327\n",
      "Train loss: 0.5785512101264053\n",
      "Train loss: 0.5785225916124394\n",
      "Train loss: 0.5784261853264838\n",
      "Train loss: 0.5785197605214728\n",
      "Train loss: 0.5785826724431488\n",
      "Train loss: 0.5784312550881252\n",
      "Train loss: 0.5785388588151882\n",
      "Train loss: 0.5786116408760934\n",
      "Train loss: 0.578615999506945\n",
      "Train loss: 0.5786173530094171\n",
      "Train loss: 0.5786404547495174\n",
      "Train loss: 0.5786662931651411\n",
      "Train loss: 0.5786626854533368\n",
      "Train loss: 0.5786639378085973\n",
      "Train loss: 0.57868117896561\n",
      "Train loss: 0.5786676612565829\n",
      "Train loss: 0.578593079746934\n",
      "Train loss: 0.578530036637004\n",
      "Train loss: 0.5785863639522256\n",
      "Train loss: 0.5785762266673684\n",
      "Train loss: 0.5784945508812604\n",
      "Train loss: 0.57856605389028\n",
      "Train loss: 0.5786371098967266\n",
      "Train loss: 0.5785840862613433\n",
      "Train loss: 0.5785643171372787\n",
      "Train loss: 0.5785829938074996\n",
      "Train loss: 0.5786628970845438\n",
      "Train loss: 0.578702386361268\n",
      "Train loss: 0.5787570566910685\n",
      "Train loss: 0.5787342093824962\n",
      "Train loss: 0.5786453884227902\n",
      "Train loss: 0.5786748422707126\n",
      "Train loss: 0.5786691530876132\n",
      "Train loss: 0.5786782420973767\n",
      "Train loss: 0.5786423624612286\n",
      "Train loss: 0.5786101930399703\n",
      "Train loss: 0.578626999921972\n",
      "Train loss: 0.5787309663946233\n",
      "Train loss: 0.5785793967407478\n",
      "Train loss: 0.5786927470248032\n",
      "Train loss: 0.5786941494895341\n",
      "Train loss: 0.5787658123795297\n",
      "Train loss: 0.5787887386456354\n",
      "Train loss: 0.5786915041998005\n",
      "Train loss: 0.5787039352265454\n",
      "Train loss: 0.5787386235594075\n",
      "Train loss: 0.5787519825608592\n",
      "Train loss: 0.578796968475589\n",
      "Train loss: 0.5787075764035295\n",
      "Train loss: 0.578663600811987\n",
      "Train loss: 0.578700522579538\n",
      "Train loss: 0.5788032203932433\n",
      "Train loss: 0.5788433375353875\n",
      "Train loss: 0.5788348588484764\n",
      "Train loss: 0.5788271592658197\n",
      "Train loss: 0.5788238139915425\n",
      "Train loss: 0.5788130687855266\n",
      "Train loss: 0.5788292485927149\n",
      "Train loss: 0.5787753039992093\n",
      "Train loss: 0.5787183911616712\n",
      "Train loss: 0.5786741351924677\n",
      "Train loss: 0.5786373739278777\n",
      "Train loss: 0.5786367797643653\n",
      "Train loss: 0.5785034524768335\n",
      "Train loss: 0.5785111397051469\n",
      "Train loss: 0.5784400777899396\n",
      "Train loss: 0.5783448710092264\n",
      "Train loss: 0.5783945455990791\n",
      "Train loss: 0.5783538690329721\n",
      "Train loss: 0.578476554935235\n",
      "Train loss: 0.5785448689677896\n",
      "Train loss: 0.5785460800481581\n",
      "Train loss: 0.5785920706504919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5785858139596232\n",
      "Train loss: 0.5785731695954575\n",
      "Train loss: 0.578596736472022\n",
      "Train loss: 0.5785877796469948\n",
      "Train loss: 0.5786089391535416\n",
      "Train loss: 0.5786144226865755\n",
      "Train loss: 0.57858607989476\n",
      "Train loss: 0.5785805516514049\n",
      "Train loss: 0.5785844724833397\n",
      "Train loss: 0.5785824681846503\n",
      "Train loss: 0.5784260182795917\n",
      "Train loss: 0.578422844902922\n",
      "Train loss: 0.5784148831069641\n",
      "Train loss: 0.5784174484090737\n",
      "Train loss: 0.5784513262724056\n",
      "Train loss: 0.5783744714639024\n",
      "Train loss: 0.5783376512065836\n",
      "Train loss: 0.5782843628118396\n",
      "Train loss: 0.5783100743589832\n",
      "Train loss: 0.5782548157297691\n",
      "Train loss: 0.5782214575351049\n",
      "Train loss: 0.57827918283099\n",
      "Train loss: 0.5783304983255365\n",
      "Train loss: 0.5783146296623196\n",
      "Train loss: 0.5783275614773579\n",
      "Train loss: 0.5783275540596509\n",
      "Train loss: 0.5783373856678901\n",
      "Train loss: 0.5783985415367269\n",
      "Train loss: 0.5784667594421209\n",
      "Train loss: 0.5784883865737126\n",
      "Train loss: 0.5784032842467219\n",
      "Train loss: 0.5784152613077489\n",
      "Train loss: 0.5783799729921741\n",
      "Train loss: 0.5783597750853173\n",
      "Train loss: 0.5783646235640425\n",
      "Train loss: 0.578374858839996\n",
      "Train loss: 0.5783385853272485\n",
      "Train loss: 0.5782740710810944\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6982894241809845\n",
      "Val loss: 0.6195325321621366\n",
      "Val loss: 0.593722158244678\n",
      "Val loss: 0.5949809598295313\n",
      "Val loss: 0.5955850643416246\n",
      "Val loss: 0.600358215899303\n",
      "Val loss: 0.5929846667191562\n",
      "Val loss: 0.5865930784971286\n",
      "Val loss: 0.5815125683491881\n",
      "Val loss: 0.5814822456058191\n",
      "Val loss: 0.5820056624986507\n",
      "Val loss: 0.5813414095822027\n",
      "Val loss: 0.5805060719139874\n",
      "Val loss: 0.5800457419692606\n",
      "Val loss: 0.5793359122566275\n",
      "Val loss: 0.5773851037779941\n",
      "Val loss: 0.5759797355248815\n",
      "Val loss: 0.5746897576230295\n",
      "Val loss: 0.5759932135647916\n",
      "Val loss: 0.573337969454852\n",
      "Val loss: 0.5742749826839337\n",
      "Val loss: 0.5725629321478922\n",
      "Val loss: 0.5715388639977104\n",
      "Val loss: 0.5714696525525647\n",
      "Val loss: 0.5717114664373859\n",
      "Val loss: 0.570734057546586\n",
      "Val loss: 0.5703457184691927\n",
      "Val loss: 0.5689108577563609\n",
      "Val loss: 0.5678417212847207\n",
      "Val loss: 0.5681077571923301\n",
      "Val loss: 0.5677343117339271\n",
      "Val loss: 0.5679808579525858\n",
      "Val loss: 0.5679611627285074\n",
      "Val loss: 0.5672204937102526\n",
      "Val loss: 0.5668881910628286\n",
      "Val loss: 0.5667737074737442\n",
      "Val loss: 0.5648678925050341\n",
      "Val loss: 0.5648878555764597\n",
      "Val loss: 0.5656483946080061\n",
      "Val loss: 0.5647100219175444\n",
      "Val loss: 0.5651086779201732\n",
      "Val loss: 0.5645347335977418\n",
      "Val loss: 0.5636056295343649\n",
      "Val loss: 0.5637971079784986\n",
      "Val loss: 0.5631521996110678\n",
      "Val loss: 0.5633939947103309\n",
      "Val loss: 0.563021696276135\n",
      "Val loss: 0.5629520072099056\n",
      "Val loss: 0.5630462841420877\n",
      "Val loss: 0.5631085358949072\n",
      "Val loss: 0.5624898805862336\n",
      "Val loss: 0.5616123506001064\n",
      "Val loss: 0.5615371650818622\n",
      "Val loss: 0.5609884968920711\n",
      "Val loss: 0.5610644687918851\n",
      "Val loss: 0.5603037997813207\n",
      "Val loss: 0.5604846399854606\n",
      "Val loss: 0.5607106791854317\n",
      "Val loss: 0.5607679221703081\n",
      "Val loss: 0.560628121812208\n",
      "Val loss: 0.5610198193278751\n",
      "Val loss: 0.560870468520038\n",
      "Val loss: 0.5603424826055575\n",
      "Val loss: 0.5601060602545365\n",
      "Val loss: 0.5599565758933256\n",
      "Val loss: 0.5600404513824312\n",
      "Val loss: 0.5598790853501794\n",
      "Val loss: 0.5594666376929719\n",
      "Val loss: 0.5607838046758674\n",
      "Val loss: 0.5605886884268512\n",
      "Val loss: 0.5599279837251383\n",
      "Val loss: 0.5598769055933673\n",
      "Val loss: 0.5594688733364199\n",
      "Val loss: 0.5597329736563571\n",
      "Val loss: 0.5597870618423676\n",
      "Val loss: 0.5592782341710496\n",
      "Val loss: 0.5594473757470647\n",
      "Val loss: 0.559161986054369\n",
      "Val loss: 0.5586536631063761\n",
      "Val loss: 0.5588945476781755\n",
      "Val loss: 0.5592216047467572\n",
      "Val loss: 0.5585962802827504\n",
      "Val loss: 0.5589067161371166\n",
      "Val loss: 0.5584709081700991\n",
      "Val loss: 0.5582117817171339\n",
      "Val loss: 0.5583698125013382\n",
      "Val loss: 0.5581286069572247\n",
      "Val loss: 0.558094951992426\n",
      "Val loss: 0.5575388941141937\n",
      "Val loss: 0.557328165772232\n",
      "Val loss: 0.5575166324949474\n",
      "Val loss: 0.5574829471916415\n",
      "Val loss: 0.5575726854390112\n",
      "Val loss: 0.5575963503389216\n",
      "Val loss: 0.5573767690844676\n",
      "Val loss: 0.5576037330891245\n",
      "Val loss: 0.5576772816048181\n",
      "Val loss: 0.5576235971933493\n",
      "Val loss: 0.5575768439031323\n",
      "Val loss: 0.5581406945575454\n",
      "Val loss: 0.5580792402819036\n",
      "Val loss: 0.5578217175011551\n",
      "Val loss: 0.5582065785209492\n",
      "Val loss: 0.5579140323549796\n",
      "Val loss: 0.5577811425310055\n",
      "Val loss: 0.5576721531794968\n",
      "Val loss: 0.5573215915469195\n",
      "Val loss: 0.5572673119950162\n",
      "Val loss: 0.5570099712623393\n",
      "Val loss: 0.5570954434532936\n",
      "Val loss: 0.557128622643784\n",
      "Val loss: 0.5572702400586259\n",
      "Val loss: 0.5572654903991848\n",
      "Val loss: 0.5575408208558765\n",
      "Val loss: 0.5573156574669615\n",
      "Val loss: 0.557464606506631\n",
      "Val loss: 0.55756715715748\n",
      "Val loss: 0.557611762465362\n",
      "Val loss: 0.5577404230130641\n",
      "Val loss: 0.5580046519711738\n",
      "Val loss: 0.558317985903743\n",
      "Val loss: 0.558650194570936\n",
      "Val loss: 0.5588685060071634\n",
      "Val loss: 0.5589245971358457\n",
      "Val loss: 0.5591067966933434\n",
      "Val loss: 0.5592268874042553\n",
      "Val loss: 0.55908649492339\n",
      "\n",
      "starting Epoch 15\n",
      "Training...\n",
      "Train loss: 0.6192291284862318\n",
      "Train loss: 0.5974163489464002\n",
      "Train loss: 0.5906409318164244\n",
      "Train loss: 0.5922276166420949\n",
      "Train loss: 0.5908159517278575\n",
      "Train loss: 0.5866778507953933\n",
      "Train loss: 0.586965181201482\n",
      "Train loss: 0.5878824413572468\n",
      "Train loss: 0.5849938322711923\n",
      "Train loss: 0.5853353009451574\n",
      "Train loss: 0.583915247628678\n",
      "Train loss: 0.5835785401914908\n",
      "Train loss: 0.5836910430528943\n",
      "Train loss: 0.5828049727665481\n",
      "Train loss: 0.5833868733217884\n",
      "Train loss: 0.5824354307778576\n",
      "Train loss: 0.5813196218822558\n",
      "Train loss: 0.5813508931142706\n",
      "Train loss: 0.5808034610779744\n",
      "Train loss: 0.5810871694171638\n",
      "Train loss: 0.5802727500976981\n",
      "Train loss: 0.5790968785117591\n",
      "Train loss: 0.5794205503915649\n",
      "Train loss: 0.578969960795066\n",
      "Train loss: 0.5789767556295605\n",
      "Train loss: 0.5787930487437974\n",
      "Train loss: 0.5779033125773874\n",
      "Train loss: 0.5776354600683734\n",
      "Train loss: 0.5789214628451023\n",
      "Train loss: 0.5785856826078514\n",
      "Train loss: 0.5785246569910034\n",
      "Train loss: 0.5792139608628687\n",
      "Train loss: 0.579312731626363\n",
      "Train loss: 0.5794731218351356\n",
      "Train loss: 0.5792279065678559\n",
      "Train loss: 0.5799580800433816\n",
      "Train loss: 0.5797137657989507\n",
      "Train loss: 0.5790557655970884\n",
      "Train loss: 0.5794951506771998\n",
      "Train loss: 0.5791856947544967\n",
      "Train loss: 0.5791122953752021\n",
      "Train loss: 0.5789295905608244\n",
      "Train loss: 0.5789118946223099\n",
      "Train loss: 0.5788598625461938\n",
      "Train loss: 0.5787701338231233\n",
      "Train loss: 0.5792365554108064\n",
      "Train loss: 0.5790550463115827\n",
      "Train loss: 0.579055671616068\n",
      "Train loss: 0.5786361213967555\n",
      "Train loss: 0.5786350724157747\n",
      "Train loss: 0.5782337369464915\n",
      "Train loss: 0.5783663472668039\n",
      "Train loss: 0.5784721160070079\n",
      "Train loss: 0.5783597054141224\n",
      "Train loss: 0.5784802040368238\n",
      "Train loss: 0.5785958265970603\n",
      "Train loss: 0.5787659609286381\n",
      "Train loss: 0.5785901383452625\n",
      "Train loss: 0.5786837920281117\n",
      "Train loss: 0.5786814074202117\n",
      "Train loss: 0.5787343648831506\n",
      "Train loss: 0.5790457030377916\n",
      "Train loss: 0.5788320364508959\n",
      "Train loss: 0.578987787578887\n",
      "Train loss: 0.578602570515582\n",
      "Train loss: 0.5783598700692204\n",
      "Train loss: 0.5782534454276617\n",
      "Train loss: 0.5780847430930934\n",
      "Train loss: 0.5778816038063248\n",
      "Train loss: 0.5778954679238276\n",
      "Train loss: 0.5778479037820831\n",
      "Train loss: 0.5778295420747072\n",
      "Train loss: 0.5777614399780551\n",
      "Train loss: 0.5778933969843298\n",
      "Train loss: 0.5778560363069385\n",
      "Train loss: 0.5777786568130923\n",
      "Train loss: 0.5777945958654008\n",
      "Train loss: 0.5781364951514525\n",
      "Train loss: 0.5784800436732285\n",
      "Train loss: 0.5786196993357543\n",
      "Train loss: 0.5785749675757801\n",
      "Train loss: 0.5786869248796921\n",
      "Train loss: 0.5784911160694683\n",
      "Train loss: 0.5785888002840375\n",
      "Train loss: 0.5787171748997956\n",
      "Train loss: 0.5789255243425386\n",
      "Train loss: 0.5784852137642389\n",
      "Train loss: 0.578404666297879\n",
      "Train loss: 0.5783916836276759\n",
      "Train loss: 0.5783333231204745\n",
      "Train loss: 0.5781657668736822\n",
      "Train loss: 0.578255637807649\n",
      "Train loss: 0.5782107416662624\n",
      "Train loss: 0.578207459711153\n",
      "Train loss: 0.5782202966687804\n",
      "Train loss: 0.5784238486969576\n",
      "Train loss: 0.5782925801802198\n",
      "Train loss: 0.5782187504845046\n",
      "Train loss: 0.5780923915005742\n",
      "Train loss: 0.5781615628726248\n",
      "Train loss: 0.5781519231434917\n",
      "Train loss: 0.5783386541792667\n",
      "Train loss: 0.5781498244805727\n",
      "Train loss: 0.5783186378302397\n",
      "Train loss: 0.5783826995316206\n",
      "Train loss: 0.5782911266539107\n",
      "Train loss: 0.5781077743551897\n",
      "Train loss: 0.5779885863087253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5779334904678156\n",
      "Train loss: 0.5778785832988614\n",
      "Train loss: 0.5778048226107021\n",
      "Train loss: 0.577810843071271\n",
      "Train loss: 0.577634485858159\n",
      "Train loss: 0.5774093886355969\n",
      "Train loss: 0.5775668712972091\n",
      "Train loss: 0.5775732034467942\n",
      "Train loss: 0.5777250236937918\n",
      "Train loss: 0.5777592349603248\n",
      "Train loss: 0.5775181552776523\n",
      "Train loss: 0.5773920825169752\n",
      "Train loss: 0.577231313927009\n",
      "Train loss: 0.5772802232840649\n",
      "Train loss: 0.5771745007398799\n",
      "Train loss: 0.5773345908723372\n",
      "Train loss: 0.5773277662906136\n",
      "Train loss: 0.5773155947865451\n",
      "Train loss: 0.5773360235994376\n",
      "Train loss: 0.5771778965424914\n",
      "Train loss: 0.5772108394030038\n",
      "Train loss: 0.5772842342874646\n",
      "Train loss: 0.5772235714874908\n",
      "Train loss: 0.5774227867256561\n",
      "Train loss: 0.5773651571751717\n",
      "Train loss: 0.5773938676900675\n",
      "Train loss: 0.5774127257537206\n",
      "Train loss: 0.5775742730966339\n",
      "Train loss: 0.5776368222039909\n",
      "Train loss: 0.5775626308570612\n",
      "Train loss: 0.5775422879949497\n",
      "Train loss: 0.5774240598545708\n",
      "Train loss: 0.5774176097711209\n",
      "Train loss: 0.5773879388198335\n",
      "Train loss: 0.5775176526137522\n",
      "Train loss: 0.5775170928179783\n",
      "Train loss: 0.5775370587970357\n",
      "Train loss: 0.5774086800165559\n",
      "Train loss: 0.5774319946928145\n",
      "Train loss: 0.5773468063606044\n",
      "Train loss: 0.5774135669732742\n",
      "Train loss: 0.5773907672389025\n",
      "Train loss: 0.5775078488387347\n",
      "Train loss: 0.5774793766387476\n",
      "Train loss: 0.5774831658252979\n",
      "Train loss: 0.5775692744780067\n",
      "Train loss: 0.5776723215910033\n",
      "Train loss: 0.57777156681539\n",
      "Train loss: 0.5778177269099813\n",
      "Train loss: 0.5777805665317461\n",
      "Train loss: 0.5776558472754708\n",
      "Train loss: 0.5776568139650256\n",
      "Train loss: 0.5775768238589348\n",
      "Train loss: 0.5775965486882314\n",
      "Train loss: 0.5776189695054967\n",
      "Train loss: 0.5777062628101234\n",
      "Train loss: 0.5777580869418268\n",
      "Train loss: 0.5777200826610129\n",
      "Train loss: 0.5776253065758745\n",
      "Train loss: 0.5776348218352854\n",
      "Train loss: 0.577615335050085\n",
      "Train loss: 0.5776054485558271\n",
      "Train loss: 0.5776732867748705\n",
      "Train loss: 0.5777781798018727\n",
      "Train loss: 0.5777605133323388\n",
      "Train loss: 0.5777691263051237\n",
      "Train loss: 0.5777371943116222\n",
      "Train loss: 0.5778874050117616\n",
      "Train loss: 0.5779222637777041\n",
      "Train loss: 0.5779681523199931\n",
      "Train loss: 0.5779916171872372\n",
      "Train loss: 0.5779623847582103\n",
      "Train loss: 0.5779648450534954\n",
      "Train loss: 0.5780231439254873\n",
      "Train loss: 0.5780264252064234\n",
      "Train loss: 0.5779945376321782\n",
      "Train loss: 0.5779024406393402\n",
      "Train loss: 0.5779378412871887\n",
      "Train loss: 0.5779728542581922\n",
      "Train loss: 0.5780085279891453\n",
      "Train loss: 0.5780342363055719\n",
      "Train loss: 0.5781315770799407\n",
      "Train loss: 0.5781410951894065\n",
      "Train loss: 0.5780956760208009\n",
      "Train loss: 0.5781500699082074\n",
      "Train loss: 0.5781459289133073\n",
      "Train loss: 0.5782110001008063\n",
      "Train loss: 0.5782785525763634\n",
      "Train loss: 0.5782065186956928\n",
      "Train loss: 0.578311565082523\n",
      "Train loss: 0.5782943804244894\n",
      "Train loss: 0.5782329303513589\n",
      "Train loss: 0.5782344902768958\n",
      "Train loss: 0.5782460289799538\n",
      "Train loss: 0.578264874215489\n",
      "Train loss: 0.5782678594890837\n",
      "Train loss: 0.578204286130471\n",
      "Train loss: 0.5781254905481077\n",
      "Train loss: 0.5781935905670019\n",
      "Train loss: 0.578133123988399\n",
      "Train loss: 0.5781226621757543\n",
      "Train loss: 0.5781731747834278\n",
      "Train loss: 0.5782116123089494\n",
      "Train loss: 0.5782475640299073\n",
      "Train loss: 0.5782328382305876\n",
      "Train loss: 0.5782605324442851\n",
      "Train loss: 0.5782131415435008\n",
      "Train loss: 0.578220506265335\n",
      "Train loss: 0.5781977753206232\n",
      "Train loss: 0.5783259238664041\n",
      "Train loss: 0.5783419093467188\n",
      "Train loss: 0.5783685276957635\n",
      "Train loss: 0.5783994759766595\n",
      "Train loss: 0.5783647843432013\n",
      "Train loss: 0.5783640066519744\n",
      "Train loss: 0.5783641690932685\n",
      "Train loss: 0.5784472871433499\n",
      "Train loss: 0.5784672687119101\n",
      "Train loss: 0.5784320392963723\n",
      "Train loss: 0.5784792654293099\n",
      "Train loss: 0.5784789739763614\n",
      "Train loss: 0.5785923513556905\n",
      "Train loss: 0.5786319227736346\n",
      "Train loss: 0.5787215995220666\n",
      "Train loss: 0.5787961099518295\n",
      "Train loss: 0.578765553365943\n",
      "Train loss: 0.5787167193014893\n",
      "Train loss: 0.578786284792446\n",
      "Train loss: 0.5787656754098076\n",
      "Train loss: 0.5787797311595269\n",
      "Train loss: 0.5786998731480355\n",
      "Train loss: 0.5786762889362073\n",
      "Train loss: 0.5786508906305665\n",
      "Train loss: 0.5787120213388387\n",
      "Train loss: 0.5786786676311866\n",
      "Train loss: 0.578585347842916\n",
      "Train loss: 0.5786355106313269\n",
      "Train loss: 0.5786368220190276\n",
      "Train loss: 0.5786665471719088\n",
      "Train loss: 0.5786898478754443\n",
      "Train loss: 0.5786360943262345\n",
      "Train loss: 0.578605866403574\n",
      "Train loss: 0.5787649996776129\n",
      "Train loss: 0.5788470889088843\n",
      "Train loss: 0.5789108554420087\n",
      "Train loss: 0.5789298437422808\n",
      "Train loss: 0.5789795876963744\n",
      "Train loss: 0.5789709558510692\n",
      "Train loss: 0.5790244821248107\n",
      "Train loss: 0.5790388603373585\n",
      "Train loss: 0.5790901531536023\n",
      "Train loss: 0.5791202301611279\n",
      "Train loss: 0.5791203245780413\n",
      "Train loss: 0.5791266854083772\n",
      "Train loss: 0.579046359112517\n",
      "Train loss: 0.5790369382160228\n",
      "Train loss: 0.5790245510717904\n",
      "Train loss: 0.5789942318816274\n",
      "Train loss: 0.5789598118033072\n",
      "Train loss: 0.5789291848718597\n",
      "Train loss: 0.5789728407014101\n",
      "Train loss: 0.5789738954725387\n",
      "Train loss: 0.5789191457184286\n",
      "Train loss: 0.5788608745225411\n",
      "Train loss: 0.5789286979095559\n",
      "Train loss: 0.5788791682635765\n",
      "Train loss: 0.5788432822474178\n",
      "Train loss: 0.5788286747405831\n",
      "Train loss: 0.578891244390248\n",
      "Train loss: 0.5788906483228611\n",
      "Train loss: 0.578901358270628\n",
      "Train loss: 0.5788984167339504\n",
      "Train loss: 0.5789356630750983\n",
      "Train loss: 0.5789660761903461\n",
      "Train loss: 0.5789355566179738\n",
      "Train loss: 0.5789183249175915\n",
      "Train loss: 0.5788469251770161\n",
      "Train loss: 0.5789030740406572\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6963638812303543\n",
      "Val loss: 0.6227830449740092\n",
      "Val loss: 0.5968390213591712\n",
      "Val loss: 0.5960202797463066\n",
      "Val loss: 0.594966700921456\n",
      "Val loss: 0.5976395699484595\n",
      "Val loss: 0.5916746302562601\n",
      "Val loss: 0.5863860043195578\n",
      "Val loss: 0.5820028260350227\n",
      "Val loss: 0.5816740241585946\n",
      "Val loss: 0.5814745829061225\n",
      "Val loss: 0.5805430225396561\n",
      "Val loss: 0.5797900236211717\n",
      "Val loss: 0.57928458288096\n",
      "Val loss: 0.5786087073989816\n",
      "Val loss: 0.5769756138324738\n",
      "Val loss: 0.5758014594515165\n",
      "Val loss: 0.5744174889634165\n",
      "Val loss: 0.5754867040730537\n",
      "Val loss: 0.573286766957755\n",
      "Val loss: 0.5741332098841667\n",
      "Val loss: 0.5726110544226585\n",
      "Val loss: 0.5716325690348943\n",
      "Val loss: 0.5714169064489734\n",
      "Val loss: 0.5713652420428491\n",
      "Val loss: 0.5703262478806251\n",
      "Val loss: 0.5700062804702503\n",
      "Val loss: 0.5689229933049181\n",
      "Val loss: 0.5681586636023389\n",
      "Val loss: 0.5683512029631826\n",
      "Val loss: 0.5679712990274677\n",
      "Val loss: 0.5681500241816418\n",
      "Val loss: 0.5680924764857059\n",
      "Val loss: 0.56741500112432\n",
      "Val loss: 0.5671616172653505\n",
      "Val loss: 0.5672356496310101\n",
      "Val loss: 0.5657614720580371\n",
      "Val loss: 0.5656831165154775\n",
      "Val loss: 0.5663599975637554\n",
      "Val loss: 0.5656167127978262\n",
      "Val loss: 0.5658634066873905\n",
      "Val loss: 0.5653951825422533\n",
      "Val loss: 0.5647321283538765\n",
      "Val loss: 0.5648827875313693\n",
      "Val loss: 0.5644039709919265\n",
      "Val loss: 0.5645837902241919\n",
      "Val loss: 0.5643621753168921\n",
      "Val loss: 0.5643383997753574\n",
      "Val loss: 0.5643471010395738\n",
      "Val loss: 0.5644206646455818\n",
      "Val loss: 0.5638915876469274\n",
      "Val loss: 0.5630947122472594\n",
      "Val loss: 0.562999619791905\n",
      "Val loss: 0.5625504070276665\n",
      "Val loss: 0.5625688998150999\n",
      "Val loss: 0.5619312438272661\n",
      "Val loss: 0.5620863792342199\n",
      "Val loss: 0.5623263958003695\n",
      "Val loss: 0.5624007083931748\n",
      "Val loss: 0.5622803542924963\n",
      "Val loss: 0.5625686643547133\n",
      "Val loss: 0.5624914479872941\n",
      "Val loss: 0.5620618249960007\n",
      "Val loss: 0.5618518088304885\n",
      "Val loss: 0.5616984176047054\n",
      "Val loss: 0.5617219136297522\n",
      "Val loss: 0.5615243912635449\n",
      "Val loss: 0.5611135538524583\n",
      "Val loss: 0.5621158358489358\n",
      "Val loss: 0.5619482757539667\n",
      "Val loss: 0.5613693129376504\n",
      "Val loss: 0.5613315657486823\n",
      "Val loss: 0.5609662778429932\n",
      "Val loss: 0.5611586863109412\n",
      "Val loss: 0.561210526342698\n",
      "Val loss: 0.5608120180843373\n",
      "Val loss: 0.5609685687813908\n",
      "Val loss: 0.5607181823498792\n",
      "Val loss: 0.5603315570784099\n",
      "Val loss: 0.56043820176507\n",
      "Val loss: 0.5606443162749309\n",
      "Val loss: 0.5602056564066404\n",
      "Val loss: 0.5604277858555605\n",
      "Val loss: 0.5601369166004345\n",
      "Val loss: 0.5599479543853481\n",
      "Val loss: 0.5600965767056791\n",
      "Val loss: 0.5598873536570281\n",
      "Val loss: 0.5598880026769529\n",
      "Val loss: 0.5594550030188518\n",
      "Val loss: 0.5592956490532592\n",
      "Val loss: 0.5594281595577753\n",
      "Val loss: 0.5594553606572494\n",
      "Val loss: 0.5595386034070418\n",
      "Val loss: 0.5595210191410488\n",
      "Val loss: 0.55930088976013\n",
      "Val loss: 0.5594978095469744\n",
      "Val loss: 0.5595774764487566\n",
      "Val loss: 0.5595438590688452\n",
      "Val loss: 0.5595171510087333\n",
      "Val loss: 0.5599532020474245\n",
      "Val loss: 0.5599108318842593\n",
      "Val loss: 0.5597187746133223\n",
      "Val loss: 0.5600096367096623\n",
      "Val loss: 0.5597798386750193\n",
      "Val loss: 0.559667616058852\n",
      "Val loss: 0.5595817431835687\n",
      "Val loss: 0.5592867604244068\n",
      "Val loss: 0.5592530520705433\n",
      "Val loss: 0.5590288667008281\n",
      "Val loss: 0.5591228152667672\n",
      "Val loss: 0.5591308290562474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5592271293211921\n",
      "Val loss: 0.5591893584487286\n",
      "Val loss: 0.5594002741187654\n",
      "Val loss: 0.5592381870601235\n",
      "Val loss: 0.55937705329251\n",
      "Val loss: 0.5594894164943531\n",
      "Val loss: 0.5595434517022507\n",
      "Val loss: 0.5596684491895265\n",
      "Val loss: 0.5598841223473939\n",
      "Val loss: 0.5600995653808511\n",
      "Val loss: 0.5603792654176064\n",
      "Val loss: 0.5605062483673375\n",
      "Val loss: 0.5605320908815294\n",
      "Val loss: 0.5607070055049963\n",
      "Val loss: 0.5608134301459278\n",
      "Val loss: 0.560725073145015\n",
      "\n",
      "starting Epoch 16\n",
      "Training...\n",
      "Train loss: 0.5974941128178647\n",
      "Train loss: 0.5990517307550479\n",
      "Train loss: 0.5963000417765925\n",
      "Train loss: 0.5893388284912592\n",
      "Train loss: 0.5873086094254195\n",
      "Train loss: 0.5867758468419564\n",
      "Train loss: 0.5852117433393602\n",
      "Train loss: 0.5854846366546439\n",
      "Train loss: 0.5857065155186467\n",
      "Train loss: 0.5852555367515315\n",
      "Train loss: 0.5867655278613034\n",
      "Train loss: 0.5869429249893172\n",
      "Train loss: 0.5864258493695941\n",
      "Train loss: 0.586819711124598\n",
      "Train loss: 0.5859426802217362\n",
      "Train loss: 0.5856229812373935\n",
      "Train loss: 0.5856458312642258\n",
      "Train loss: 0.584288847313618\n",
      "Train loss: 0.5830238479580288\n",
      "Train loss: 0.5817512136354184\n",
      "Train loss: 0.5799502685559393\n",
      "Train loss: 0.5801734768855544\n",
      "Train loss: 0.5798115636788163\n",
      "Train loss: 0.5804695906370319\n",
      "Train loss: 0.5810879886986497\n",
      "Train loss: 0.5811079773829392\n",
      "Train loss: 0.5817132119455674\n",
      "Train loss: 0.5822881461256092\n",
      "Train loss: 0.5827297268753843\n",
      "Train loss: 0.5826137024094545\n",
      "Train loss: 0.5825034517656428\n",
      "Train loss: 0.5826068167022324\n",
      "Train loss: 0.5821508321125337\n",
      "Train loss: 0.5823648604008868\n",
      "Train loss: 0.5816399206504631\n",
      "Train loss: 0.5810456186823122\n",
      "Train loss: 0.5804036467339899\n",
      "Train loss: 0.5802697536973614\n",
      "Train loss: 0.5800754316581536\n",
      "Train loss: 0.5797262744924452\n",
      "Train loss: 0.5794671854329487\n",
      "Train loss: 0.5792624486146297\n",
      "Train loss: 0.5791996083936813\n",
      "Train loss: 0.579543301575013\n",
      "Train loss: 0.5796821347797275\n",
      "Train loss: 0.5794394320126327\n",
      "Train loss: 0.5795593331860531\n",
      "Train loss: 0.5797233077172069\n",
      "Train loss: 0.579367701165888\n",
      "Train loss: 0.5791423869085264\n",
      "Train loss: 0.5788287718700824\n",
      "Train loss: 0.579096335132496\n",
      "Train loss: 0.5790985435725834\n",
      "Train loss: 0.5788953897851391\n",
      "Train loss: 0.5785711141637068\n",
      "Train loss: 0.578642112426826\n",
      "Train loss: 0.5788071933705311\n",
      "Train loss: 0.5788998868158914\n",
      "Train loss: 0.5787439756316589\n",
      "Train loss: 0.5789474071662957\n",
      "Train loss: 0.5789071573792568\n",
      "Train loss: 0.5791928832140346\n",
      "Train loss: 0.5790989592588545\n",
      "Train loss: 0.5790345825377249\n",
      "Train loss: 0.5792007749130214\n",
      "Train loss: 0.5793570213646849\n",
      "Train loss: 0.5795293400276945\n",
      "Train loss: 0.5796968819596822\n",
      "Train loss: 0.5798030318645051\n",
      "Train loss: 0.5799187299734869\n",
      "Train loss: 0.5799517314185384\n",
      "Train loss: 0.580058855282258\n",
      "Train loss: 0.5796895053102351\n",
      "Train loss: 0.579638974762511\n",
      "Train loss: 0.5794827820182722\n",
      "Train loss: 0.5798076614688461\n",
      "Train loss: 0.5798511537078761\n",
      "Train loss: 0.5797522512860143\n",
      "Train loss: 0.5797096708290482\n",
      "Train loss: 0.5797609619940721\n",
      "Train loss: 0.5799425993347109\n",
      "Train loss: 0.5800779543867047\n",
      "Train loss: 0.5799334325511604\n",
      "Train loss: 0.5798712124989245\n",
      "Train loss: 0.5797073594993392\n",
      "Train loss: 0.580057367032319\n",
      "Train loss: 0.5798463369834828\n",
      "Train loss: 0.5799104205887042\n",
      "Train loss: 0.5799656673352862\n",
      "Train loss: 0.5798767925865985\n",
      "Train loss: 0.5799271598532018\n",
      "Train loss: 0.5799237033833363\n",
      "Train loss: 0.5800765769304672\n",
      "Train loss: 0.5799642794623281\n",
      "Train loss: 0.5798248633263675\n",
      "Train loss: 0.5796311739803044\n",
      "Train loss: 0.5794928340244195\n",
      "Train loss: 0.5796259755897181\n",
      "Train loss: 0.5795040152752381\n",
      "Train loss: 0.5796806571214065\n",
      "Train loss: 0.5799689811459268\n",
      "Train loss: 0.5799471115206549\n",
      "Train loss: 0.580002268449724\n",
      "Train loss: 0.5799754705063249\n",
      "Train loss: 0.5800203721031454\n",
      "Train loss: 0.5798816671766387\n",
      "Train loss: 0.5797130915119917\n",
      "Train loss: 0.579425138102576\n",
      "Train loss: 0.5793050944996624\n",
      "Train loss: 0.5793069388553087\n",
      "Train loss: 0.5792084726706019\n",
      "Train loss: 0.579223835439009\n",
      "Train loss: 0.5792970380433738\n",
      "Train loss: 0.5793382285220835\n",
      "Train loss: 0.5793877196550473\n",
      "Train loss: 0.5792956105230593\n",
      "Train loss: 0.5792376928066484\n",
      "Train loss: 0.5791943303959609\n",
      "Train loss: 0.579098387982676\n",
      "Train loss: 0.5789624816827746\n",
      "Train loss: 0.5787797529079441\n",
      "Train loss: 0.578866840142019\n",
      "Train loss: 0.5787228278364288\n",
      "Train loss: 0.5785823574754777\n",
      "Train loss: 0.5786136727468545\n",
      "Train loss: 0.5785390994126667\n",
      "Train loss: 0.5785682039755731\n",
      "Train loss: 0.5785608645084548\n",
      "Train loss: 0.5786065005456444\n",
      "Train loss: 0.5786066521080974\n",
      "Train loss: 0.578613921015624\n",
      "Train loss: 0.5786277680471116\n",
      "Train loss: 0.5787185933097676\n",
      "Train loss: 0.5786599624775469\n",
      "Train loss: 0.5785765993060691\n",
      "Train loss: 0.5785679462838147\n",
      "Train loss: 0.5785302274002564\n",
      "Train loss: 0.5785548770371535\n",
      "Train loss: 0.578419286287187\n",
      "Train loss: 0.5783174229694631\n",
      "Train loss: 0.5781366553933285\n",
      "Train loss: 0.5780735578113729\n",
      "Train loss: 0.5779969598309649\n",
      "Train loss: 0.5779251745277993\n",
      "Train loss: 0.5779974057309255\n",
      "Train loss: 0.5780425902893789\n",
      "Train loss: 0.5779520632276408\n",
      "Train loss: 0.5779735349992434\n",
      "Train loss: 0.5781149728040481\n",
      "Train loss: 0.5780374387396539\n",
      "Train loss: 0.577948738660746\n",
      "Train loss: 0.5779801255564582\n",
      "Train loss: 0.5779419219369162\n",
      "Train loss: 0.5778546715351853\n",
      "Train loss: 0.5779433355923044\n",
      "Train loss: 0.5779214405161175\n",
      "Train loss: 0.5780303733920018\n",
      "Train loss: 0.5778611696815521\n",
      "Train loss: 0.5778772373756698\n",
      "Train loss: 0.5779804445293107\n",
      "Train loss: 0.5780664418435459\n",
      "Train loss: 0.5780720512194927\n",
      "Train loss: 0.5780939851556317\n",
      "Train loss: 0.5780336321781743\n",
      "Train loss: 0.5780572366645821\n",
      "Train loss: 0.5781139272025781\n",
      "Train loss: 0.5781840520656263\n",
      "Train loss: 0.5782331387071816\n",
      "Train loss: 0.5783501652067461\n",
      "Train loss: 0.5782140462880416\n",
      "Train loss: 0.5782444120569165\n",
      "Train loss: 0.5783394070412052\n",
      "Train loss: 0.5783879834987345\n",
      "Train loss: 0.5784566828611083\n",
      "Train loss: 0.5785527260567876\n",
      "Train loss: 0.5786456608084597\n",
      "Train loss: 0.5786857684054729\n",
      "Train loss: 0.5786124007123747\n",
      "Train loss: 0.5787922242028183\n",
      "Train loss: 0.5787809205472585\n",
      "Train loss: 0.5788024811001568\n",
      "Train loss: 0.578832331349739\n",
      "Train loss: 0.5788250414195951\n",
      "Train loss: 0.5787378557595457\n",
      "Train loss: 0.5787197279024524\n",
      "Train loss: 0.5788772733550009\n",
      "Train loss: 0.578759807703743\n",
      "Train loss: 0.5787079582413514\n",
      "Train loss: 0.5788025639618033\n",
      "Train loss: 0.5787098171617459\n",
      "Train loss: 0.5787391243383003\n",
      "Train loss: 0.5786683049618685\n",
      "Train loss: 0.5786864287141507\n",
      "Train loss: 0.5786621573967674\n",
      "Train loss: 0.5786867108474055\n",
      "Train loss: 0.578619330528288\n",
      "Train loss: 0.5786967518956687\n",
      "Train loss: 0.5786658837040075\n",
      "Train loss: 0.5787426504051605\n",
      "Train loss: 0.5787343864337776\n",
      "Train loss: 0.5787573827309643\n",
      "Train loss: 0.5787066173349816\n",
      "Train loss: 0.5787646145281988\n",
      "Train loss: 0.5788238281843037\n",
      "Train loss: 0.5788540866832147\n",
      "Train loss: 0.5789184955593798\n",
      "Train loss: 0.578929419886464\n",
      "Train loss: 0.5788252185488241\n",
      "Train loss: 0.5788135761263611\n",
      "Train loss: 0.5788868938380407\n",
      "Train loss: 0.5790014493999449\n",
      "Train loss: 0.5789421648665908\n",
      "Train loss: 0.578886855226583\n",
      "Train loss: 0.5788589052561707\n",
      "Train loss: 0.5788589942438654\n",
      "Train loss: 0.5788654286115085\n",
      "Train loss: 0.5787807024687385\n",
      "Train loss: 0.578808286500978\n",
      "Train loss: 0.578767086217381\n",
      "Train loss: 0.5787990827144723\n",
      "Train loss: 0.5788658715381588\n",
      "Train loss: 0.578808796267489\n",
      "Train loss: 0.5787719846587193\n",
      "Train loss: 0.5788015216941624\n",
      "Train loss: 0.5788341661735915\n",
      "Train loss: 0.5788316529299417\n",
      "Train loss: 0.5787761182899542\n",
      "Train loss: 0.5787507232656917\n",
      "Train loss: 0.5787339187405156\n",
      "Train loss: 0.578767770145167\n",
      "Train loss: 0.5786992151658755\n",
      "Train loss: 0.5788370503590883\n",
      "Train loss: 0.5788441851789488\n",
      "Train loss: 0.5788266461905467\n",
      "Train loss: 0.578800325356638\n",
      "Train loss: 0.578870710420467\n",
      "Train loss: 0.5788427650312201\n",
      "Train loss: 0.5787797811891031\n",
      "Train loss: 0.5787557839386931\n",
      "Train loss: 0.5787789067903591\n",
      "Train loss: 0.5787142260944381\n",
      "Train loss: 0.5788113238040907\n",
      "Train loss: 0.5788101147270713\n",
      "Train loss: 0.5788154083457387\n",
      "Train loss: 0.5788123639877243\n",
      "Train loss: 0.5788054270917843\n",
      "Train loss: 0.5787922253877099\n",
      "Train loss: 0.5787989721107829\n",
      "Train loss: 0.5786894726319877\n",
      "Train loss: 0.5786862657842313\n",
      "Train loss: 0.5786325336037036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5786483391726105\n",
      "Train loss: 0.5787837778916353\n",
      "Train loss: 0.5788121535731573\n",
      "Train loss: 0.5788046220782505\n",
      "Train loss: 0.5788762211322226\n",
      "Train loss: 0.5789338296595008\n",
      "Train loss: 0.5789803195240009\n",
      "Train loss: 0.5789867595520045\n",
      "Train loss: 0.5789615353968859\n",
      "Train loss: 0.578911934776721\n",
      "Train loss: 0.5788443717344\n",
      "Train loss: 0.5788346597746675\n",
      "Train loss: 0.5788090140908974\n",
      "Train loss: 0.5787409599332635\n",
      "Train loss: 0.5786810123980953\n",
      "Train loss: 0.5786900028773385\n",
      "Train loss: 0.5787055622198884\n",
      "Train loss: 0.5786917267689631\n",
      "Train loss: 0.5787140637509225\n",
      "Train loss: 0.5787269203925534\n",
      "Train loss: 0.5787416737643777\n",
      "Train loss: 0.5786973332764793\n",
      "Train loss: 0.5786856004989023\n",
      "Train loss: 0.5786610386451388\n",
      "Train loss: 0.5786089148560735\n",
      "Train loss: 0.5786202880616953\n",
      "Train loss: 0.5785728806127509\n",
      "Train loss: 0.5785635688181398\n",
      "Train loss: 0.5786244435105117\n",
      "Train loss: 0.5786519666958331\n",
      "Train loss: 0.5786144689774552\n",
      "Train loss: 0.5785576198069025\n",
      "Train loss: 0.5785759816887219\n",
      "Train loss: 0.5785376351639814\n",
      "Train loss: 0.5785626166140747\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.7010796219110489\n",
      "Val loss: 0.6276074846585592\n",
      "Val loss: 0.6012275474412101\n",
      "Val loss: 0.5990613949926276\n",
      "Val loss: 0.5979710097114245\n",
      "Val loss: 0.6001398378405077\n",
      "Val loss: 0.5939043462276459\n",
      "Val loss: 0.5883696598884387\n",
      "Val loss: 0.584237277507782\n",
      "Val loss: 0.5839974941039572\n",
      "Val loss: 0.5841628677315183\n",
      "Val loss: 0.5837562750961821\n",
      "Val loss: 0.5827532578259706\n",
      "Val loss: 0.5822292238041975\n",
      "Val loss: 0.5816081625384253\n",
      "Val loss: 0.580126699013046\n",
      "Val loss: 0.578956813329742\n",
      "Val loss: 0.5777411213081874\n",
      "Val loss: 0.5785912877701699\n",
      "Val loss: 0.5762180023723178\n",
      "Val loss: 0.5773337116608253\n",
      "Val loss: 0.575604220049097\n",
      "Val loss: 0.5747118623633134\n",
      "Val loss: 0.5745215673907464\n",
      "Val loss: 0.5743527650352447\n",
      "Val loss: 0.5736325279224751\n",
      "Val loss: 0.5734821493501094\n",
      "Val loss: 0.5723060517431163\n",
      "Val loss: 0.5714973329255978\n",
      "Val loss: 0.5716216110143085\n",
      "Val loss: 0.5713065852979561\n",
      "Val loss: 0.5715533433095464\n",
      "Val loss: 0.5715138612360489\n",
      "Val loss: 0.5706944664907173\n",
      "Val loss: 0.5703785299569711\n",
      "Val loss: 0.5703403769924654\n",
      "Val loss: 0.5688915947533172\n",
      "Val loss: 0.5687151259530789\n",
      "Val loss: 0.5691550264960712\n",
      "Val loss: 0.5684602675425946\n",
      "Val loss: 0.5686500220029962\n",
      "Val loss: 0.5680994100547864\n",
      "Val loss: 0.5673597751655312\n",
      "Val loss: 0.5675245685392318\n",
      "Val loss: 0.5670889593394739\n",
      "Val loss: 0.5673278706823374\n",
      "Val loss: 0.5670206165975995\n",
      "Val loss: 0.5669362149727395\n",
      "Val loss: 0.5669914780825865\n",
      "Val loss: 0.5670300476761707\n",
      "Val loss: 0.5664892837287873\n",
      "Val loss: 0.5656899121737388\n",
      "Val loss: 0.5656998098799677\n",
      "Val loss: 0.5652522619772135\n",
      "Val loss: 0.565296628831947\n",
      "Val loss: 0.5646894420346906\n",
      "Val loss: 0.5648358768560517\n",
      "Val loss: 0.5651024938867166\n",
      "Val loss: 0.5651903705937522\n",
      "Val loss: 0.56510740159746\n",
      "Val loss: 0.5653525879116434\n",
      "Val loss: 0.565174121301151\n",
      "Val loss: 0.5648278917666454\n",
      "Val loss: 0.5646591378043064\n",
      "Val loss: 0.5645498385951843\n",
      "Val loss: 0.5645941793012764\n",
      "Val loss: 0.5645270636695587\n",
      "Val loss: 0.5641382688266338\n",
      "Val loss: 0.5651813560100489\n",
      "Val loss: 0.5650999026858704\n",
      "Val loss: 0.5645298254018449\n",
      "Val loss: 0.5645246133000738\n",
      "Val loss: 0.5641871929823697\n",
      "Val loss: 0.5643516867787535\n",
      "Val loss: 0.5643322083720549\n",
      "Val loss: 0.5638503554156714\n",
      "Val loss: 0.5639761662265906\n",
      "Val loss: 0.5637471606921414\n",
      "Val loss: 0.5633956808124096\n",
      "Val loss: 0.5635050379841549\n",
      "Val loss: 0.5637717351760014\n",
      "Val loss: 0.5633034519577959\n",
      "Val loss: 0.5635148116931823\n",
      "Val loss: 0.5631935338274107\n",
      "Val loss: 0.5630249495635617\n",
      "Val loss: 0.5631352755434308\n",
      "Val loss: 0.5628901079365735\n",
      "Val loss: 0.5628257163700591\n",
      "Val loss: 0.5624005696526518\n",
      "Val loss: 0.5622528756248925\n",
      "Val loss: 0.56237292782063\n",
      "Val loss: 0.562324607748871\n",
      "Val loss: 0.5624354163762825\n",
      "Val loss: 0.5624579292243478\n",
      "Val loss: 0.562294675642428\n",
      "Val loss: 0.5624403506206321\n",
      "Val loss: 0.5624657552227501\n",
      "Val loss: 0.5624047596396112\n",
      "Val loss: 0.5623475561137141\n",
      "Val loss: 0.562810134971309\n",
      "Val loss: 0.5628092449217562\n",
      "Val loss: 0.5625424665528805\n",
      "Val loss: 0.5628027300774355\n",
      "Val loss: 0.562582841957695\n",
      "Val loss: 0.5624957671602264\n",
      "Val loss: 0.5624646406993974\n",
      "Val loss: 0.562203817543912\n",
      "Val loss: 0.5621878662334966\n",
      "Val loss: 0.5619612831951064\n",
      "Val loss: 0.5620712744842244\n",
      "Val loss: 0.5620755678587441\n",
      "Val loss: 0.5621567050224979\n",
      "Val loss: 0.5621299467293929\n",
      "Val loss: 0.5623322446128396\n",
      "Val loss: 0.5621546787771199\n",
      "Val loss: 0.5622826133676143\n",
      "Val loss: 0.562362603281867\n",
      "Val loss: 0.56237230752239\n",
      "Val loss: 0.5625044085361339\n",
      "Val loss: 0.5626742355214535\n",
      "Val loss: 0.5629144834958955\n",
      "Val loss: 0.5631620520832895\n",
      "Val loss: 0.5633741058530559\n",
      "Val loss: 0.5634066864535189\n",
      "Val loss: 0.5635816750522608\n",
      "Val loss: 0.5637211846528258\n",
      "Val loss: 0.5636366082102718\n",
      "\n",
      "starting Epoch 17\n",
      "Training...\n",
      "Train loss: 0.6056069794454073\n",
      "Train loss: 0.5894013200050745\n",
      "Train loss: 0.5842570915060529\n",
      "Train loss: 0.5829449273362944\n",
      "Train loss: 0.5794866313837995\n",
      "Train loss: 0.5829068607642871\n",
      "Train loss: 0.5838281983951871\n",
      "Train loss: 0.5825629781627055\n",
      "Train loss: 0.5782643338488467\n",
      "Train loss: 0.578738033471994\n",
      "Train loss: 0.5788165868417313\n",
      "Train loss: 0.5788283661070229\n",
      "Train loss: 0.5777162185737065\n",
      "Train loss: 0.5782695169089943\n",
      "Train loss: 0.5798425475091838\n",
      "Train loss: 0.5799080073646617\n",
      "Train loss: 0.579878882320933\n",
      "Train loss: 0.5792030512788502\n",
      "Train loss: 0.5794570698429853\n",
      "Train loss: 0.5786866933331454\n",
      "Train loss: 0.5789987873775009\n",
      "Train loss: 0.5789436848668683\n",
      "Train loss: 0.5786016518513881\n",
      "Train loss: 0.5782753563872957\n",
      "Train loss: 0.5781276306671227\n",
      "Train loss: 0.5786780975113025\n",
      "Train loss: 0.5782046777529708\n",
      "Train loss: 0.5783733695159018\n",
      "Train loss: 0.5780496802148836\n",
      "Train loss: 0.5777342372227193\n",
      "Train loss: 0.5792569373651545\n",
      "Train loss: 0.5788296482092897\n",
      "Train loss: 0.5786334358958446\n",
      "Train loss: 0.5785286738173776\n",
      "Train loss: 0.5787990957882271\n",
      "Train loss: 0.5786431425552873\n",
      "Train loss: 0.5789067734320206\n",
      "Train loss: 0.578453842830281\n",
      "Train loss: 0.5784327314356019\n",
      "Train loss: 0.5783307980909216\n",
      "Train loss: 0.5780396762057247\n",
      "Train loss: 0.5785802215941614\n",
      "Train loss: 0.5785308317893876\n",
      "Train loss: 0.5787563742359886\n",
      "Train loss: 0.5787678074518486\n",
      "Train loss: 0.5786614718971625\n",
      "Train loss: 0.5787304519210507\n",
      "Train loss: 0.5787873202245353\n",
      "Train loss: 0.5792638688092334\n",
      "Train loss: 0.5793087500351686\n",
      "Train loss: 0.5789750481026212\n",
      "Train loss: 0.579053418841697\n",
      "Train loss: 0.5792727849469982\n",
      "Train loss: 0.5793434125444644\n",
      "Train loss: 0.579331787094623\n",
      "Train loss: 0.5792244426686387\n",
      "Train loss: 0.5789848955918866\n",
      "Train loss: 0.578590228883638\n",
      "Train loss: 0.5786297812211099\n",
      "Train loss: 0.5789598962781427\n",
      "Train loss: 0.5790049046693618\n",
      "Train loss: 0.5788715283797574\n",
      "Train loss: 0.578752190821506\n",
      "Train loss: 0.5786825736534604\n",
      "Train loss: 0.5784799154902349\n",
      "Train loss: 0.5784812811606757\n",
      "Train loss: 0.5783150762297308\n",
      "Train loss: 0.5780384852177436\n",
      "Train loss: 0.5778522987007836\n",
      "Train loss: 0.5780984627254696\n",
      "Train loss: 0.5778971363785739\n",
      "Train loss: 0.5776731863338306\n",
      "Train loss: 0.5776485836081018\n",
      "Train loss: 0.5773721190624095\n",
      "Train loss: 0.5773442933526017\n",
      "Train loss: 0.5772935327650137\n",
      "Train loss: 0.5771361334204597\n",
      "Train loss: 0.5771105080517994\n",
      "Train loss: 0.5768777845289377\n",
      "Train loss: 0.5767268681242885\n",
      "Train loss: 0.5765899535938132\n",
      "Train loss: 0.5762941476191741\n",
      "Train loss: 0.5764290404434733\n",
      "Train loss: 0.5764530090046043\n",
      "Train loss: 0.5764525423441444\n",
      "Train loss: 0.5766417954361945\n",
      "Train loss: 0.5766844919798634\n",
      "Train loss: 0.5765937910695317\n",
      "Train loss: 0.5762261604076694\n",
      "Train loss: 0.5764645922343025\n",
      "Train loss: 0.5766334510262423\n",
      "Train loss: 0.5763985567471461\n",
      "Train loss: 0.576494623330282\n",
      "Train loss: 0.5766165958813622\n",
      "Train loss: 0.5765841094803471\n",
      "Train loss: 0.5765905097911233\n",
      "Train loss: 0.5765716095078908\n",
      "Train loss: 0.5765330758004726\n",
      "Train loss: 0.5766415332470837\n",
      "Train loss: 0.5765159072400093\n",
      "Train loss: 0.576407638795899\n",
      "Train loss: 0.5762797255560486\n",
      "Train loss: 0.5766472962656434\n",
      "Train loss: 0.5768885150903001\n",
      "Train loss: 0.5769457379768665\n",
      "Train loss: 0.5769454592547928\n",
      "Train loss: 0.5770074428799777\n",
      "Train loss: 0.5771493016569191\n",
      "Train loss: 0.5772948826929472\n",
      "Train loss: 0.5773162872696964\n",
      "Train loss: 0.5771239236326248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5770866749887223\n",
      "Train loss: 0.5771592697272949\n",
      "Train loss: 0.5771998753538253\n",
      "Train loss: 0.5772041483073713\n",
      "Train loss: 0.577123428750727\n",
      "Train loss: 0.5771331473414131\n",
      "Train loss: 0.5772477403289435\n",
      "Train loss: 0.5770357395606464\n",
      "Train loss: 0.5771292816380155\n",
      "Train loss: 0.577142492472608\n",
      "Train loss: 0.5770840784989208\n",
      "Train loss: 0.5771155127692679\n",
      "Train loss: 0.5771689665514694\n",
      "Train loss: 0.5771395926620542\n",
      "Train loss: 0.5770873204968003\n",
      "Train loss: 0.5770914650259172\n",
      "Train loss: 0.5771407978685632\n",
      "Train loss: 0.5771093269497353\n",
      "Train loss: 0.5772860784996652\n",
      "Train loss: 0.5774801432426397\n",
      "Train loss: 0.5774902078679855\n",
      "Train loss: 0.5774203206126337\n",
      "Train loss: 0.5774559302310438\n",
      "Train loss: 0.5774738275594029\n",
      "Train loss: 0.5774953125942801\n",
      "Train loss: 0.5776055506564876\n",
      "Train loss: 0.5777401069058331\n",
      "Train loss: 0.5775170792624778\n",
      "Train loss: 0.577441542297229\n",
      "Train loss: 0.577375111458529\n",
      "Train loss: 0.5772735427995516\n",
      "Train loss: 0.5772686054361329\n",
      "Train loss: 0.5773478363093588\n",
      "Train loss: 0.5773553597635136\n",
      "Train loss: 0.5774557002197598\n",
      "Train loss: 0.5774707144602256\n",
      "Train loss: 0.5775648645953416\n",
      "Train loss: 0.5776906526500885\n",
      "Train loss: 0.5775795977049647\n",
      "Train loss: 0.5776257472110924\n",
      "Train loss: 0.5777073535686968\n",
      "Train loss: 0.5776758168346938\n",
      "Train loss: 0.5777286182134405\n",
      "Train loss: 0.5776667401901404\n",
      "Train loss: 0.5777434183164605\n",
      "Train loss: 0.5776056171356018\n",
      "Train loss: 0.577615650300551\n",
      "Train loss: 0.5776839560590926\n",
      "Train loss: 0.5778127088700283\n",
      "Train loss: 0.5777703741575183\n",
      "Train loss: 0.5777575872808594\n",
      "Train loss: 0.5777343695205426\n",
      "Train loss: 0.5779034727119243\n",
      "Train loss: 0.5779584360216632\n",
      "Train loss: 0.5779099727420428\n",
      "Train loss: 0.5780175434200662\n",
      "Train loss: 0.5780508352039329\n",
      "Train loss: 0.5780304000729743\n",
      "Train loss: 0.5780920680216671\n",
      "Train loss: 0.5780030868927893\n",
      "Train loss: 0.5780520816378276\n",
      "Train loss: 0.5780776168663193\n",
      "Train loss: 0.5780763323963563\n",
      "Train loss: 0.5780009167744249\n",
      "Train loss: 0.5779703503459244\n",
      "Train loss: 0.5780076462271254\n",
      "Train loss: 0.5780345603518019\n",
      "Train loss: 0.5779727677221104\n",
      "Train loss: 0.5779231099179467\n",
      "Train loss: 0.5778683956797922\n",
      "Train loss: 0.5779067572164287\n",
      "Train loss: 0.577843238948049\n",
      "Train loss: 0.5778349439114064\n",
      "Train loss: 0.577896001073727\n",
      "Train loss: 0.5778975116610366\n",
      "Train loss: 0.5778488190408759\n",
      "Train loss: 0.5779804029769801\n",
      "Train loss: 0.5779186107314873\n",
      "Train loss: 0.5778579907876688\n",
      "Train loss: 0.5778394253599173\n",
      "Train loss: 0.5778481531155609\n",
      "Train loss: 0.5778194639574032\n",
      "Train loss: 0.5777505468034535\n",
      "Train loss: 0.5777646142355936\n",
      "Train loss: 0.5777956671284668\n",
      "Train loss: 0.5778440745099603\n",
      "Train loss: 0.5779640123184114\n",
      "Train loss: 0.5779172104562638\n",
      "Train loss: 0.5779074723793882\n",
      "Train loss: 0.5779975154086054\n",
      "Train loss: 0.5780541685790996\n",
      "Train loss: 0.578122304765777\n",
      "Train loss: 0.5781017562196838\n",
      "Train loss: 0.5780510953008386\n",
      "Train loss: 0.5780478854670227\n",
      "Train loss: 0.5780903469338224\n",
      "Train loss: 0.5780424566818103\n",
      "Train loss: 0.578042147789426\n",
      "Train loss: 0.578113560436396\n",
      "Train loss: 0.5780478937418491\n",
      "Train loss: 0.5781968459961521\n",
      "Train loss: 0.5782446175792683\n",
      "Train loss: 0.578261145206221\n",
      "Train loss: 0.5782370061119037\n",
      "Train loss: 0.5783034103781737\n",
      "Train loss: 0.5781889319502282\n",
      "Train loss: 0.57827378904144\n",
      "Train loss: 0.578403834080854\n",
      "Train loss: 0.5784122310299363\n",
      "Train loss: 0.5783610756404262\n",
      "Train loss: 0.5783717934278035\n",
      "Train loss: 0.578469975830381\n",
      "Train loss: 0.5784275214183434\n",
      "Train loss: 0.5784439938443056\n",
      "Train loss: 0.5784794024646638\n",
      "Train loss: 0.5784921057228476\n",
      "Train loss: 0.578472473787932\n",
      "Train loss: 0.5785181515079889\n",
      "Train loss: 0.578545404494029\n",
      "Train loss: 0.5785873424673937\n",
      "Train loss: 0.5785624328988666\n",
      "Train loss: 0.5785184325156587\n",
      "Train loss: 0.5784999447313224\n",
      "Train loss: 0.5785400529546062\n",
      "Train loss: 0.5785799070696983\n",
      "Train loss: 0.5785841447538805\n",
      "Train loss: 0.5785282121458132\n",
      "Train loss: 0.5785351392239501\n",
      "Train loss: 0.5785916562899124\n",
      "Train loss: 0.5785112862308432\n",
      "Train loss: 0.5784938871109133\n",
      "Train loss: 0.5783498598717691\n",
      "Train loss: 0.5784140364768002\n",
      "Train loss: 0.5784002990351815\n",
      "Train loss: 0.578382812875101\n",
      "Train loss: 0.5783209849837628\n",
      "Train loss: 0.5782431943278612\n",
      "Train loss: 0.5782527110858672\n",
      "Train loss: 0.5782439720036865\n",
      "Train loss: 0.5782795085995338\n",
      "Train loss: 0.5782912315582326\n",
      "Train loss: 0.578241120657918\n",
      "Train loss: 0.5781822714438986\n",
      "Train loss: 0.5781889658601734\n",
      "Train loss: 0.5781461285017132\n",
      "Train loss: 0.5781364909612986\n",
      "Train loss: 0.5781014625890746\n",
      "Train loss: 0.5780767579869567\n",
      "Train loss: 0.5782173363308101\n",
      "Train loss: 0.5782412475360081\n",
      "Train loss: 0.5782695545975455\n",
      "Train loss: 0.5782345638208957\n",
      "Train loss: 0.5782997052898504\n",
      "Train loss: 0.5782442759918613\n",
      "Train loss: 0.5781486968356296\n",
      "Train loss: 0.578173682950458\n",
      "Train loss: 0.5781688391050088\n",
      "Train loss: 0.5782232211078935\n",
      "Train loss: 0.5782930525525187\n",
      "Train loss: 0.5782393412102542\n",
      "Train loss: 0.5783248972007294\n",
      "Train loss: 0.5782844360592296\n",
      "Train loss: 0.5782859393564274\n",
      "Train loss: 0.5783088419746976\n",
      "Train loss: 0.578317005304047\n",
      "Train loss: 0.5782979812717197\n",
      "Train loss: 0.5782715723301829\n",
      "Train loss: 0.578264698741427\n",
      "Train loss: 0.5782270272792844\n",
      "Train loss: 0.5782010746311942\n",
      "Train loss: 0.5782096872073493\n",
      "Train loss: 0.578147006879575\n",
      "Train loss: 0.5781097748756493\n",
      "Train loss: 0.5782300445369972\n",
      "Train loss: 0.5782485479480175\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6896157786250114\n",
      "Val loss: 0.6166107753912607\n",
      "Val loss: 0.5886664262839726\n",
      "Val loss: 0.5900756873582539\n",
      "Val loss: 0.589505930741628\n",
      "Val loss: 0.5926820471368986\n",
      "Val loss: 0.5867370139150059\n",
      "Val loss: 0.5815265576044718\n",
      "Val loss: 0.576629251241684\n",
      "Val loss: 0.5766890280100764\n",
      "Val loss: 0.5770215016824228\n",
      "Val loss: 0.5759462966757306\n",
      "Val loss: 0.5752519881352782\n",
      "Val loss: 0.5751046490842018\n",
      "Val loss: 0.5740546741195627\n",
      "Val loss: 0.5727392461480973\n",
      "Val loss: 0.5713180871236891\n",
      "Val loss: 0.5703793948286036\n",
      "Val loss: 0.5715194233554475\n",
      "Val loss: 0.5693833722610666\n",
      "Val loss: 0.5697895996272564\n",
      "Val loss: 0.56834988227678\n",
      "Val loss: 0.5672389234890017\n",
      "Val loss: 0.5669459935997715\n",
      "Val loss: 0.5667225622361706\n",
      "Val loss: 0.5653370956117793\n",
      "Val loss: 0.56507135549588\n",
      "Val loss: 0.5638427151192864\n",
      "Val loss: 0.563227177494102\n",
      "Val loss: 0.5633646005752103\n",
      "Val loss: 0.5631452806584247\n",
      "Val loss: 0.5632611273219751\n",
      "Val loss: 0.5631185762765931\n",
      "Val loss: 0.5625307750419752\n",
      "Val loss: 0.5622618844111761\n",
      "Val loss: 0.5622475978049486\n",
      "Val loss: 0.5606062297263871\n",
      "Val loss: 0.5605062982708058\n",
      "Val loss: 0.5611839507965698\n",
      "Val loss: 0.5604038870514337\n",
      "Val loss: 0.5608012080192566\n",
      "Val loss: 0.5604342732703287\n",
      "Val loss: 0.5598421142758611\n",
      "Val loss: 0.5599462023880928\n",
      "Val loss: 0.5595031597518495\n",
      "Val loss: 0.5597814234323376\n",
      "Val loss: 0.5594982518854305\n",
      "Val loss: 0.5595516853003323\n",
      "Val loss: 0.5596554573686396\n",
      "Val loss: 0.5597615226444949\n",
      "Val loss: 0.5591430192387948\n",
      "Val loss: 0.558486185478888\n",
      "Val loss: 0.5582381561398506\n",
      "Val loss: 0.5579241059969792\n",
      "Val loss: 0.5580586119942421\n",
      "Val loss: 0.5575596788450808\n",
      "Val loss: 0.5576387550419485\n",
      "Val loss: 0.5578127564237192\n",
      "Val loss: 0.5578462719714561\n",
      "Val loss: 0.5578087322089983\n",
      "Val loss: 0.5580130045939433\n",
      "Val loss: 0.5580384078342167\n",
      "Val loss: 0.5575006536807224\n",
      "Val loss: 0.5572108952416148\n",
      "Val loss: 0.5570099618699815\n",
      "Val loss: 0.5571078850143224\n",
      "Val loss: 0.5569116723930051\n",
      "Val loss: 0.5565389074758794\n",
      "Val loss: 0.5574238024825273\n",
      "Val loss: 0.5572541321929342\n",
      "Val loss: 0.5566707547269972\n",
      "Val loss: 0.5566927198746078\n",
      "Val loss: 0.5564505741163924\n",
      "Val loss: 0.5567590248617054\n",
      "Val loss: 0.5567913128730447\n",
      "Val loss: 0.5564306199865166\n",
      "Val loss: 0.5565584757520506\n",
      "Val loss: 0.5562759254003246\n",
      "Val loss: 0.5559006954358919\n",
      "Val loss: 0.556075951733386\n",
      "Val loss: 0.5563711946701059\n",
      "Val loss: 0.5559472570646654\n",
      "Val loss: 0.5561151925636374\n",
      "Val loss: 0.5558259196952875\n",
      "Val loss: 0.5555357917580964\n",
      "Val loss: 0.5556891029809182\n",
      "Val loss: 0.5555457327223043\n",
      "Val loss: 0.5555786238580196\n",
      "Val loss: 0.5550754728752214\n",
      "Val loss: 0.5548828370985316\n",
      "Val loss: 0.5550712745489003\n",
      "Val loss: 0.5551163109429261\n",
      "Val loss: 0.5551921343752022\n",
      "Val loss: 0.5551186177903401\n",
      "Val loss: 0.5548878227230869\n",
      "Val loss: 0.5551363975742913\n",
      "Val loss: 0.555188463554402\n",
      "Val loss: 0.5551854377509626\n",
      "Val loss: 0.5552226179524472\n",
      "Val loss: 0.5557120383860831\n",
      "Val loss: 0.5556079998375878\n",
      "Val loss: 0.5554226910904021\n",
      "Val loss: 0.5557521705729488\n",
      "Val loss: 0.5555322160495728\n",
      "Val loss: 0.5554593838466942\n",
      "Val loss: 0.5554221464578055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5550973360123259\n",
      "Val loss: 0.5550383546688561\n",
      "Val loss: 0.5548046055962058\n",
      "Val loss: 0.5549351995325696\n",
      "Val loss: 0.5550050443367838\n",
      "Val loss: 0.5550615503344425\n",
      "Val loss: 0.5550262152303195\n",
      "Val loss: 0.5552208870911222\n",
      "Val loss: 0.5550443626654689\n",
      "Val loss: 0.5552066695298967\n",
      "Val loss: 0.5553530288273341\n",
      "Val loss: 0.5553677210075139\n",
      "Val loss: 0.555448327652533\n",
      "Val loss: 0.555697241092166\n",
      "Val loss: 0.55591765736902\n",
      "Val loss: 0.5561974768959634\n",
      "Val loss: 0.5562734607764874\n",
      "Val loss: 0.5562831178813066\n",
      "Val loss: 0.5564432834776548\n",
      "Val loss: 0.5565501222170782\n",
      "Val loss: 0.5564519611630906\n",
      "\n",
      "starting Epoch 18\n",
      "Training...\n",
      "Train loss: 0.6118097368039583\n",
      "Train loss: 0.5856505441359985\n",
      "Train loss: 0.5750212149094727\n",
      "Train loss: 0.5810702054560939\n",
      "Train loss: 0.5804677018613527\n",
      "Train loss: 0.5807494513627862\n",
      "Train loss: 0.5808661067657334\n",
      "Train loss: 0.581126128727535\n",
      "Train loss: 0.5803121158530592\n",
      "Train loss: 0.5800061055164242\n",
      "Train loss: 0.5799345180868558\n",
      "Train loss: 0.5793622782040839\n",
      "Train loss: 0.5787115546957406\n",
      "Train loss: 0.5808064819023173\n",
      "Train loss: 0.5816628597053796\n",
      "Train loss: 0.5797149709025894\n",
      "Train loss: 0.5786918939742367\n",
      "Train loss: 0.5773893620476417\n",
      "Train loss: 0.577298209582281\n",
      "Train loss: 0.5780265738342639\n",
      "Train loss: 0.5779910197121432\n",
      "Train loss: 0.5788270761863518\n",
      "Train loss: 0.5793706691602736\n",
      "Train loss: 0.5800069858609759\n",
      "Train loss: 0.580489103325861\n",
      "Train loss: 0.579835227918533\n",
      "Train loss: 0.5805086641674361\n",
      "Train loss: 0.5805757764413659\n",
      "Train loss: 0.5806179855256912\n",
      "Train loss: 0.5807944297890034\n",
      "Train loss: 0.580556547266216\n",
      "Train loss: 0.5802282840246699\n",
      "Train loss: 0.5806014113614338\n",
      "Train loss: 0.5805113833501869\n",
      "Train loss: 0.5807153575751233\n",
      "Train loss: 0.5805928174908868\n",
      "Train loss: 0.5806977292358956\n",
      "Train loss: 0.5808134777900886\n",
      "Train loss: 0.5804922704289599\n",
      "Train loss: 0.5800577568470761\n",
      "Train loss: 0.580147174906818\n",
      "Train loss: 0.5800008497093232\n",
      "Train loss: 0.5804105174485963\n",
      "Train loss: 0.5799415719793926\n",
      "Train loss: 0.5803045546584188\n",
      "Train loss: 0.580186696773774\n",
      "Train loss: 0.580320492561379\n",
      "Train loss: 0.5800124354606128\n",
      "Train loss: 0.5801825730683733\n",
      "Train loss: 0.5804651499808849\n",
      "Train loss: 0.5800934453532319\n",
      "Train loss: 0.5798799285129128\n",
      "Train loss: 0.5799992589932551\n",
      "Train loss: 0.5802634173349941\n",
      "Train loss: 0.5799316953972755\n",
      "Train loss: 0.579873043820728\n",
      "Train loss: 0.5799893303279609\n",
      "Train loss: 0.5800615395575993\n",
      "Train loss: 0.5800620205731994\n",
      "Train loss: 0.5802088918489053\n",
      "Train loss: 0.5801500368304092\n",
      "Train loss: 0.5804186859998711\n",
      "Train loss: 0.580430376851625\n",
      "Train loss: 0.5803958238615852\n",
      "Train loss: 0.5804084711711713\n",
      "Train loss: 0.5804591697892788\n",
      "Train loss: 0.5803443995805887\n",
      "Train loss: 0.5801767541741167\n",
      "Train loss: 0.5802888153816497\n",
      "Train loss: 0.5805718736873515\n",
      "Train loss: 0.5803615798237796\n",
      "Train loss: 0.5805375056187256\n",
      "Train loss: 0.5805614509551471\n",
      "Train loss: 0.5804874849851427\n",
      "Train loss: 0.5804016134435134\n",
      "Train loss: 0.5805912472999276\n",
      "Train loss: 0.580638663462031\n",
      "Train loss: 0.5804357551626091\n",
      "Train loss: 0.5807515661349547\n",
      "Train loss: 0.5807018891358987\n",
      "Train loss: 0.5805117929327849\n",
      "Train loss: 0.5804632253913053\n",
      "Train loss: 0.5805185888638477\n",
      "Train loss: 0.5803422821533687\n",
      "Train loss: 0.5802827330986986\n",
      "Train loss: 0.5802005652669837\n",
      "Train loss: 0.5800343581902151\n",
      "Train loss: 0.5800685455780127\n",
      "Train loss: 0.5799775940605333\n",
      "Train loss: 0.5799806542930634\n",
      "Train loss: 0.5800423229572994\n",
      "Train loss: 0.5798612091449242\n",
      "Train loss: 0.5798363005076127\n",
      "Train loss: 0.5799201952770329\n",
      "Train loss: 0.5799739575655979\n",
      "Train loss: 0.5796737131226367\n",
      "Train loss: 0.5794961659363583\n",
      "Train loss: 0.5793829715452734\n",
      "Train loss: 0.5793764491675176\n",
      "Train loss: 0.5792539004029603\n",
      "Train loss: 0.5795279005011926\n",
      "Train loss: 0.5795576758627917\n",
      "Train loss: 0.5796553302934414\n",
      "Train loss: 0.5795336198841167\n",
      "Train loss: 0.5796565864698384\n",
      "Train loss: 0.5794979446251587\n",
      "Train loss: 0.579333544251182\n",
      "Train loss: 0.579332285316082\n",
      "Train loss: 0.5793589657192222\n",
      "Train loss: 0.5792643251935153\n",
      "Train loss: 0.5792933394104148\n",
      "Train loss: 0.5792648119817838\n",
      "Train loss: 0.5792469335188112\n",
      "Train loss: 0.5791087041773677\n",
      "Train loss: 0.5790764871707632\n",
      "Train loss: 0.5788320725130696\n",
      "Train loss: 0.5787622234336124\n",
      "Train loss: 0.5787523505595742\n",
      "Train loss: 0.5786009395683749\n",
      "Train loss: 0.5785531496171804\n",
      "Train loss: 0.578320153270097\n",
      "Train loss: 0.578345977854367\n",
      "Train loss: 0.5785016187713998\n",
      "Train loss: 0.5785478306013809\n",
      "Train loss: 0.5784998486021987\n",
      "Train loss: 0.5783066093424759\n",
      "Train loss: 0.578309412040594\n",
      "Train loss: 0.5783841518300173\n",
      "Train loss: 0.5783326167406708\n",
      "Train loss: 0.5784294092687289\n",
      "Train loss: 0.5784809336197108\n",
      "Train loss: 0.5784261887296668\n",
      "Train loss: 0.5785374722340897\n",
      "Train loss: 0.5785388701795953\n",
      "Train loss: 0.5785812934092479\n",
      "Train loss: 0.5785705218774599\n",
      "Train loss: 0.578606095190856\n",
      "Train loss: 0.5786155888830332\n",
      "Train loss: 0.5785663147846029\n",
      "Train loss: 0.5782552519816507\n",
      "Train loss: 0.5781778675015407\n",
      "Train loss: 0.5782676228086223\n",
      "Train loss: 0.578105761532435\n",
      "Train loss: 0.5781510091047396\n",
      "Train loss: 0.5781150782630542\n",
      "Train loss: 0.5782188492999089\n",
      "Train loss: 0.5781735981877624\n",
      "Train loss: 0.5781844996165165\n",
      "Train loss: 0.5781759583497855\n",
      "Train loss: 0.5781526980836537\n",
      "Train loss: 0.5780705266261029\n",
      "Train loss: 0.5780600098529588\n",
      "Train loss: 0.5781613535528753\n",
      "Train loss: 0.5781962605179344\n",
      "Train loss: 0.5782474324506266\n",
      "Train loss: 0.5783102964984183\n",
      "Train loss: 0.5784120741696371\n",
      "Train loss: 0.5784580769154722\n",
      "Train loss: 0.578550919454478\n",
      "Train loss: 0.578677124662599\n",
      "Train loss: 0.5787062877988771\n",
      "Train loss: 0.5786120562080102\n",
      "Train loss: 0.5786242731951174\n",
      "Train loss: 0.5785811420947031\n",
      "Train loss: 0.5785446089211649\n",
      "Train loss: 0.5784459760854244\n",
      "Train loss: 0.5784368410764535\n",
      "Train loss: 0.5784570823896851\n",
      "Train loss: 0.5784417243531625\n",
      "Train loss: 0.5784352892337389\n",
      "Train loss: 0.578515625472444\n",
      "Train loss: 0.5785290645925623\n",
      "Train loss: 0.5785632194915098\n",
      "Train loss: 0.5785384616929932\n",
      "Train loss: 0.5784632974469549\n",
      "Train loss: 0.5784050326568899\n",
      "Train loss: 0.5784513159268453\n",
      "Train loss: 0.5784645500260814\n",
      "Train loss: 0.5782685770639396\n",
      "Train loss: 0.578230366802242\n",
      "Train loss: 0.5782277569417698\n",
      "Train loss: 0.5783115096900057\n",
      "Train loss: 0.5784498347826231\n",
      "Train loss: 0.5784594624481243\n",
      "Train loss: 0.578467673556358\n",
      "Train loss: 0.5785235746248527\n",
      "Train loss: 0.5786460655581731\n",
      "Train loss: 0.5785471560806631\n",
      "Train loss: 0.5784647456865899\n",
      "Train loss: 0.5784153334245333\n",
      "Train loss: 0.5785055051114616\n",
      "Train loss: 0.5784633149157343\n",
      "Train loss: 0.5784018967135578\n",
      "Train loss: 0.5784188125913492\n",
      "Train loss: 0.578444341064325\n",
      "Train loss: 0.5784735806698883\n",
      "Train loss: 0.5784188146199573\n",
      "Train loss: 0.5784825566185939\n",
      "Train loss: 0.578590271992131\n",
      "Train loss: 0.5785577003286313\n",
      "Train loss: 0.578562929799943\n",
      "Train loss: 0.5784923358700246\n",
      "Train loss: 0.5785236784788622\n",
      "Train loss: 0.5785334208748685\n",
      "Train loss: 0.5785993648959706\n",
      "Train loss: 0.578634054517827\n",
      "Train loss: 0.5786276649495604\n",
      "Train loss: 0.5786083287045084\n",
      "Train loss: 0.5786906033932635\n",
      "Train loss: 0.5786881143846918\n",
      "Train loss: 0.5786643047543548\n",
      "Train loss: 0.5785948025586892\n",
      "Train loss: 0.5786530129098926\n",
      "Train loss: 0.578631131378505\n",
      "Train loss: 0.5785619488120717\n",
      "Train loss: 0.5785328487495384\n",
      "Train loss: 0.5784757404963139\n",
      "Train loss: 0.5784678603085555\n",
      "Train loss: 0.5785130958533717\n",
      "Train loss: 0.5784713497932024\n",
      "Train loss: 0.5784020283139111\n",
      "Train loss: 0.5784052722114087\n",
      "Train loss: 0.5784357810774361\n",
      "Train loss: 0.5784155142429711\n",
      "Train loss: 0.5783430584392751\n",
      "Train loss: 0.5783740527718487\n",
      "Train loss: 0.578518971882877\n",
      "Train loss: 0.5783873577145637\n",
      "Train loss: 0.5783845404853308\n",
      "Train loss: 0.5783000026650729\n",
      "Train loss: 0.5781769125292275\n",
      "Train loss: 0.5781449242369526\n",
      "Train loss: 0.578172448348477\n",
      "Train loss: 0.5781271773975459\n",
      "Train loss: 0.5781756579546148\n",
      "Train loss: 0.5781495407820506\n",
      "Train loss: 0.5781333435623172\n",
      "Train loss: 0.5781785690075562\n",
      "Train loss: 0.5781630239842901\n",
      "Train loss: 0.5781451020483723\n",
      "Train loss: 0.5781039203764319\n",
      "Train loss: 0.5781672319909663\n",
      "Train loss: 0.5782100587230721\n",
      "Train loss: 0.578264382755837\n",
      "Train loss: 0.5782793749349656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5782817752168876\n",
      "Train loss: 0.578251701927687\n",
      "Train loss: 0.5782684077055951\n",
      "Train loss: 0.5783314924446975\n",
      "Train loss: 0.5783325058265933\n",
      "Train loss: 0.5782757187998848\n",
      "Train loss: 0.5782692818727775\n",
      "Train loss: 0.578217367847627\n",
      "Train loss: 0.5782803494962188\n",
      "Train loss: 0.5783182048381462\n",
      "Train loss: 0.5782881963937677\n",
      "Train loss: 0.5782212537767173\n",
      "Train loss: 0.5781250963737439\n",
      "Train loss: 0.5781093690242978\n",
      "Train loss: 0.578063275010844\n",
      "Train loss: 0.5781016567185547\n",
      "Train loss: 0.5780409198666056\n",
      "Train loss: 0.5780441550094153\n",
      "Train loss: 0.5780801661178501\n",
      "Train loss: 0.5780259365742106\n",
      "Train loss: 0.5780660381810651\n",
      "Train loss: 0.5780801686606039\n",
      "Train loss: 0.578086749884281\n",
      "Train loss: 0.5781667546382467\n",
      "Train loss: 0.5781595086660578\n",
      "Train loss: 0.5781199459297935\n",
      "Train loss: 0.5780882907332382\n",
      "Train loss: 0.5780974312249197\n",
      "Train loss: 0.5780702935537515\n",
      "Train loss: 0.5780998299741424\n",
      "Train loss: 0.5780200031188589\n",
      "Train loss: 0.5780590069618955\n",
      "Train loss: 0.5781019915241756\n",
      "Train loss: 0.5780885915727165\n",
      "Train loss: 0.5780678719447156\n",
      "Train loss: 0.5780844461251714\n",
      "Train loss: 0.5780483408110808\n",
      "Train loss: 0.5780024636727084\n",
      "Train loss: 0.5780094633494534\n",
      "Train loss: 0.5779690944138149\n",
      "Train loss: 0.57804378131341\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6970462799072266\n",
      "Val loss: 0.6229275200102065\n",
      "Val loss: 0.5945134077753339\n",
      "Val loss: 0.5955006574329577\n",
      "Val loss: 0.5946990524729093\n",
      "Val loss: 0.5970424968620827\n",
      "Val loss: 0.5920237705988043\n",
      "Val loss: 0.5870534578959147\n",
      "Val loss: 0.5826430564576929\n",
      "Val loss: 0.5824948220836873\n",
      "Val loss: 0.5822527474827237\n",
      "Val loss: 0.5811979760557918\n",
      "Val loss: 0.5805363887920976\n",
      "Val loss: 0.5802596885225048\n",
      "Val loss: 0.5788889656195769\n",
      "Val loss: 0.5776553787762606\n",
      "Val loss: 0.5762770179481733\n",
      "Val loss: 0.575375774938069\n",
      "Val loss: 0.5759150331958811\n",
      "Val loss: 0.5740224703995869\n",
      "Val loss: 0.5745676707189816\n",
      "Val loss: 0.5732911045945018\n",
      "Val loss: 0.5725361100937191\n",
      "Val loss: 0.5721212199756077\n",
      "Val loss: 0.5718549915379093\n",
      "Val loss: 0.5703786692416021\n",
      "Val loss: 0.5701605399597937\n",
      "Val loss: 0.5692314241858695\n",
      "Val loss: 0.5685441529171334\n",
      "Val loss: 0.5686249622962619\n",
      "Val loss: 0.568425747674781\n",
      "Val loss: 0.5684242823963646\n",
      "Val loss: 0.5683168759796677\n",
      "Val loss: 0.5678619130475987\n",
      "Val loss: 0.5675848133262547\n",
      "Val loss: 0.5676164573797301\n",
      "Val loss: 0.5661913266648417\n",
      "Val loss: 0.56609253971665\n",
      "Val loss: 0.5667700736793047\n",
      "Val loss: 0.566158318040359\n",
      "Val loss: 0.5665473708627271\n",
      "Val loss: 0.566177605299288\n",
      "Val loss: 0.5657640501996067\n",
      "Val loss: 0.5658684953979162\n",
      "Val loss: 0.5654567180733595\n",
      "Val loss: 0.5656723188781322\n",
      "Val loss: 0.5654692485546454\n",
      "Val loss: 0.565556315323299\n",
      "Val loss: 0.5655682549857702\n",
      "Val loss: 0.5657475611052839\n",
      "Val loss: 0.5651675328964324\n",
      "Val loss: 0.5645776282866489\n",
      "Val loss: 0.5643710685950337\n",
      "Val loss: 0.5640183277289663\n",
      "Val loss: 0.5640618090864515\n",
      "Val loss: 0.5636100968793302\n",
      "Val loss: 0.5636366069526739\n",
      "Val loss: 0.5638183546107534\n",
      "Val loss: 0.5639002354574852\n",
      "Val loss: 0.5638453632493482\n",
      "Val loss: 0.5640079989833268\n",
      "Val loss: 0.564137531812137\n",
      "Val loss: 0.5636685502946757\n",
      "Val loss: 0.5633200516334522\n",
      "Val loss: 0.5631433732715654\n",
      "Val loss: 0.563199810162866\n",
      "Val loss: 0.563036718946731\n",
      "Val loss: 0.5626251115559828\n",
      "Val loss: 0.5632813597141311\n",
      "Val loss: 0.5631270009842848\n",
      "Val loss: 0.5625775283844457\n",
      "Val loss: 0.5625186186480987\n",
      "Val loss: 0.5622674372497496\n",
      "Val loss: 0.5625726853605855\n",
      "Val loss: 0.5626437023361737\n",
      "Val loss: 0.562329046329911\n",
      "Val loss: 0.5624269558272014\n",
      "Val loss: 0.5622026416973467\n",
      "Val loss: 0.5618573819774056\n",
      "Val loss: 0.5619012309345686\n",
      "Val loss: 0.5621361762727841\n",
      "Val loss: 0.5617501463895905\n",
      "Val loss: 0.5618631587800197\n",
      "Val loss: 0.5615817259348094\n",
      "Val loss: 0.5613627607389441\n",
      "Val loss: 0.5614858363474999\n",
      "Val loss: 0.5613718266998019\n",
      "Val loss: 0.5613791573970899\n",
      "Val loss: 0.5609412939698847\n",
      "Val loss: 0.5607923053553482\n",
      "Val loss: 0.5609174528967441\n",
      "Val loss: 0.5609980240496675\n",
      "Val loss: 0.5610532012350601\n",
      "Val loss: 0.5609457016245388\n",
      "Val loss: 0.5607057425040233\n",
      "Val loss: 0.560938681963839\n",
      "Val loss: 0.5610419989617403\n",
      "Val loss: 0.5610345030367253\n",
      "Val loss: 0.5610165100710595\n",
      "Val loss: 0.561455637216568\n",
      "Val loss: 0.5613872427197676\n",
      "Val loss: 0.5612447993108941\n",
      "Val loss: 0.5615351032538173\n",
      "Val loss: 0.5613310750509273\n",
      "Val loss: 0.561279131597235\n",
      "Val loss: 0.5612430767436108\n",
      "Val loss: 0.5609289912806914\n",
      "Val loss: 0.5608688217478913\n",
      "Val loss: 0.5606833060326821\n",
      "Val loss: 0.5608114096962038\n",
      "Val loss: 0.560838703859584\n",
      "Val loss: 0.5609038386873781\n",
      "Val loss: 0.5608154042815485\n",
      "Val loss: 0.5610104314262712\n",
      "Val loss: 0.5608693335322138\n",
      "Val loss: 0.5609937189774192\n",
      "Val loss: 0.5611150150429712\n",
      "Val loss: 0.5611439426706278\n",
      "Val loss: 0.561267449297889\n",
      "Val loss: 0.561478031199046\n",
      "Val loss: 0.5616002884132183\n",
      "Val loss: 0.5618534591201882\n",
      "Val loss: 0.5618720464287053\n",
      "Val loss: 0.5618717986277887\n",
      "Val loss: 0.5620457439277416\n",
      "Val loss: 0.5621727420718946\n",
      "Val loss: 0.5620654027920792\n",
      "\n",
      "starting Epoch 19\n",
      "Training...\n",
      "Train loss: 0.6288219721693742\n",
      "Train loss: 0.6020945280026166\n",
      "Train loss: 0.5914664899898787\n",
      "Train loss: 0.5836491505556469\n",
      "Train loss: 0.5843239143641308\n",
      "Train loss: 0.5813025388897968\n",
      "Train loss: 0.582896122186304\n",
      "Train loss: 0.5807983916510576\n",
      "Train loss: 0.5804960767650071\n",
      "Train loss: 0.5814741634244296\n",
      "Train loss: 0.5814196949135767\n",
      "Train loss: 0.5813593893120977\n",
      "Train loss: 0.5832248832958545\n",
      "Train loss: 0.5828026852513727\n",
      "Train loss: 0.581302396728841\n",
      "Train loss: 0.580382775083231\n",
      "Train loss: 0.5807515923428325\n",
      "Train loss: 0.5814525230183243\n",
      "Train loss: 0.5816377864506754\n",
      "Train loss: 0.5811038845613188\n",
      "Train loss: 0.5813199123078713\n",
      "Train loss: 0.5801237859595609\n",
      "Train loss: 0.5804639082069231\n",
      "Train loss: 0.5803718478545267\n",
      "Train loss: 0.580264365028045\n",
      "Train loss: 0.5801907405687895\n",
      "Train loss: 0.580125373292279\n",
      "Train loss: 0.5802955063916276\n",
      "Train loss: 0.5801924318847261\n",
      "Train loss: 0.5801134928439018\n",
      "Train loss: 0.5800927771505131\n",
      "Train loss: 0.5802401694147054\n",
      "Train loss: 0.580151261255846\n",
      "Train loss: 0.5795929989337218\n",
      "Train loss: 0.5792103601627595\n",
      "Train loss: 0.5791740949279908\n",
      "Train loss: 0.5783395681716754\n",
      "Train loss: 0.5782531564257676\n",
      "Train loss: 0.5783096122649882\n",
      "Train loss: 0.5781026937636327\n",
      "Train loss: 0.5783914009948353\n",
      "Train loss: 0.5779368126847036\n",
      "Train loss: 0.5778460442950478\n",
      "Train loss: 0.5785440110238068\n",
      "Train loss: 0.5785219435299331\n",
      "Train loss: 0.5784447655260109\n",
      "Train loss: 0.5786647254857011\n",
      "Train loss: 0.5785843320695899\n",
      "Train loss: 0.578632242345956\n",
      "Train loss: 0.578525335014284\n",
      "Train loss: 0.5783021445363263\n",
      "Train loss: 0.5787852936002127\n",
      "Train loss: 0.5783074717683765\n",
      "Train loss: 0.5782763304204384\n",
      "Train loss: 0.5785202316472051\n",
      "Train loss: 0.5786123901898569\n",
      "Train loss: 0.5788757236669522\n",
      "Train loss: 0.579105119981881\n",
      "Train loss: 0.5790387402407895\n",
      "Train loss: 0.5789849161853186\n",
      "Train loss: 0.5788677417420285\n",
      "Train loss: 0.5788180755933896\n",
      "Train loss: 0.5786059341608674\n",
      "Train loss: 0.5788998841959765\n",
      "Train loss: 0.5789931408022806\n",
      "Train loss: 0.5789032514073615\n",
      "Train loss: 0.5786725927903814\n",
      "Train loss: 0.5787140648315898\n",
      "Train loss: 0.5787335561832887\n",
      "Train loss: 0.5787951742818818\n",
      "Train loss: 0.578706749040867\n",
      "Train loss: 0.5785747406340209\n",
      "Train loss: 0.5785689602942398\n",
      "Train loss: 0.5783757867040951\n",
      "Train loss: 0.5780514231954439\n",
      "Train loss: 0.5780191104062063\n",
      "Train loss: 0.5777766054097981\n",
      "Train loss: 0.5778444975786594\n",
      "Train loss: 0.577919605734833\n",
      "Train loss: 0.5777356108514274\n",
      "Train loss: 0.577766037543222\n",
      "Train loss: 0.5775373368237061\n",
      "Train loss: 0.5774797421754879\n",
      "Train loss: 0.5773831571121171\n",
      "Train loss: 0.5773422577368504\n",
      "Train loss: 0.5774039083299698\n",
      "Train loss: 0.577364197728139\n",
      "Train loss: 0.5773081153614268\n",
      "Train loss: 0.5770885733455134\n",
      "Train loss: 0.5770700213940956\n",
      "Train loss: 0.5770152001966808\n",
      "Train loss: 0.577093261020089\n",
      "Train loss: 0.5771387738685188\n",
      "Train loss: 0.5770929869722343\n",
      "Train loss: 0.5771705213504569\n",
      "Train loss: 0.577143479489982\n",
      "Train loss: 0.5769506165697256\n",
      "Train loss: 0.5768771418477273\n",
      "Train loss: 0.5768298929601441\n",
      "Train loss: 0.5768098000319616\n",
      "Train loss: 0.5768164139888616\n",
      "Train loss: 0.5766999274419183\n",
      "Train loss: 0.5769775866767398\n",
      "Train loss: 0.5768597229791983\n",
      "Train loss: 0.5767774100017411\n",
      "Train loss: 0.5765640904116934\n",
      "Train loss: 0.57643628231885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5764759348022988\n",
      "Train loss: 0.5763603287341672\n",
      "Train loss: 0.5763843128658415\n",
      "Train loss: 0.5764054314087725\n",
      "Train loss: 0.5762794047474276\n",
      "Train loss: 0.5761485477631575\n",
      "Train loss: 0.5761860209162061\n",
      "Train loss: 0.5761887697797072\n",
      "Train loss: 0.5760855084223293\n",
      "Train loss: 0.576122299327255\n",
      "Train loss: 0.5762625871800224\n",
      "Train loss: 0.5763136512995668\n",
      "Train loss: 0.5763728212147466\n",
      "Train loss: 0.5763950615369766\n",
      "Train loss: 0.5765801479626796\n",
      "Train loss: 0.5768075487438185\n",
      "Train loss: 0.5767632578471439\n",
      "Train loss: 0.5767910091363702\n",
      "Train loss: 0.5766206025225438\n",
      "Train loss: 0.5766212898429429\n",
      "Train loss: 0.5767159508210041\n",
      "Train loss: 0.5767303554066026\n",
      "Train loss: 0.5767382771269823\n",
      "Train loss: 0.5767686255745599\n",
      "Train loss: 0.5768613886227885\n",
      "Train loss: 0.5767367685948516\n",
      "Train loss: 0.5766026511504696\n",
      "Train loss: 0.5766789920575621\n",
      "Train loss: 0.5765611760732048\n",
      "Train loss: 0.5764046508428519\n",
      "Train loss: 0.5765074238953447\n",
      "Train loss: 0.5766637033161905\n",
      "Train loss: 0.5767612668947477\n",
      "Train loss: 0.5768434087534076\n",
      "Train loss: 0.5768319360699272\n",
      "Train loss: 0.5769061094795586\n",
      "Train loss: 0.5768916931633488\n",
      "Train loss: 0.5769275226708979\n",
      "Train loss: 0.5768747444367646\n",
      "Train loss: 0.5770201366768681\n",
      "Train loss: 0.5770028718382412\n",
      "Train loss: 0.5769699540623245\n",
      "Train loss: 0.5771941403203426\n",
      "Train loss: 0.5771801032523282\n",
      "Train loss: 0.5770827767565754\n",
      "Train loss: 0.5770775389441403\n",
      "Train loss: 0.5769914323321872\n",
      "Train loss: 0.5769731939204396\n",
      "Train loss: 0.5770865209441263\n",
      "Train loss: 0.5772020470644115\n",
      "Train loss: 0.5772920622879208\n",
      "Train loss: 0.5772712633515275\n",
      "Train loss: 0.5771679080866694\n",
      "Train loss: 0.5770631989139546\n",
      "Train loss: 0.5770558947746163\n",
      "Train loss: 0.5771132251187752\n",
      "Train loss: 0.5771133638657095\n",
      "Train loss: 0.5771040069619682\n",
      "Train loss: 0.5770452708122636\n",
      "Train loss: 0.5769242783637417\n",
      "Train loss: 0.5770135885123945\n",
      "Train loss: 0.5770182608676119\n",
      "Train loss: 0.5770710038942392\n",
      "Train loss: 0.5770824444715863\n",
      "Train loss: 0.5770081575958838\n",
      "Train loss: 0.5771012606132095\n",
      "Train loss: 0.5771683556024766\n",
      "Train loss: 0.5771226370528821\n",
      "Train loss: 0.5772640448329326\n",
      "Train loss: 0.5773109126373953\n",
      "Train loss: 0.577387428210002\n",
      "Train loss: 0.5773892240605564\n",
      "Train loss: 0.5774711768274077\n",
      "Train loss: 0.5775578138445516\n",
      "Train loss: 0.577641765440385\n",
      "Train loss: 0.577705476723202\n",
      "Train loss: 0.5776322094698774\n",
      "Train loss: 0.5777069170045092\n",
      "Train loss: 0.5776826753895845\n",
      "Train loss: 0.5776411409316199\n",
      "Train loss: 0.5777083536940516\n",
      "Train loss: 0.5777802188723262\n",
      "Train loss: 0.5777820693831532\n",
      "Train loss: 0.5778043109276869\n",
      "Train loss: 0.5777839822798975\n",
      "Train loss: 0.5778088365913395\n",
      "Train loss: 0.5778333197767264\n",
      "Train loss: 0.5778718687274818\n",
      "Train loss: 0.5778967412143498\n",
      "Train loss: 0.5777797544175037\n",
      "Train loss: 0.5778466482811668\n",
      "Train loss: 0.5778662624012199\n",
      "Train loss: 0.5778213964533466\n",
      "Train loss: 0.577947682418109\n",
      "Train loss: 0.5779345523649939\n",
      "Train loss: 0.5780025088499261\n",
      "Train loss: 0.5778531221073321\n",
      "Train loss: 0.577879878079551\n",
      "Train loss: 0.5779147109423312\n",
      "Train loss: 0.577844840579057\n",
      "Train loss: 0.5778475082832106\n",
      "Train loss: 0.5777489514435328\n",
      "Train loss: 0.5776722104840688\n",
      "Train loss: 0.577680249651\n",
      "Train loss: 0.5776930314157616\n",
      "Train loss: 0.5776852397858243\n",
      "Train loss: 0.5777097532099501\n",
      "Train loss: 0.5776997789737096\n",
      "Train loss: 0.5775513194465174\n",
      "Train loss: 0.5775851018556831\n",
      "Train loss: 0.5776187142228609\n",
      "Train loss: 0.5776242588883422\n",
      "Train loss: 0.5776992136170903\n",
      "Train loss: 0.5777241374794027\n",
      "Train loss: 0.5776536005202326\n",
      "Train loss: 0.5777453700237997\n",
      "Train loss: 0.5778011192991201\n",
      "Train loss: 0.5778205497950071\n",
      "Train loss: 0.5779425513958029\n",
      "Train loss: 0.5779083863206045\n",
      "Train loss: 0.5779177692996015\n",
      "Train loss: 0.57794790447318\n",
      "Train loss: 0.5779827794424216\n",
      "Train loss: 0.5781384069028161\n",
      "Train loss: 0.5781620580540003\n",
      "Train loss: 0.5781310180285684\n",
      "Train loss: 0.5781509706125119\n",
      "Train loss: 0.5781920004852479\n",
      "Train loss: 0.5781793644251928\n",
      "Train loss: 0.578199990529203\n",
      "Train loss: 0.5781841333497595\n",
      "Train loss: 0.5781727821838409\n",
      "Train loss: 0.5781675752164622\n",
      "Train loss: 0.5781520126914306\n",
      "Train loss: 0.5781069372490582\n",
      "Train loss: 0.5779649264946424\n",
      "Train loss: 0.5780146014411861\n",
      "Train loss: 0.5780608379602383\n",
      "Train loss: 0.578073899001701\n",
      "Train loss: 0.5781083863245907\n",
      "Train loss: 0.5780371092913443\n",
      "Train loss: 0.5780024098198345\n",
      "Train loss: 0.5779775642387674\n",
      "Train loss: 0.5779909871386494\n",
      "Train loss: 0.5780064119046395\n",
      "Train loss: 0.5780233226644355\n",
      "Train loss: 0.5779900755861609\n",
      "Train loss: 0.5779628649216629\n",
      "Train loss: 0.5779378668571222\n",
      "Train loss: 0.5779172786364506\n",
      "Train loss: 0.5779018038221283\n",
      "Train loss: 0.5778529843922218\n",
      "Train loss: 0.577783314734382\n",
      "Train loss: 0.5778066487770516\n",
      "Train loss: 0.5778347775496515\n",
      "Train loss: 0.5778785161326924\n",
      "Train loss: 0.5778829423648251\n",
      "Train loss: 0.5778725055798785\n",
      "Train loss: 0.5779135798372526\n",
      "Train loss: 0.5778168065982446\n",
      "Train loss: 0.5778200134042707\n",
      "Train loss: 0.5778585286261446\n",
      "Train loss: 0.5778479562642111\n",
      "Train loss: 0.5777716848612228\n",
      "Train loss: 0.5778016418925452\n",
      "Train loss: 0.577848208138392\n",
      "Train loss: 0.5778382374631551\n",
      "Train loss: 0.5779316916013115\n",
      "Train loss: 0.5778515247584556\n",
      "Train loss: 0.5778761299955519\n",
      "Train loss: 0.577873956274699\n",
      "Train loss: 0.5778260503453707\n",
      "Train loss: 0.5778037606658499\n",
      "Train loss: 0.5778263603045564\n",
      "Train loss: 0.5778177458833224\n",
      "Train loss: 0.5778689156374466\n",
      "Train loss: 0.5778492117068742\n",
      "Train loss: 0.5778504955672364\n",
      "Train loss: 0.5778527850714111\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6953311040997505\n",
      "Val loss: 0.6209409866068099\n",
      "Val loss: 0.5951444059610367\n",
      "Val loss: 0.5941002290499838\n",
      "Val loss: 0.5937696980933348\n",
      "Val loss: 0.5967081240538893\n",
      "Val loss: 0.5904781353824279\n",
      "Val loss: 0.5849862580115979\n",
      "Val loss: 0.5806553939526732\n",
      "Val loss: 0.5808379230450611\n",
      "Val loss: 0.581379160284996\n",
      "Val loss: 0.5807095498351728\n",
      "Val loss: 0.5796710955910385\n",
      "Val loss: 0.5791543182255565\n",
      "Val loss: 0.5783013286622795\n",
      "Val loss: 0.5766650570344322\n",
      "Val loss: 0.575362945596377\n",
      "Val loss: 0.5737908003705271\n",
      "Val loss: 0.5746618157371561\n",
      "Val loss: 0.5724331643244233\n",
      "Val loss: 0.5733840451217614\n",
      "Val loss: 0.5718852333519437\n",
      "Val loss: 0.5710903259745815\n",
      "Val loss: 0.5708582010589728\n",
      "Val loss: 0.5708539288851523\n",
      "Val loss: 0.5699604911397594\n",
      "Val loss: 0.5697484785941109\n",
      "Val loss: 0.5686805291141538\n",
      "Val loss: 0.5675989973048369\n",
      "Val loss: 0.5677204040072908\n",
      "Val loss: 0.5674294960963262\n",
      "Val loss: 0.5676807881151356\n",
      "Val loss: 0.5676191198389705\n",
      "Val loss: 0.5668646940112819\n",
      "Val loss: 0.5665481141928969\n",
      "Val loss: 0.5664584723264812\n",
      "Val loss: 0.5648807429749033\n",
      "Val loss: 0.564870987935041\n",
      "Val loss: 0.5655877697713596\n",
      "Val loss: 0.5647523400172516\n",
      "Val loss: 0.5650447401053765\n",
      "Val loss: 0.5645495017179462\n",
      "Val loss: 0.5638587389872453\n",
      "Val loss: 0.5640955878991515\n",
      "Val loss: 0.5636494743770787\n",
      "Val loss: 0.5638584383970785\n",
      "Val loss: 0.5635477150352592\n",
      "Val loss: 0.5634295805727587\n",
      "Val loss: 0.5634201921400477\n",
      "Val loss: 0.5634062084328219\n",
      "Val loss: 0.5628401815421938\n",
      "Val loss: 0.5620854104577805\n",
      "Val loss: 0.5619937372252797\n",
      "Val loss: 0.5615150152972197\n",
      "Val loss: 0.5616516500276371\n",
      "Val loss: 0.5610145671179645\n",
      "Val loss: 0.5610872751600305\n",
      "Val loss: 0.5613736552556906\n",
      "Val loss: 0.5614956525110063\n",
      "Val loss: 0.5613811385870777\n",
      "Val loss: 0.5616337657955132\n",
      "Val loss: 0.5615356679294488\n",
      "Val loss: 0.5611066959655968\n",
      "Val loss: 0.5609317044479346\n",
      "Val loss: 0.5607186029722662\n",
      "Val loss: 0.56073286431901\n",
      "Val loss: 0.5606126740664065\n",
      "Val loss: 0.5602561114987793\n",
      "Val loss: 0.5613586863632812\n",
      "Val loss: 0.5611787735561928\n",
      "Val loss: 0.5606203924464641\n",
      "Val loss: 0.5605873806728958\n",
      "Val loss: 0.5601955191283435\n",
      "Val loss: 0.5604137373649007\n",
      "Val loss: 0.560440042200573\n",
      "Val loss: 0.5599572788285077\n",
      "Val loss: 0.5600737186614424\n",
      "Val loss: 0.5598404867943271\n",
      "Val loss: 0.5594536272553623\n",
      "Val loss: 0.5595668041168299\n",
      "Val loss: 0.559844074051569\n",
      "Val loss: 0.5593641194854214\n",
      "Val loss: 0.5596527894913862\n",
      "Val loss: 0.559298909507105\n",
      "Val loss: 0.5591592525817314\n",
      "Val loss: 0.5593423197319457\n",
      "Val loss: 0.5590911025550508\n",
      "Val loss: 0.559063255651122\n",
      "Val loss: 0.5586489990085095\n",
      "Val loss: 0.5584871250563581\n",
      "Val loss: 0.5586135083489481\n",
      "Val loss: 0.5586230417871787\n",
      "Val loss: 0.5586897462349514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.558711085365271\n",
      "Val loss: 0.5585079758982116\n",
      "Val loss: 0.558669483114135\n",
      "Val loss: 0.5587275128596085\n",
      "Val loss: 0.558669710878458\n",
      "Val loss: 0.5586050017642589\n",
      "Val loss: 0.5591032140718434\n",
      "Val loss: 0.5590750901472001\n",
      "Val loss: 0.5588242200245324\n",
      "Val loss: 0.5590773887555422\n",
      "Val loss: 0.5588194932896278\n",
      "Val loss: 0.5587282346291397\n",
      "Val loss: 0.5586791378225856\n",
      "Val loss: 0.5584033431855033\n",
      "Val loss: 0.5583733737136968\n",
      "Val loss: 0.5581696746213471\n",
      "Val loss: 0.5582554496159753\n",
      "Val loss: 0.5582351834119873\n",
      "Val loss: 0.5583556950198943\n",
      "Val loss: 0.5583076339664189\n",
      "Val loss: 0.5585374811202655\n",
      "Val loss: 0.5583553417530625\n",
      "Val loss: 0.5584812282177657\n",
      "Val loss: 0.5585840019359164\n",
      "Val loss: 0.5585860449106058\n",
      "Val loss: 0.5586733587261804\n",
      "Val loss: 0.5588912119053441\n",
      "Val loss: 0.5591519994056777\n",
      "Val loss: 0.55943393168974\n",
      "Val loss: 0.5596159733467848\n",
      "Val loss: 0.5596529768241242\n",
      "Val loss: 0.5598457410740547\n",
      "Val loss: 0.5599687495557606\n",
      "Val loss: 0.5598590868504641\n",
      "\n",
      "starting Epoch 20\n",
      "Training...\n",
      "Train loss: 0.610522913305383\n",
      "Train loss: 0.5948315369777191\n",
      "Train loss: 0.5882626852746737\n",
      "Train loss: 0.5909697251229347\n",
      "Train loss: 0.5884275454463381\n",
      "Train loss: 0.585167621864992\n",
      "Train loss: 0.5852948238523744\n",
      "Train loss: 0.5851047976211932\n",
      "Train loss: 0.5834325648552878\n",
      "Train loss: 0.5831300190944767\n",
      "Train loss: 0.5841142352857546\n",
      "Train loss: 0.5825647545908286\n",
      "Train loss: 0.5806778430018186\n",
      "Train loss: 0.5803017897204259\n",
      "Train loss: 0.5802702458207822\n",
      "Train loss: 0.581176356462102\n",
      "Train loss: 0.5803052583451116\n",
      "Train loss: 0.5807959867387097\n",
      "Train loss: 0.579462335182054\n",
      "Train loss: 0.5804687444876907\n",
      "Train loss: 0.5813293605687225\n",
      "Train loss: 0.5818306080708471\n",
      "Train loss: 0.5817766826526791\n",
      "Train loss: 0.5824960870259988\n",
      "Train loss: 0.5823375058914712\n",
      "Train loss: 0.5821270143939811\n",
      "Train loss: 0.5818188503412236\n",
      "Train loss: 0.5815677197653407\n",
      "Train loss: 0.581578795413691\n",
      "Train loss: 0.5816150536023714\n",
      "Train loss: 0.5812097799893535\n",
      "Train loss: 0.5809931940222757\n",
      "Train loss: 0.5806837179139822\n",
      "Train loss: 0.5802312380084234\n",
      "Train loss: 0.5801318411919181\n",
      "Train loss: 0.580300790917061\n",
      "Train loss: 0.5806112797963603\n",
      "Train loss: 0.5801898941142443\n",
      "Train loss: 0.5798436999320984\n",
      "Train loss: 0.5798275313329637\n",
      "Train loss: 0.5799120125316438\n",
      "Train loss: 0.5795344520666602\n",
      "Train loss: 0.5801552469572172\n",
      "Train loss: 0.5800428479333515\n",
      "Train loss: 0.5801375333273636\n",
      "Train loss: 0.579999045072624\n",
      "Train loss: 0.5796069947484964\n",
      "Train loss: 0.5797488756547755\n",
      "Train loss: 0.5794185619188645\n",
      "Train loss: 0.5794503954915075\n",
      "Train loss: 0.5794298872863463\n",
      "Train loss: 0.5796145581074698\n",
      "Train loss: 0.5795592342602746\n",
      "Train loss: 0.5795481845501731\n",
      "Train loss: 0.5795725164519754\n",
      "Train loss: 0.5793444439435879\n",
      "Train loss: 0.5792506295824595\n",
      "Train loss: 0.5792507198836085\n",
      "Train loss: 0.5792707541011166\n",
      "Train loss: 0.5793563564982983\n",
      "Train loss: 0.579243519518393\n",
      "Train loss: 0.579527449641524\n",
      "Train loss: 0.5793171669160496\n",
      "Train loss: 0.5789899198546644\n",
      "Train loss: 0.5787921260190616\n",
      "Train loss: 0.578589223003279\n",
      "Train loss: 0.5786173814619718\n",
      "Train loss: 0.5785317732140455\n",
      "Train loss: 0.5780574550569879\n",
      "Train loss: 0.5781460208412236\n",
      "Train loss: 0.5781194177876904\n",
      "Train loss: 0.5780358510020709\n",
      "Train loss: 0.5780812685950477\n",
      "Train loss: 0.577952133573496\n",
      "Train loss: 0.5778881158130498\n",
      "Train loss: 0.5779821431464157\n",
      "Train loss: 0.5780195235005441\n",
      "Train loss: 0.577770285800606\n",
      "Train loss: 0.5776424661250413\n",
      "Train loss: 0.5776698336219549\n",
      "Train loss: 0.5774766168084831\n",
      "Train loss: 0.5774857163829291\n",
      "Train loss: 0.5775943709073691\n",
      "Train loss: 0.5775309266647887\n",
      "Train loss: 0.5776953920151923\n",
      "Train loss: 0.5776549930867932\n",
      "Train loss: 0.5777086701764671\n",
      "Train loss: 0.5774428290423512\n",
      "Train loss: 0.5775144531058086\n",
      "Train loss: 0.5774878723347034\n",
      "Train loss: 0.5774859283047761\n",
      "Train loss: 0.5774320406295347\n",
      "Train loss: 0.5773689537326645\n",
      "Train loss: 0.5771376735958538\n",
      "Train loss: 0.5770505217422367\n",
      "Train loss: 0.577009133897516\n",
      "Train loss: 0.5770963758399279\n",
      "Train loss: 0.5771503516336435\n",
      "Train loss: 0.5772993652853116\n",
      "Train loss: 0.5773292907242061\n",
      "Train loss: 0.5774249647028556\n",
      "Train loss: 0.5774666826885199\n",
      "Train loss: 0.5776842976872814\n",
      "Train loss: 0.5778432047642327\n",
      "Train loss: 0.5779181934215161\n",
      "Train loss: 0.577992478945504\n",
      "Train loss: 0.5778682163632212\n",
      "Train loss: 0.5777889408271254\n",
      "Train loss: 0.5777333634689676\n",
      "Train loss: 0.5777134764948016\n",
      "Train loss: 0.5776734602993917\n",
      "Train loss: 0.5775511777922744\n",
      "Train loss: 0.5778819332744443\n",
      "Train loss: 0.5778939418622201\n",
      "Train loss: 0.5780112490984812\n",
      "Train loss: 0.5778426326981594\n",
      "Train loss: 0.577857611741506\n",
      "Train loss: 0.5776610293975569\n",
      "Train loss: 0.5775185370580546\n",
      "Train loss: 0.5774597017876354\n",
      "Train loss: 0.5778825373603274\n",
      "Train loss: 0.578092659995809\n",
      "Train loss: 0.5783068848824588\n",
      "Train loss: 0.578332460013163\n",
      "Train loss: 0.5783307839031456\n",
      "Train loss: 0.578374504321338\n",
      "Train loss: 0.5782927359098898\n",
      "Train loss: 0.5782388832570055\n",
      "Train loss: 0.5782617639205677\n",
      "Train loss: 0.5782190086911302\n",
      "Train loss: 0.5783122814238731\n",
      "Train loss: 0.5783522573552741\n",
      "Train loss: 0.5783158340324984\n",
      "Train loss: 0.578280313996603\n",
      "Train loss: 0.5782843105392838\n",
      "Train loss: 0.5783706573648196\n",
      "Train loss: 0.5784908719918466\n",
      "Train loss: 0.5785695998921002\n",
      "Train loss: 0.5785321991471087\n",
      "Train loss: 0.5783468669506003\n",
      "Train loss: 0.5784250081706106\n",
      "Train loss: 0.5783526505244874\n",
      "Train loss: 0.5782869149751286\n",
      "Train loss: 0.5783879726390203\n",
      "Train loss: 0.5785035645295604\n",
      "Train loss: 0.5785160380077101\n",
      "Train loss: 0.578438834821176\n",
      "Train loss: 0.5784756097389904\n",
      "Train loss: 0.5785169987099889\n",
      "Train loss: 0.5785892018579412\n",
      "Train loss: 0.5787564017651057\n",
      "Train loss: 0.5788714938177094\n",
      "Train loss: 0.5789513573825028\n",
      "Train loss: 0.5788629895663563\n",
      "Train loss: 0.578765108695758\n",
      "Train loss: 0.5788222945779896\n",
      "Train loss: 0.5787566620458954\n",
      "Train loss: 0.5788072448774545\n",
      "Train loss: 0.5787396969548484\n",
      "Train loss: 0.5787050627234878\n",
      "Train loss: 0.5786323085746694\n",
      "Train loss: 0.5785411532994735\n",
      "Train loss: 0.578497311406137\n",
      "Train loss: 0.5784717462309326\n",
      "Train loss: 0.5785195985664994\n",
      "Train loss: 0.5784262143171993\n",
      "Train loss: 0.5784765270681715\n",
      "Train loss: 0.5785080581575179\n",
      "Train loss: 0.5784936504591214\n",
      "Train loss: 0.5784837494144793\n",
      "Train loss: 0.5784903937489291\n",
      "Train loss: 0.5786148661373313\n",
      "Train loss: 0.5785772295124305\n",
      "Train loss: 0.5787154669715742\n",
      "Train loss: 0.5787771977792981\n",
      "Train loss: 0.5786727099856994\n",
      "Train loss: 0.5786497658358081\n",
      "Train loss: 0.5787807393606609\n",
      "Train loss: 0.5787156300775896\n",
      "Train loss: 0.5786236224902541\n",
      "Train loss: 0.5786386507507449\n",
      "Train loss: 0.5786090739057037\n",
      "Train loss: 0.5785217936588265\n",
      "Train loss: 0.5784083917579045\n",
      "Train loss: 0.578548330097593\n",
      "Train loss: 0.5784624985434991\n",
      "Train loss: 0.578463438034504\n",
      "Train loss: 0.5784300536781463\n",
      "Train loss: 0.5783786905934868\n",
      "Train loss: 0.5783117341897964\n",
      "Train loss: 0.5783622308640806\n",
      "Train loss: 0.5783426463992125\n",
      "Train loss: 0.5783128774082701\n",
      "Train loss: 0.5783263250981818\n",
      "Train loss: 0.5783636699764813\n",
      "Train loss: 0.5783779440743791\n",
      "Train loss: 0.578391044529963\n",
      "Train loss: 0.5783604793203513\n",
      "Train loss: 0.578350312013547\n",
      "Train loss: 0.5783215852104505\n",
      "Train loss: 0.5782684729700808\n",
      "Train loss: 0.578287741864309\n",
      "Train loss: 0.5782443635582366\n",
      "Train loss: 0.5782701446455347\n",
      "Train loss: 0.5782458521689284\n",
      "Train loss: 0.5781854689917133\n",
      "Train loss: 0.5782997212524603\n",
      "Train loss: 0.5782352539890516\n",
      "Train loss: 0.578229814903335\n",
      "Train loss: 0.5782503267077214\n",
      "Train loss: 0.5781562280657733\n",
      "Train loss: 0.5781771540009039\n",
      "Train loss: 0.5781359190489099\n",
      "Train loss: 0.5781639466346439\n",
      "Train loss: 0.5781264859150942\n",
      "Train loss: 0.5781066418798138\n",
      "Train loss: 0.5781048767218862\n",
      "Train loss: 0.5780951882836249\n",
      "Train loss: 0.5780356502963081\n",
      "Train loss: 0.578076855144221\n",
      "Train loss: 0.5780782924232021\n",
      "Train loss: 0.5781522365498849\n",
      "Train loss: 0.5781762212895845\n",
      "Train loss: 0.5781731988189209\n",
      "Train loss: 0.578164126448272\n",
      "Train loss: 0.5780707344620543\n",
      "Train loss: 0.5780331593012594\n",
      "Train loss: 0.5779489049605043\n",
      "Train loss: 0.5778295343087265\n",
      "Train loss: 0.5778453766223943\n",
      "Train loss: 0.5779286995868864\n",
      "Train loss: 0.5778397106487038\n",
      "Train loss: 0.5778677016528642\n",
      "Train loss: 0.5778081030526154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5778333502681896\n",
      "Train loss: 0.5777778442531452\n",
      "Train loss: 0.5777332711934188\n",
      "Train loss: 0.5777417049923472\n",
      "Train loss: 0.5776725707255689\n",
      "Train loss: 0.5777092170134066\n",
      "Train loss: 0.577728306667071\n",
      "Train loss: 0.577712546753031\n",
      "Train loss: 0.5777246109753338\n",
      "Train loss: 0.5777814558390884\n",
      "Train loss: 0.5778325851181619\n",
      "Train loss: 0.5778795406267783\n",
      "Train loss: 0.577952660615687\n",
      "Train loss: 0.578043021581711\n",
      "Train loss: 0.5780463989395338\n",
      "Train loss: 0.5780331865206412\n",
      "Train loss: 0.5780353926465103\n",
      "Train loss: 0.5780499597217006\n",
      "Train loss: 0.5780113575769098\n",
      "Train loss: 0.5780019780668975\n",
      "Train loss: 0.5780339261835101\n",
      "Train loss: 0.5780352694980333\n",
      "Train loss: 0.5779959827365753\n",
      "Train loss: 0.5779849820576982\n",
      "Train loss: 0.5779226495103731\n",
      "Train loss: 0.5778783166791458\n",
      "Train loss: 0.5779243862864203\n",
      "Train loss: 0.5779409686341316\n",
      "Train loss: 0.5778696889670136\n",
      "Train loss: 0.5778809179691095\n",
      "Train loss: 0.5778183603336235\n",
      "Train loss: 0.5777977152378583\n",
      "Train loss: 0.5777201950382619\n",
      "Train loss: 0.5776889262403608\n",
      "Train loss: 0.5776923661239529\n",
      "Train loss: 0.577710295633202\n",
      "Train loss: 0.5777620453855652\n",
      "Train loss: 0.5777729245577036\n",
      "Train loss: 0.5778151667364833\n",
      "Train loss: 0.5777587676204933\n",
      "Train loss: 0.5777515079524652\n",
      "Train loss: 0.5777310160196093\n",
      "Train loss: 0.5777669421364026\n",
      "Train loss: 0.5777238828337307\n",
      "Train loss: 0.5778285453964421\n",
      "Train loss: 0.5778303194640044\n",
      "Train loss: 0.5778147437316198\n",
      "Train loss: 0.5778117121893752\n",
      "Train loss: 0.5778419220641342\n",
      "Train loss: 0.5778385008760564\n",
      "Train loss: 0.5778151305543057\n",
      "Train loss: 0.5777999464709727\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6980691105127335\n",
      "Val loss: 0.6221062541007996\n",
      "Val loss: 0.5924934936421258\n",
      "Val loss: 0.5926839348516966\n",
      "Val loss: 0.5927123688161373\n",
      "Val loss: 0.5954206061774286\n",
      "Val loss: 0.5898419259225621\n",
      "Val loss: 0.5848399454202408\n",
      "Val loss: 0.5801230459050699\n",
      "Val loss: 0.5803652509134642\n",
      "Val loss: 0.5806064423587587\n",
      "Val loss: 0.5794986511691141\n",
      "Val loss: 0.578903874848038\n",
      "Val loss: 0.5786747202493142\n",
      "Val loss: 0.5780284360453889\n",
      "Val loss: 0.5766440362115449\n",
      "Val loss: 0.5752593757850784\n",
      "Val loss: 0.5742230418692814\n",
      "Val loss: 0.574999396788313\n",
      "Val loss: 0.5729228803003678\n",
      "Val loss: 0.5733954794704914\n",
      "Val loss: 0.5719842270973625\n",
      "Val loss: 0.5708903529142079\n",
      "Val loss: 0.5704468072963362\n",
      "Val loss: 0.5701729436555216\n",
      "Val loss: 0.5687347740627998\n",
      "Val loss: 0.5684336339359852\n",
      "Val loss: 0.5674063691989981\n",
      "Val loss: 0.5667272868255774\n",
      "Val loss: 0.5668757649876127\n",
      "Val loss: 0.5665066211254566\n",
      "Val loss: 0.5665610151470832\n",
      "Val loss: 0.5664688228833966\n",
      "Val loss: 0.5658918349347877\n",
      "Val loss: 0.5656074744531479\n",
      "Val loss: 0.5655783171760304\n",
      "Val loss: 0.5640129843807739\n",
      "Val loss: 0.5639572413194747\n",
      "Val loss: 0.564763422018474\n",
      "Val loss: 0.5640439811962933\n",
      "Val loss: 0.5643947264727425\n",
      "Val loss: 0.5639848922998711\n",
      "Val loss: 0.5635398794835973\n",
      "Val loss: 0.56376101859084\n",
      "Val loss: 0.5634020682690399\n",
      "Val loss: 0.5636342806847335\n",
      "Val loss: 0.5632699629944614\n",
      "Val loss: 0.5633573402420746\n",
      "Val loss: 0.5634438397942997\n",
      "Val loss: 0.5636053516204098\n",
      "Val loss: 0.5629955198117128\n",
      "Val loss: 0.5623630109210732\n",
      "Val loss: 0.5621744160625067\n",
      "Val loss: 0.5617815855488901\n",
      "Val loss: 0.5618044415312092\n",
      "Val loss: 0.5613303849987659\n",
      "Val loss: 0.5614420989869346\n",
      "Val loss: 0.5615464861417724\n",
      "Val loss: 0.5616012558239658\n",
      "Val loss: 0.5615317674982907\n",
      "Val loss: 0.5616621858391323\n",
      "Val loss: 0.5617246262271042\n",
      "Val loss: 0.5612701654054557\n",
      "Val loss: 0.5609489946529783\n",
      "Val loss: 0.5607830484708151\n",
      "Val loss: 0.5608031117445067\n",
      "Val loss: 0.5606454962384915\n",
      "Val loss: 0.560243417028129\n",
      "Val loss: 0.561090987955415\n",
      "Val loss: 0.5609494574090471\n",
      "Val loss: 0.5603057593949097\n",
      "Val loss: 0.5602277433473751\n",
      "Val loss: 0.5599527865812018\n",
      "Val loss: 0.5602320210558935\n",
      "Val loss: 0.5602780821967253\n",
      "Val loss: 0.5598927941517339\n",
      "Val loss: 0.5600621075524638\n",
      "Val loss: 0.5598622871548473\n",
      "Val loss: 0.5594608215662429\n",
      "Val loss: 0.5595544094878032\n",
      "Val loss: 0.5597979043527405\n",
      "Val loss: 0.5593812676251372\n",
      "Val loss: 0.5595527444628702\n",
      "Val loss: 0.5592896575574943\n",
      "Val loss: 0.5590463523875993\n",
      "Val loss: 0.5591743867714088\n",
      "Val loss: 0.5590096838463287\n",
      "Val loss: 0.5590171758976504\n",
      "Val loss: 0.5585864634664209\n",
      "Val loss: 0.5583818722806688\n",
      "Val loss: 0.5585639852915566\n",
      "Val loss: 0.5586487906041488\n",
      "Val loss: 0.5586878158922853\n",
      "Val loss: 0.558606973589102\n",
      "Val loss: 0.5583935680520182\n",
      "Val loss: 0.5586293434300353\n",
      "Val loss: 0.5587186794881979\n",
      "Val loss: 0.5587712311793447\n",
      "Val loss: 0.5587541521319493\n",
      "Val loss: 0.5592216892328434\n",
      "Val loss: 0.5591634522591319\n",
      "Val loss: 0.5589672763830084\n",
      "Val loss: 0.5592762192399585\n",
      "Val loss: 0.5590579725644033\n",
      "Val loss: 0.5589651021793598\n",
      "Val loss: 0.558910109821024\n",
      "Val loss: 0.5585918590593874\n",
      "Val loss: 0.5585567328615844\n",
      "Val loss: 0.5583721005412585\n",
      "Val loss: 0.558454797893274\n",
      "Val loss: 0.5585056364213516\n",
      "Val loss: 0.5585782106212726\n",
      "Val loss: 0.5585293751870487\n",
      "Val loss: 0.5587539767129559\n",
      "Val loss: 0.5585971117435017\n",
      "Val loss: 0.5587309410534992\n",
      "Val loss: 0.5588563959280105\n",
      "Val loss: 0.5588877658487783\n",
      "Val loss: 0.5589899655544397\n",
      "Val loss: 0.5592347697941011\n",
      "Val loss: 0.5594229412986743\n",
      "Val loss: 0.5596990977797798\n",
      "Val loss: 0.5597876234629255\n",
      "Val loss: 0.5598125797673843\n",
      "Val loss: 0.5599899592881019\n",
      "Val loss: 0.5600842373928319\n",
      "Val loss: 0.5599870542622515\n",
      "\n",
      "starting Epoch 21\n",
      "Training...\n",
      "Train loss: 0.5927201619273738\n",
      "Train loss: 0.5659647683302561\n",
      "Train loss: 0.570948358309471\n",
      "Train loss: 0.5727109697800649\n",
      "Train loss: 0.5706355020855413\n",
      "Train loss: 0.5684480446727336\n",
      "Train loss: 0.5724307558090567\n",
      "Train loss: 0.5732740248149296\n",
      "Train loss: 0.572712365974927\n",
      "Train loss: 0.5723809879928378\n",
      "Train loss: 0.5732845638168457\n",
      "Train loss: 0.5740946310584016\n",
      "Train loss: 0.5741440266025573\n",
      "Train loss: 0.5758527506636889\n",
      "Train loss: 0.5755768232919699\n",
      "Train loss: 0.574456644469294\n",
      "Train loss: 0.5749724409749023\n",
      "Train loss: 0.5750953639782239\n",
      "Train loss: 0.5747462360871499\n",
      "Train loss: 0.5745888760215357\n",
      "Train loss: 0.5741353738421756\n",
      "Train loss: 0.573982342296961\n",
      "Train loss: 0.5739401957438143\n",
      "Train loss: 0.5751796841994705\n",
      "Train loss: 0.5750827158620219\n",
      "Train loss: 0.5744270461257942\n",
      "Train loss: 0.574307716899104\n",
      "Train loss: 0.5743731748652586\n",
      "Train loss: 0.5735366452028509\n",
      "Train loss: 0.5737140833933485\n",
      "Train loss: 0.5738161510046156\n",
      "Train loss: 0.5739256942962444\n",
      "Train loss: 0.5736722037173547\n",
      "Train loss: 0.5747749041738496\n",
      "Train loss: 0.5755456789198181\n",
      "Train loss: 0.5756724848203434\n",
      "Train loss: 0.5762625841552415\n",
      "Train loss: 0.5764698187510172\n",
      "Train loss: 0.5766178190478618\n",
      "Train loss: 0.5771451735675559\n",
      "Train loss: 0.5768194271385743\n",
      "Train loss: 0.57667946652093\n",
      "Train loss: 0.5762812113248428\n",
      "Train loss: 0.576387148295382\n",
      "Train loss: 0.5765948205423832\n",
      "Train loss: 0.5759893888942046\n",
      "Train loss: 0.576083768829624\n",
      "Train loss: 0.5760444750751021\n",
      "Train loss: 0.576101409159106\n",
      "Train loss: 0.5761360913902909\n",
      "Train loss: 0.5768758176996851\n",
      "Train loss: 0.5772192632784857\n",
      "Train loss: 0.5767071245130894\n",
      "Train loss: 0.5765358513157273\n",
      "Train loss: 0.5766519551878089\n",
      "Train loss: 0.5769699963743501\n",
      "Train loss: 0.5768119190689552\n",
      "Train loss: 0.577043933252305\n",
      "Train loss: 0.5769232446051331\n",
      "Train loss: 0.5772438597390809\n",
      "Train loss: 0.577720113055837\n",
      "Train loss: 0.5776662824373269\n",
      "Train loss: 0.5774379270336191\n",
      "Train loss: 0.5770221807883995\n",
      "Train loss: 0.5771789224997588\n",
      "Train loss: 0.5775183732759058\n",
      "Train loss: 0.5772612013417986\n",
      "Train loss: 0.5773788419955789\n",
      "Train loss: 0.5774518908842307\n",
      "Train loss: 0.5776247666340543\n",
      "Train loss: 0.5779207569978202\n",
      "Train loss: 0.578001384543743\n",
      "Train loss: 0.5776691353427293\n",
      "Train loss: 0.5775954329016404\n",
      "Train loss: 0.5778389475479215\n",
      "Train loss: 0.5779499695872381\n",
      "Train loss: 0.5780645170484447\n",
      "Train loss: 0.5784232832056502\n",
      "Train loss: 0.5783690191508094\n",
      "Train loss: 0.5781494301695462\n",
      "Train loss: 0.5783145904099227\n",
      "Train loss: 0.5784759859328884\n",
      "Train loss: 0.578337906749224\n",
      "Train loss: 0.5783880636619627\n",
      "Train loss: 0.5782169170804835\n",
      "Train loss: 0.5780919159027363\n",
      "Train loss: 0.5782443795860877\n",
      "Train loss: 0.5783462871401875\n",
      "Train loss: 0.5783295209586051\n",
      "Train loss: 0.5782456575929622\n",
      "Train loss: 0.5782931499457609\n",
      "Train loss: 0.5782929544963546\n",
      "Train loss: 0.5782605693217181\n",
      "Train loss: 0.5782703120477534\n",
      "Train loss: 0.5782667628522293\n",
      "Train loss: 0.5783111971141522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5782526337098313\n",
      "Train loss: 0.5784474408949073\n",
      "Train loss: 0.5783415803438248\n",
      "Train loss: 0.578337275099313\n",
      "Train loss: 0.5783974289628407\n",
      "Train loss: 0.5786113760207084\n",
      "Train loss: 0.5787887283035017\n",
      "Train loss: 0.5786748443044338\n",
      "Train loss: 0.578637440327294\n",
      "Train loss: 0.5785533108200426\n",
      "Train loss: 0.5787405485915157\n",
      "Train loss: 0.5786916045968099\n",
      "Train loss: 0.5785043543087556\n",
      "Train loss: 0.5786464192234099\n",
      "Train loss: 0.5786286088301825\n",
      "Train loss: 0.5784985094983006\n",
      "Train loss: 0.5784423358807684\n",
      "Train loss: 0.5784233223194091\n",
      "Train loss: 0.5785103954663221\n",
      "Train loss: 0.5785326731261533\n",
      "Train loss: 0.5784892424807278\n",
      "Train loss: 0.5783227894734709\n",
      "Train loss: 0.5784105298623361\n",
      "Train loss: 0.5784610141039193\n",
      "Train loss: 0.5782543425966068\n",
      "Train loss: 0.5782170092389934\n",
      "Train loss: 0.5783621032168197\n",
      "Train loss: 0.5784313934687983\n",
      "Train loss: 0.5783162796292223\n",
      "Train loss: 0.5783053046656582\n",
      "Train loss: 0.5783159534684219\n",
      "Train loss: 0.5784761416651206\n",
      "Train loss: 0.5785999356336767\n",
      "Train loss: 0.578595318373188\n",
      "Train loss: 0.5787026739703103\n",
      "Train loss: 0.5788583012185525\n",
      "Train loss: 0.5788559494116289\n",
      "Train loss: 0.5787359715709494\n",
      "Train loss: 0.5787421686974399\n",
      "Train loss: 0.5787779981158769\n",
      "Train loss: 0.5788046522987459\n",
      "Train loss: 0.5789214193648123\n",
      "Train loss: 0.5790749845872812\n",
      "Train loss: 0.5790959653172079\n",
      "Train loss: 0.5791917226444792\n",
      "Train loss: 0.5791471230286702\n",
      "Train loss: 0.5792149116564314\n",
      "Train loss: 0.5791348633914409\n",
      "Train loss: 0.5790920336109147\n",
      "Train loss: 0.5790513733777741\n",
      "Train loss: 0.578960830498488\n",
      "Train loss: 0.5789614976076872\n",
      "Train loss: 0.579064779211187\n",
      "Train loss: 0.5790001256102599\n",
      "Train loss: 0.578950799431537\n",
      "Train loss: 0.5788953287913714\n",
      "Train loss: 0.5787524361554296\n",
      "Train loss: 0.5787435460106124\n",
      "Train loss: 0.5787174325483235\n",
      "Train loss: 0.5787471401343631\n",
      "Train loss: 0.5787771990455689\n",
      "Train loss: 0.578736614628131\n",
      "Train loss: 0.578624432812175\n",
      "Train loss: 0.5785954911640265\n",
      "Train loss: 0.5786926061813044\n",
      "Train loss: 0.5786715441277896\n",
      "Train loss: 0.5787120523921695\n",
      "Train loss: 0.578790323268571\n",
      "Train loss: 0.5787399621868393\n",
      "Train loss: 0.5787827690149079\n",
      "Train loss: 0.578893305290313\n",
      "Train loss: 0.5789513145308652\n",
      "Train loss: 0.5790597827221452\n",
      "Train loss: 0.5790228376443963\n",
      "Train loss: 0.5791226304494418\n",
      "Train loss: 0.579081526968014\n",
      "Train loss: 0.5789888046421251\n",
      "Train loss: 0.5789282535660297\n",
      "Train loss: 0.5789195490448977\n",
      "Train loss: 0.5788809260664152\n",
      "Train loss: 0.5788224751782909\n",
      "Train loss: 0.5788491107868861\n",
      "Train loss: 0.5789455495239736\n",
      "Train loss: 0.5789347714551722\n",
      "Train loss: 0.5789957582275326\n",
      "Train loss: 0.5789995941364952\n",
      "Train loss: 0.5789726468245493\n",
      "Train loss: 0.5789204204364392\n",
      "Train loss: 0.5788164598034923\n",
      "Train loss: 0.5787084721034815\n",
      "Train loss: 0.5787874923555186\n",
      "Train loss: 0.57881454204143\n",
      "Train loss: 0.5788488287195764\n",
      "Train loss: 0.5788266177034967\n",
      "Train loss: 0.5788581305210052\n",
      "Train loss: 0.5788143987409086\n",
      "Train loss: 0.5788693010467234\n",
      "Train loss: 0.5788956421357081\n",
      "Train loss: 0.5788705364714283\n",
      "Train loss: 0.5786888993616827\n",
      "Train loss: 0.5787646751411072\n",
      "Train loss: 0.5787419113781032\n",
      "Train loss: 0.5787386130930102\n",
      "Train loss: 0.5786265412563978\n",
      "Train loss: 0.5786064161786276\n",
      "Train loss: 0.5786479202480297\n",
      "Train loss: 0.5787365032067643\n",
      "Train loss: 0.5786323566944116\n",
      "Train loss: 0.5786312109495145\n",
      "Train loss: 0.5785997309175965\n",
      "Train loss: 0.5786084949639029\n",
      "Train loss: 0.5786087963336877\n",
      "Train loss: 0.5785532448958711\n",
      "Train loss: 0.5785420950778072\n",
      "Train loss: 0.5785625346268091\n",
      "Train loss: 0.5785695273489794\n",
      "Train loss: 0.5786304642236633\n",
      "Train loss: 0.5785306938537568\n",
      "Train loss: 0.5786682433043837\n",
      "Train loss: 0.5787043395790962\n",
      "Train loss: 0.5786802481330191\n",
      "Train loss: 0.5786518812740197\n",
      "Train loss: 0.5785218061554729\n",
      "Train loss: 0.5785381186005744\n",
      "Train loss: 0.5785673826116867\n",
      "Train loss: 0.5785328535231741\n",
      "Train loss: 0.5784982548791429\n",
      "Train loss: 0.5784907996268591\n",
      "Train loss: 0.5785386600569106\n",
      "Train loss: 0.5785528269653485\n",
      "Train loss: 0.5785373897378102\n",
      "Train loss: 0.578565131718455\n",
      "Train loss: 0.5785229565743719\n",
      "Train loss: 0.5785314086780105\n",
      "Train loss: 0.5786170360329193\n",
      "Train loss: 0.5786256516229029\n",
      "Train loss: 0.5785489036535905\n",
      "Train loss: 0.5786552153659901\n",
      "Train loss: 0.5786909400072218\n",
      "Train loss: 0.5786925234527349\n",
      "Train loss: 0.5785842615105629\n",
      "Train loss: 0.5786169463908329\n",
      "Train loss: 0.5786270264939151\n",
      "Train loss: 0.578616285844504\n",
      "Train loss: 0.5787123575092327\n",
      "Train loss: 0.5785916885651693\n",
      "Train loss: 0.5786661161981902\n",
      "Train loss: 0.5786424527331487\n",
      "Train loss: 0.5786053459482258\n",
      "Train loss: 0.578615349097377\n",
      "Train loss: 0.5787472453320598\n",
      "Train loss: 0.5786957487352771\n",
      "Train loss: 0.5786929677698263\n",
      "Train loss: 0.5787109213606026\n",
      "Train loss: 0.5787734906793139\n",
      "Train loss: 0.5787660063827149\n",
      "Train loss: 0.5786845461727014\n",
      "Train loss: 0.5786515185625963\n",
      "Train loss: 0.578644304205142\n",
      "Train loss: 0.5786553856904536\n",
      "Train loss: 0.5787156808524773\n",
      "Train loss: 0.5786353733609365\n",
      "Train loss: 0.5787008342698726\n",
      "Train loss: 0.5787622463269793\n",
      "Train loss: 0.5787713844718136\n",
      "Train loss: 0.5787445568792645\n",
      "Train loss: 0.5786885588845502\n",
      "Train loss: 0.5786761940203058\n",
      "Train loss: 0.5786426350977718\n",
      "Train loss: 0.5785365712151848\n",
      "Train loss: 0.5784897079771881\n",
      "Train loss: 0.5784644879140746\n",
      "Train loss: 0.578439084426203\n",
      "Train loss: 0.5784964244781677\n",
      "Train loss: 0.5784818961141206\n",
      "Train loss: 0.5784774350098639\n",
      "Train loss: 0.5784565928513411\n",
      "Train loss: 0.5784831209260196\n",
      "Train loss: 0.5784751412965619\n",
      "Train loss: 0.5785472530680607\n",
      "Train loss: 0.5785453577057983\n",
      "Train loss: 0.578618563461012\n",
      "Train loss: 0.5785846549414632\n",
      "Train loss: 0.5785250441903279\n",
      "Train loss: 0.578463114274314\n",
      "Train loss: 0.5785072499406868\n",
      "Train loss: 0.5784490379918104\n",
      "Train loss: 0.57844776089898\n",
      "Train loss: 0.5784605557792206\n",
      "Train loss: 0.5784786345711078\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6961176469922066\n",
      "Val loss: 0.6193758679760827\n",
      "Val loss: 0.5919565856456757\n",
      "Val loss: 0.5924986663617586\n",
      "Val loss: 0.5929542531569799\n",
      "Val loss: 0.5955390251916031\n",
      "Val loss: 0.5892267367419075\n",
      "Val loss: 0.5843285520871481\n",
      "Val loss: 0.5789170400662855\n",
      "Val loss: 0.5791479750555388\n",
      "Val loss: 0.5797311155884354\n",
      "Val loss: 0.5786212723133928\n",
      "Val loss: 0.5778500894084573\n",
      "Val loss: 0.5775385023891062\n",
      "Val loss: 0.5765220442333737\n",
      "Val loss: 0.5750261077397987\n",
      "Val loss: 0.5736514755657741\n",
      "Val loss: 0.572780935951833\n",
      "Val loss: 0.5737733301964212\n",
      "Val loss: 0.5715541300749538\n",
      "Val loss: 0.5721589954426656\n",
      "Val loss: 0.5707753013580217\n",
      "Val loss: 0.5697152632893178\n",
      "Val loss: 0.569159891174621\n",
      "Val loss: 0.5690378017963902\n",
      "Val loss: 0.5676191014836925\n",
      "Val loss: 0.5672667920589447\n",
      "Val loss: 0.5661379789276947\n",
      "Val loss: 0.5655035624901453\n",
      "Val loss: 0.5656299923090327\n",
      "Val loss: 0.5653556894946408\n",
      "Val loss: 0.5653237179390289\n",
      "Val loss: 0.5651424403597669\n",
      "Val loss: 0.5645071917031643\n",
      "Val loss: 0.5642156744825428\n",
      "Val loss: 0.5642334246102658\n",
      "Val loss: 0.5626784627852233\n",
      "Val loss: 0.5625997444309255\n",
      "Val loss: 0.5633157602290517\n",
      "Val loss: 0.5625796894631793\n",
      "Val loss: 0.5629583771030108\n",
      "Val loss: 0.5625288911413348\n",
      "Val loss: 0.5620132921455062\n",
      "Val loss: 0.5621432155778964\n",
      "Val loss: 0.5616784949920007\n",
      "Val loss: 0.5619348730062294\n",
      "Val loss: 0.5616458506665678\n",
      "Val loss: 0.5617486149945519\n",
      "Val loss: 0.5618065584634171\n",
      "Val loss: 0.5619883696477576\n",
      "Val loss: 0.5613792045848576\n",
      "Val loss: 0.5607260701509056\n",
      "Val loss: 0.5605788073982253\n",
      "Val loss: 0.5602567286739563\n",
      "Val loss: 0.5603406194352756\n",
      "Val loss: 0.5597769524034206\n",
      "Val loss: 0.5598236367526189\n",
      "Val loss: 0.5600091096232919\n",
      "Val loss: 0.5600606294310823\n",
      "Val loss: 0.5599427669742035\n",
      "Val loss: 0.5601060321848643\n",
      "Val loss: 0.5601804422329159\n",
      "Val loss: 0.5596957005512943\n",
      "Val loss: 0.5593669334175445\n",
      "Val loss: 0.5592098356581029\n",
      "Val loss: 0.5592831197480662\n",
      "Val loss: 0.5591370342912788\n",
      "Val loss: 0.5587294242375016\n",
      "Val loss: 0.5595224073459936\n",
      "Val loss: 0.5593717584978886\n",
      "Val loss: 0.5587059587745343\n",
      "Val loss: 0.5586669446698138\n",
      "Val loss: 0.5583979683099213\n",
      "Val loss: 0.5587152850821735\n",
      "Val loss: 0.5588054146199303\n",
      "Val loss: 0.5584725293289073\n",
      "Val loss: 0.5585980981122702\n",
      "Val loss: 0.5583356476381689\n",
      "Val loss: 0.5579456764126792\n",
      "Val loss: 0.5580486050225738\n",
      "Val loss: 0.558352399875622\n",
      "Val loss: 0.5579078016042126\n",
      "Val loss: 0.5580745985110601\n",
      "Val loss: 0.5578095587881766\n",
      "Val loss: 0.5575709269856507\n",
      "Val loss: 0.5577011368213556\n",
      "Val loss: 0.5575562602913324\n",
      "Val loss: 0.5575954595703742\n",
      "Val loss: 0.5570989255835345\n",
      "Val loss: 0.5569430097174273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5571130887002147\n",
      "Val loss: 0.5571846767188677\n",
      "Val loss: 0.5572349355909331\n",
      "Val loss: 0.5571731274316052\n",
      "Val loss: 0.5569045840436396\n",
      "Val loss: 0.5571492548277582\n",
      "Val loss: 0.5572583755432081\n",
      "Val loss: 0.5573090651283966\n",
      "Val loss: 0.5572880911320327\n",
      "Val loss: 0.5577854965038911\n",
      "Val loss: 0.5577097703658399\n",
      "Val loss: 0.5575256677296869\n",
      "Val loss: 0.5578943789585091\n",
      "Val loss: 0.5576854930447706\n",
      "Val loss: 0.5576009288543963\n",
      "Val loss: 0.5575262549243487\n",
      "Val loss: 0.5571924790684204\n",
      "Val loss: 0.5571426510368517\n",
      "Val loss: 0.556956525070264\n",
      "Val loss: 0.5570806153593602\n",
      "Val loss: 0.5571165796544147\n",
      "Val loss: 0.5571976518908213\n",
      "Val loss: 0.5571600087251224\n",
      "Val loss: 0.557358610242448\n",
      "Val loss: 0.5571843144266447\n",
      "Val loss: 0.5573106387015657\n",
      "Val loss: 0.5574346177075824\n",
      "Val loss: 0.5574638984705272\n",
      "Val loss: 0.5575877923255015\n",
      "Val loss: 0.5578132984236206\n",
      "Val loss: 0.557989499229469\n",
      "Val loss: 0.5582754675586431\n",
      "Val loss: 0.5583394954666641\n",
      "Val loss: 0.5583486149210922\n",
      "Val loss: 0.55847586132586\n",
      "Val loss: 0.5585838498011682\n",
      "Val loss: 0.5584735484435355\n",
      "\n",
      "starting Epoch 22\n",
      "Training...\n",
      "Train loss: 0.5949823040711252\n",
      "Train loss: 0.5751983928374755\n",
      "Train loss: 0.5791846357159696\n",
      "Train loss: 0.5760947726195371\n",
      "Train loss: 0.5779989394876692\n",
      "Train loss: 0.5764050689064154\n",
      "Train loss: 0.578236357342425\n",
      "Train loss: 0.5780517042807813\n",
      "Train loss: 0.5783927114316205\n",
      "Train loss: 0.5792414942877976\n",
      "Train loss: 0.5783836789871459\n",
      "Train loss: 0.5790480824195191\n",
      "Train loss: 0.5779113245976938\n",
      "Train loss: 0.5783861701420131\n",
      "Train loss: 0.5785548093924953\n",
      "Train loss: 0.5795042144841162\n",
      "Train loss: 0.5786095431244831\n",
      "Train loss: 0.5790499604844117\n",
      "Train loss: 0.5774108843784533\n",
      "Train loss: 0.5762558226149184\n",
      "Train loss: 0.5764123921178121\n",
      "Train loss: 0.5772556491484675\n",
      "Train loss: 0.5769880320511612\n",
      "Train loss: 0.5766455992776317\n",
      "Train loss: 0.5767136062672716\n",
      "Train loss: 0.5771350778481404\n",
      "Train loss: 0.5778744105173617\n",
      "Train loss: 0.5775455648016204\n",
      "Train loss: 0.5772503260297891\n",
      "Train loss: 0.5771039524838601\n",
      "Train loss: 0.5775477734058855\n",
      "Train loss: 0.577256312299409\n",
      "Train loss: 0.5774642281539524\n",
      "Train loss: 0.5776777153338178\n",
      "Train loss: 0.5776214189454381\n",
      "Train loss: 0.5779299563177769\n",
      "Train loss: 0.577550070534217\n",
      "Train loss: 0.5770501536421468\n",
      "Train loss: 0.5772523379601319\n",
      "Train loss: 0.5777898129444099\n",
      "Train loss: 0.578324563119001\n",
      "Train loss: 0.5780842924430629\n",
      "Train loss: 0.5781017021472296\n",
      "Train loss: 0.577676888584955\n",
      "Train loss: 0.5776742012866745\n",
      "Train loss: 0.5779436850054869\n",
      "Train loss: 0.5782083151693416\n",
      "Train loss: 0.5781089724104645\n",
      "Train loss: 0.5782152144913776\n",
      "Train loss: 0.578637499500204\n",
      "Train loss: 0.5787153329126267\n",
      "Train loss: 0.5783315703086376\n",
      "Train loss: 0.5783233582354358\n",
      "Train loss: 0.5784685961950478\n",
      "Train loss: 0.5784721839850984\n",
      "Train loss: 0.5786155398345825\n",
      "Train loss: 0.5784199021959431\n",
      "Train loss: 0.5784252524838764\n",
      "Train loss: 0.578582364917307\n",
      "Train loss: 0.5786254534977888\n",
      "Train loss: 0.5786640437486428\n",
      "Train loss: 0.5784721689853484\n",
      "Train loss: 0.5784119786583686\n",
      "Train loss: 0.5782398021109315\n",
      "Train loss: 0.5777127474110525\n",
      "Train loss: 0.5778246934753734\n",
      "Train loss: 0.5779471558319089\n",
      "Train loss: 0.5780165055334963\n",
      "Train loss: 0.5780697222245267\n",
      "Train loss: 0.5779850989090195\n",
      "Train loss: 0.5780635411005107\n",
      "Train loss: 0.5782473591402888\n",
      "Train loss: 0.5782996212030781\n",
      "Train loss: 0.5781898776206558\n",
      "Train loss: 0.5781063657350903\n",
      "Train loss: 0.5783782155204557\n",
      "Train loss: 0.5785371869163439\n",
      "Train loss: 0.5785507222564653\n",
      "Train loss: 0.5781589588309632\n",
      "Train loss: 0.5783156248239371\n",
      "Train loss: 0.5783324184205666\n",
      "Train loss: 0.5780834537079016\n",
      "Train loss: 0.577943090127131\n",
      "Train loss: 0.577765658346935\n",
      "Train loss: 0.5777254341945289\n",
      "Train loss: 0.577427216832065\n",
      "Train loss: 0.5777155930554476\n",
      "Train loss: 0.5778637974522478\n",
      "Train loss: 0.5779754726353357\n",
      "Train loss: 0.5778272679907007\n",
      "Train loss: 0.5779953807870918\n",
      "Train loss: 0.5779227832531787\n",
      "Train loss: 0.578002730385471\n",
      "Train loss: 0.5780845351711494\n",
      "Train loss: 0.578142136539391\n",
      "Train loss: 0.5781009211522828\n",
      "Train loss: 0.577891162970805\n",
      "Train loss: 0.5778413934327924\n",
      "Train loss: 0.5776567944015379\n",
      "Train loss: 0.5775015296012894\n",
      "Train loss: 0.577390992800867\n",
      "Train loss: 0.5776553942971279\n",
      "Train loss: 0.5776487315320575\n",
      "Train loss: 0.5776790171044558\n",
      "Train loss: 0.5774878532106165\n",
      "Train loss: 0.5773864595774155\n",
      "Train loss: 0.5774328509548547\n",
      "Train loss: 0.5776111684142577\n",
      "Train loss: 0.5775268439198372\n",
      "Train loss: 0.5774829604507523\n",
      "Train loss: 0.5775552623816684\n",
      "Train loss: 0.5773457148049548\n",
      "Train loss: 0.5774272560097043\n",
      "Train loss: 0.5774862042980897\n",
      "Train loss: 0.5777178045950437\n",
      "Train loss: 0.5778593217416692\n",
      "Train loss: 0.5778445636374974\n",
      "Train loss: 0.5779281734106347\n",
      "Train loss: 0.5777953006781265\n",
      "Train loss: 0.5776510109698688\n",
      "Train loss: 0.5777620203167804\n",
      "Train loss: 0.5778362620996129\n",
      "Train loss: 0.5778677145995567\n",
      "Train loss: 0.5776268691726152\n",
      "Train loss: 0.5777184260802634\n",
      "Train loss: 0.5777941031226945\n",
      "Train loss: 0.577814602790831\n",
      "Train loss: 0.5778304478674021\n",
      "Train loss: 0.5778291742785026\n",
      "Train loss: 0.5778285685457969\n",
      "Train loss: 0.5778125788633857\n",
      "Train loss: 0.5777955649589309\n",
      "Train loss: 0.5777389065086864\n",
      "Train loss: 0.577700659716116\n",
      "Train loss: 0.5777688754978512\n",
      "Train loss: 0.5777908821411105\n",
      "Train loss: 0.5777685890540715\n",
      "Train loss: 0.577788981655967\n",
      "Train loss: 0.5776786684410827\n",
      "Train loss: 0.5776381376237689\n",
      "Train loss: 0.5777381012713753\n",
      "Train loss: 0.5776564463967797\n",
      "Train loss: 0.5777785909805485\n",
      "Train loss: 0.5778062371546794\n",
      "Train loss: 0.5777155435179875\n",
      "Train loss: 0.5776502534354048\n",
      "Train loss: 0.5775927427919109\n",
      "Train loss: 0.5776586686287751\n",
      "Train loss: 0.5777452016645088\n",
      "Train loss: 0.577737921057005\n",
      "Train loss: 0.5777345351053018\n",
      "Train loss: 0.577781311410083\n",
      "Train loss: 0.5776408270571348\n",
      "Train loss: 0.5776732800048681\n",
      "Train loss: 0.5776364543950339\n",
      "Train loss: 0.5777087575944427\n",
      "Train loss: 0.5777740924363685\n",
      "Train loss: 0.5778884964300206\n",
      "Train loss: 0.5779256310973688\n",
      "Train loss: 0.5778274567994001\n",
      "Train loss: 0.5777647021432292\n",
      "Train loss: 0.5777669076788532\n",
      "Train loss: 0.5778256691683652\n",
      "Train loss: 0.5777468853841725\n",
      "Train loss: 0.5777882919252985\n",
      "Train loss: 0.5779674063518343\n",
      "Train loss: 0.5779548863712703\n",
      "Train loss: 0.5779236232322325\n",
      "Train loss: 0.5779002339274053\n",
      "Train loss: 0.5778278769395323\n",
      "Train loss: 0.5778936598342077\n",
      "Train loss: 0.5779265218104828\n",
      "Train loss: 0.578025948694587\n",
      "Train loss: 0.5781158303933501\n",
      "Train loss: 0.5780565026624778\n",
      "Train loss: 0.5780619355334108\n",
      "Train loss: 0.5780630750856888\n",
      "Train loss: 0.5780139370525978\n",
      "Train loss: 0.577922351785331\n",
      "Train loss: 0.5779569602469731\n",
      "Train loss: 0.5780164059864022\n",
      "Train loss: 0.5781129847912055\n",
      "Train loss: 0.5780281897743206\n",
      "Train loss: 0.5780392353237891\n",
      "Train loss: 0.5780824204253712\n",
      "Train loss: 0.5781016501648896\n",
      "Train loss: 0.578076458385393\n",
      "Train loss: 0.5782185681451664\n",
      "Train loss: 0.5780950005911242\n",
      "Train loss: 0.5781210468345078\n",
      "Train loss: 0.5782098239021921\n",
      "Train loss: 0.578202765709598\n",
      "Train loss: 0.578233907123278\n",
      "Train loss: 0.578242338074025\n",
      "Train loss: 0.5782161176617435\n",
      "Train loss: 0.5782464425093545\n",
      "Train loss: 0.5782476888449853\n",
      "Train loss: 0.5782821824617475\n",
      "Train loss: 0.5782223623522145\n",
      "Train loss: 0.5782547395477953\n",
      "Train loss: 0.5780625051380124\n",
      "Train loss: 0.5781813008666009\n",
      "Train loss: 0.5781958625142755\n",
      "Train loss: 0.5781213449084546\n",
      "Train loss: 0.5781648549823594\n",
      "Train loss: 0.5781343689920486\n",
      "Train loss: 0.5781106541743742\n",
      "Train loss: 0.5781564051269711\n",
      "Train loss: 0.5781514402153561\n",
      "Train loss: 0.5781355316538446\n",
      "Train loss: 0.578191284716313\n",
      "Train loss: 0.5781674070666494\n",
      "Train loss: 0.5780750368928428\n",
      "Train loss: 0.5780345260971588\n",
      "Train loss: 0.5779743981683052\n",
      "Train loss: 0.5778792642641741\n",
      "Train loss: 0.5778992821692761\n",
      "Train loss: 0.57789421243579\n",
      "Train loss: 0.5778211020230974\n",
      "Train loss: 0.57783096840393\n",
      "Train loss: 0.5779239071537721\n",
      "Train loss: 0.5778395014387661\n",
      "Train loss: 0.5776672544006703\n",
      "Train loss: 0.5777569166662225\n",
      "Train loss: 0.5777164522238004\n",
      "Train loss: 0.5777463920613943\n",
      "Train loss: 0.5778115436116302\n",
      "Train loss: 0.5777269539327887\n",
      "Train loss: 0.5777675885057105\n",
      "Train loss: 0.577723675602077\n",
      "Train loss: 0.5776682497748412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5776473173205517\n",
      "Train loss: 0.5776750487520296\n",
      "Train loss: 0.5777504025908917\n",
      "Train loss: 0.5778534446632286\n",
      "Train loss: 0.5777692473157024\n",
      "Train loss: 0.5777960669380574\n",
      "Train loss: 0.5777667486747791\n",
      "Train loss: 0.5777254844714618\n",
      "Train loss: 0.5778170639859013\n",
      "Train loss: 0.5778196093439437\n",
      "Train loss: 0.5778074102981644\n",
      "Train loss: 0.5779032472181428\n",
      "Train loss: 0.5779222567982057\n",
      "Train loss: 0.5779419139432138\n",
      "Train loss: 0.5779090486701656\n",
      "Train loss: 0.5778855193141598\n",
      "Train loss: 0.5778344392740431\n",
      "Train loss: 0.5777447043975498\n",
      "Train loss: 0.5776899944581755\n",
      "Train loss: 0.5776458616856679\n",
      "Train loss: 0.5776282848110509\n",
      "Train loss: 0.5776397606310436\n",
      "Train loss: 0.577649565604663\n",
      "Train loss: 0.5777197081999677\n",
      "Train loss: 0.5776610970671788\n",
      "Train loss: 0.5777004483864683\n",
      "Train loss: 0.5776181093919506\n",
      "Train loss: 0.5777252779478363\n",
      "Train loss: 0.5776218080731579\n",
      "Train loss: 0.5776640612073135\n",
      "Train loss: 0.577704371000251\n",
      "Train loss: 0.5777612760178511\n",
      "Train loss: 0.5777065682860572\n",
      "Train loss: 0.5776627897192654\n",
      "Train loss: 0.5777297554876285\n",
      "Train loss: 0.5778081224895323\n",
      "Train loss: 0.5778612271576903\n",
      "Train loss: 0.5778192852116979\n",
      "Train loss: 0.5778667717360637\n",
      "Train loss: 0.5779135628894018\n",
      "Train loss: 0.5778994668294981\n",
      "Train loss: 0.57785060296436\n",
      "Train loss: 0.577798880888263\n",
      "Train loss: 0.5778090226680588\n",
      "Train loss: 0.5778324703285499\n",
      "Train loss: 0.5778985033398566\n",
      "Train loss: 0.5779521247748313\n",
      "Train loss: 0.578030428529149\n",
      "Train loss: 0.5780224105804642\n",
      "Train loss: 0.5780215920195246\n",
      "Train loss: 0.5780027211293787\n",
      "Train loss: 0.578030254941333\n",
      "Train loss: 0.5780510774725528\n",
      "Train loss: 0.5780249269233709\n",
      "Train loss: 0.5780118124940258\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6906306445598602\n",
      "Val loss: 0.6150446931521097\n",
      "Val loss: 0.587282623563494\n",
      "Val loss: 0.589154346993095\n",
      "Val loss: 0.5892522037029266\n",
      "Val loss: 0.5920965486559374\n",
      "Val loss: 0.5858873409383437\n",
      "Val loss: 0.5808500830943768\n",
      "Val loss: 0.5762286443601955\n",
      "Val loss: 0.5760871317921853\n",
      "Val loss: 0.5766562521457672\n",
      "Val loss: 0.575726535360692\n",
      "Val loss: 0.5753086619079113\n",
      "Val loss: 0.5748499079027037\n",
      "Val loss: 0.573792087065207\n",
      "Val loss: 0.5724174252039269\n",
      "Val loss: 0.5710204523943719\n",
      "Val loss: 0.5702163262313671\n",
      "Val loss: 0.5712047479254134\n",
      "Val loss: 0.5689811345302698\n",
      "Val loss: 0.5695636599109724\n",
      "Val loss: 0.5682398050750067\n",
      "Val loss: 0.5673418366595319\n",
      "Val loss: 0.5669157387328749\n",
      "Val loss: 0.5669018364241046\n",
      "Val loss: 0.5656654185564943\n",
      "Val loss: 0.5654673987804953\n",
      "Val loss: 0.5644374290387407\n",
      "Val loss: 0.563638301152322\n",
      "Val loss: 0.5637520845704431\n",
      "Val loss: 0.5635841846853108\n",
      "Val loss: 0.5636640507845009\n",
      "Val loss: 0.5634770671405444\n",
      "Val loss: 0.5628607703383858\n",
      "Val loss: 0.5625137732974415\n",
      "Val loss: 0.5624593218278618\n",
      "Val loss: 0.5608171177623065\n",
      "Val loss: 0.5608101110294382\n",
      "Val loss: 0.5615472927228692\n",
      "Val loss: 0.5608695612181371\n",
      "Val loss: 0.5612568780779839\n",
      "Val loss: 0.5607974841549066\n",
      "Val loss: 0.56025991372973\n",
      "Val loss: 0.5603876982105377\n",
      "Val loss: 0.5598372695967555\n",
      "Val loss: 0.5600599463069282\n",
      "Val loss: 0.5597677484282062\n",
      "Val loss: 0.5598302133163149\n",
      "Val loss: 0.5599320704331163\n",
      "Val loss: 0.5600975857202308\n",
      "Val loss: 0.5595086531376275\n",
      "Val loss: 0.5589149288229041\n",
      "Val loss: 0.5587251922397902\n",
      "Val loss: 0.5583326977203327\n",
      "Val loss: 0.5584091135185131\n",
      "Val loss: 0.5578627357773456\n",
      "Val loss: 0.5580010652332239\n",
      "Val loss: 0.5581527180118957\n",
      "Val loss: 0.5581651166791007\n",
      "Val loss: 0.5580959259267635\n",
      "Val loss: 0.5582685311766047\n",
      "Val loss: 0.5583172911193378\n",
      "Val loss: 0.557837110416145\n",
      "Val loss: 0.5575378313894183\n",
      "Val loss: 0.5573926751076439\n",
      "Val loss: 0.5575264930543928\n",
      "Val loss: 0.5574305303796323\n",
      "Val loss: 0.5570489724652957\n",
      "Val loss: 0.5578598739622638\n",
      "Val loss: 0.5576530697523352\n",
      "Val loss: 0.5570738311372908\n",
      "Val loss: 0.5570151600831066\n",
      "Val loss: 0.5567516261240938\n",
      "Val loss: 0.5570875941737881\n",
      "Val loss: 0.5571665015609507\n",
      "Val loss: 0.5568803393903697\n",
      "Val loss: 0.5570328433532268\n",
      "Val loss: 0.5568140762783874\n",
      "Val loss: 0.5564067152702263\n",
      "Val loss: 0.5565587862541801\n",
      "Val loss: 0.5568653946318248\n",
      "Val loss: 0.5564046751316136\n",
      "Val loss: 0.5565588504507921\n",
      "Val loss: 0.5562330761075305\n",
      "Val loss: 0.5559961882261735\n",
      "Val loss: 0.556174003925079\n",
      "Val loss: 0.5560059092149207\n",
      "Val loss: 0.5560233911376337\n",
      "Val loss: 0.5555310404381236\n",
      "Val loss: 0.5553570258325352\n",
      "Val loss: 0.5555336592218424\n",
      "Val loss: 0.5555704480162893\n",
      "Val loss: 0.5556029271462868\n",
      "Val loss: 0.5555699925178658\n",
      "Val loss: 0.5553231263211005\n",
      "Val loss: 0.5556010022790546\n",
      "Val loss: 0.5557051500505652\n",
      "Val loss: 0.5556927428167534\n",
      "Val loss: 0.5557158992358064\n",
      "Val loss: 0.5562078052149985\n",
      "Val loss: 0.5561189289603915\n",
      "Val loss: 0.5559391969547759\n",
      "Val loss: 0.5562765005729542\n",
      "Val loss: 0.5560445899219182\n",
      "Val loss: 0.555950018968291\n",
      "Val loss: 0.5558883057650186\n",
      "Val loss: 0.5555645092931133\n",
      "Val loss: 0.5555022811734831\n",
      "Val loss: 0.5553098151131588\n",
      "Val loss: 0.5554500264548213\n",
      "Val loss: 0.5554925685324824\n",
      "Val loss: 0.5555810057199918\n",
      "Val loss: 0.5555340524049516\n",
      "Val loss: 0.5557344062466613\n",
      "Val loss: 0.5555598221797146\n",
      "Val loss: 0.5556844389500396\n",
      "Val loss: 0.5558064566695526\n",
      "Val loss: 0.5558014046558905\n",
      "Val loss: 0.5559253477387958\n",
      "Val loss: 0.5561524002699303\n",
      "Val loss: 0.5563511598781244\n",
      "Val loss: 0.5566298318613926\n",
      "Val loss: 0.5567064469237281\n",
      "Val loss: 0.5567186760305396\n",
      "Val loss: 0.556865828207288\n",
      "Val loss: 0.5570036760858588\n",
      "Val loss: 0.5568941622408407\n",
      "\n",
      "starting Epoch 23\n",
      "Training...\n",
      "Train loss: 0.6097418838425687\n",
      "Train loss: 0.5945236957990206\n",
      "Train loss: 0.5909463743032035\n",
      "Train loss: 0.5890555887282649\n",
      "Train loss: 0.5873578973490783\n",
      "Train loss: 0.5865524015506777\n",
      "Train loss: 0.5835582650393891\n",
      "Train loss: 0.5832725409066902\n",
      "Train loss: 0.5838807113676764\n",
      "Train loss: 0.5806686319598001\n",
      "Train loss: 0.5797598694013134\n",
      "Train loss: 0.5792276157245476\n",
      "Train loss: 0.5800393230666525\n",
      "Train loss: 0.581013078757939\n",
      "Train loss: 0.5809256574979993\n",
      "Train loss: 0.5805547328951964\n",
      "Train loss: 0.5805253781224422\n",
      "Train loss: 0.5800987506145222\n",
      "Train loss: 0.5793512918861057\n",
      "Train loss: 0.5793665805107968\n",
      "Train loss: 0.5798265273218223\n",
      "Train loss: 0.5799701161563804\n",
      "Train loss: 0.5805514185387065\n",
      "Train loss: 0.5803246470109904\n",
      "Train loss: 0.5813284424956672\n",
      "Train loss: 0.5814514266388034\n",
      "Train loss: 0.5814673697683055\n",
      "Train loss: 0.5809281813330642\n",
      "Train loss: 0.5815046154258576\n",
      "Train loss: 0.581307925594868\n",
      "Train loss: 0.5812113896999298\n",
      "Train loss: 0.5805924190601832\n",
      "Train loss: 0.5807647834869726\n",
      "Train loss: 0.5804407078668541\n",
      "Train loss: 0.5800435789494385\n",
      "Train loss: 0.5800012961465228\n",
      "Train loss: 0.5803290830978037\n",
      "Train loss: 0.580095546832983\n",
      "Train loss: 0.5797446269125932\n",
      "Train loss: 0.579576183506783\n",
      "Train loss: 0.5799117819366292\n",
      "Train loss: 0.579455916269175\n",
      "Train loss: 0.5792071485547164\n",
      "Train loss: 0.5790930023507996\n",
      "Train loss: 0.579298739969001\n",
      "Train loss: 0.5792909381309194\n",
      "Train loss: 0.5786626674750616\n",
      "Train loss: 0.5785128987929371\n",
      "Train loss: 0.5783378027371903\n",
      "Train loss: 0.5781164477179358\n",
      "Train loss: 0.5785301955045732\n",
      "Train loss: 0.5789576549445126\n",
      "Train loss: 0.5792145257124482\n",
      "Train loss: 0.5793241626471696\n",
      "Train loss: 0.5793262169076921\n",
      "Train loss: 0.5790550091417908\n",
      "Train loss: 0.5794480684768536\n",
      "Train loss: 0.5792616023083408\n",
      "Train loss: 0.5791277416537028\n",
      "Train loss: 0.579380545122013\n",
      "Train loss: 0.5792291537721789\n",
      "Train loss: 0.5792055060490184\n",
      "Train loss: 0.5790693018765938\n",
      "Train loss: 0.5791101153471024\n",
      "Train loss: 0.5789566268546477\n",
      "Train loss: 0.5791718002473702\n",
      "Train loss: 0.5795712489481735\n",
      "Train loss: 0.5795610532811497\n",
      "Train loss: 0.5797064829728498\n",
      "Train loss: 0.5800069467649875\n",
      "Train loss: 0.5800217889706125\n",
      "Train loss: 0.5799710394153171\n",
      "Train loss: 0.5800002570876199\n",
      "Train loss: 0.5798570448438246\n",
      "Train loss: 0.5800395382890071\n",
      "Train loss: 0.580060811596846\n",
      "Train loss: 0.5799736233986067\n",
      "Train loss: 0.5799444199678911\n",
      "Train loss: 0.5798619056404807\n",
      "Train loss: 0.5798136771768984\n",
      "Train loss: 0.579942562187653\n",
      "Train loss: 0.579920775847438\n",
      "Train loss: 0.5801136475539193\n",
      "Train loss: 0.5799380402929943\n",
      "Train loss: 0.5801069844709277\n",
      "Train loss: 0.579943878748446\n",
      "Train loss: 0.579885784872926\n",
      "Train loss: 0.5798436734166723\n",
      "Train loss: 0.5800340887768474\n",
      "Train loss: 0.5799618732803328\n",
      "Train loss: 0.5798225189841272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5797488112823026\n",
      "Train loss: 0.5799384732410561\n",
      "Train loss: 0.5798833358154581\n",
      "Train loss: 0.5798681865496783\n",
      "Train loss: 0.5798214793577985\n",
      "Train loss: 0.5797391686936271\n",
      "Train loss: 0.5798379710099112\n",
      "Train loss: 0.579770914664348\n",
      "Train loss: 0.5795869570693235\n",
      "Train loss: 0.5796927985813194\n",
      "Train loss: 0.5796806111149837\n",
      "Train loss: 0.5796470544968142\n",
      "Train loss: 0.5795988215311838\n",
      "Train loss: 0.5796380387623574\n",
      "Train loss: 0.5794416852383301\n",
      "Train loss: 0.5796300839631008\n",
      "Train loss: 0.5796376908195411\n",
      "Train loss: 0.5797169617470824\n",
      "Train loss: 0.5798391187228957\n",
      "Train loss: 0.5798047885882956\n",
      "Train loss: 0.5800140203642706\n",
      "Train loss: 0.5798295990272453\n",
      "Train loss: 0.5798272149324313\n",
      "Train loss: 0.5797165275371919\n",
      "Train loss: 0.5796127801904806\n",
      "Train loss: 0.5797029746781561\n",
      "Train loss: 0.5798858961673953\n",
      "Train loss: 0.5797973703741777\n",
      "Train loss: 0.5799024678633182\n",
      "Train loss: 0.5798517657538592\n",
      "Train loss: 0.5797464559438924\n",
      "Train loss: 0.579557012752965\n",
      "Train loss: 0.5795807335322881\n",
      "Train loss: 0.5795712142216772\n",
      "Train loss: 0.5796946399194847\n",
      "Train loss: 0.5797117653325957\n",
      "Train loss: 0.5798025415739355\n",
      "Train loss: 0.5798246517644552\n",
      "Train loss: 0.5797434126518928\n",
      "Train loss: 0.5797737245115445\n",
      "Train loss: 0.5796627043977566\n",
      "Train loss: 0.5795569005273437\n",
      "Train loss: 0.5795677736896595\n",
      "Train loss: 0.5796368379599962\n",
      "Train loss: 0.5796191800206231\n",
      "Train loss: 0.5795572360269603\n",
      "Train loss: 0.5796838013233159\n",
      "Train loss: 0.5797122928812247\n",
      "Train loss: 0.5795977182497336\n",
      "Train loss: 0.5795251892449459\n",
      "Train loss: 0.5794853977058588\n",
      "Train loss: 0.5794893947044591\n",
      "Train loss: 0.5795040557723196\n",
      "Train loss: 0.5794763083538543\n",
      "Train loss: 0.5794297022111369\n",
      "Train loss: 0.5793944770440475\n",
      "Train loss: 0.5793265925343273\n",
      "Train loss: 0.5794582551219553\n",
      "Train loss: 0.5796082962370189\n",
      "Train loss: 0.5796026455601859\n",
      "Train loss: 0.5796056804040342\n",
      "Train loss: 0.5794179744777356\n",
      "Train loss: 0.5794882741927636\n",
      "Train loss: 0.579552802444466\n",
      "Train loss: 0.579505461288133\n",
      "Train loss: 0.5795378117623622\n",
      "Train loss: 0.5794826135600658\n",
      "Train loss: 0.5794278333943833\n",
      "Train loss: 0.5793532149871463\n",
      "Train loss: 0.5793165444550066\n",
      "Train loss: 0.5793037458992034\n",
      "Train loss: 0.5792837849732153\n",
      "Train loss: 0.579172104224449\n",
      "Train loss: 0.5792513331452006\n",
      "Train loss: 0.5792950713318419\n",
      "Train loss: 0.5792121051688792\n",
      "Train loss: 0.5792597291564686\n",
      "Train loss: 0.5792266219969013\n",
      "Train loss: 0.5792614210437557\n",
      "Train loss: 0.5792307239172925\n",
      "Train loss: 0.5792783317298312\n",
      "Train loss: 0.5792736601677889\n",
      "Train loss: 0.5793580499898774\n",
      "Train loss: 0.5793228819306219\n",
      "Train loss: 0.5793530822189805\n",
      "Train loss: 0.5793751372678633\n",
      "Train loss: 0.5792989392868754\n",
      "Train loss: 0.5793174157248558\n",
      "Train loss: 0.5792880072531418\n",
      "Train loss: 0.5792225381721282\n",
      "Train loss: 0.579287084077996\n",
      "Train loss: 0.5793755128429899\n",
      "Train loss: 0.5793630844161833\n",
      "Train loss: 0.5793477944761845\n",
      "Train loss: 0.5793350017272711\n",
      "Train loss: 0.5792105547579638\n",
      "Train loss: 0.5792868643261772\n",
      "Train loss: 0.5792404373870637\n",
      "Train loss: 0.5792534272038142\n",
      "Train loss: 0.5792118643756061\n",
      "Train loss: 0.5791786453723287\n",
      "Train loss: 0.5790913577260796\n",
      "Train loss: 0.5791199557266151\n",
      "Train loss: 0.5791226395217844\n",
      "Train loss: 0.5790382299337438\n",
      "Train loss: 0.5791192959107915\n",
      "Train loss: 0.5791045212405412\n",
      "Train loss: 0.5790967815079321\n",
      "Train loss: 0.5791742187063227\n",
      "Train loss: 0.5791869489269845\n",
      "Train loss: 0.5791242828121809\n",
      "Train loss: 0.5790412856546286\n",
      "Train loss: 0.5788979345193298\n",
      "Train loss: 0.5788177934600889\n",
      "Train loss: 0.5788280528475573\n",
      "Train loss: 0.578793092190456\n",
      "Train loss: 0.5788935165790504\n",
      "Train loss: 0.5789281050606646\n",
      "Train loss: 0.5789591298554164\n",
      "Train loss: 0.5789629891885629\n",
      "Train loss: 0.5790296887052203\n",
      "Train loss: 0.5790128816766732\n",
      "Train loss: 0.5790668765805987\n",
      "Train loss: 0.5790477610377551\n",
      "Train loss: 0.57906724047843\n",
      "Train loss: 0.5791462387157272\n",
      "Train loss: 0.5791680771813914\n",
      "Train loss: 0.579144372110308\n",
      "Train loss: 0.5792240872928354\n",
      "Train loss: 0.5792641225619725\n",
      "Train loss: 0.5792975903201892\n",
      "Train loss: 0.579303218607786\n",
      "Train loss: 0.579267566472618\n",
      "Train loss: 0.5792118636110831\n",
      "Train loss: 0.5791231924637041\n",
      "Train loss: 0.5791626473530507\n",
      "Train loss: 0.579125320075975\n",
      "Train loss: 0.5790432145317508\n",
      "Train loss: 0.5790237698588171\n",
      "Train loss: 0.5789084726596555\n",
      "Train loss: 0.5789363599011137\n",
      "Train loss: 0.5787908342755079\n",
      "Train loss: 0.5787481140519697\n",
      "Train loss: 0.578736843365663\n",
      "Train loss: 0.5787376197219577\n",
      "Train loss: 0.5787919135781928\n",
      "Train loss: 0.5788284835757316\n",
      "Train loss: 0.5787948398977724\n",
      "Train loss: 0.5786869725564491\n",
      "Train loss: 0.578742644739339\n",
      "Train loss: 0.5787393020484831\n",
      "Train loss: 0.5786552403438825\n",
      "Train loss: 0.5787121723191272\n",
      "Train loss: 0.5787358663597504\n",
      "Train loss: 0.5787119440163362\n",
      "Train loss: 0.578629944525549\n",
      "Train loss: 0.5786821593810868\n",
      "Train loss: 0.5787103442895314\n",
      "Train loss: 0.5785800672502702\n",
      "Train loss: 0.57856166604435\n",
      "Train loss: 0.5785681989001812\n",
      "Train loss: 0.5786179362148794\n",
      "Train loss: 0.5786675642233332\n",
      "Train loss: 0.5786529532084864\n",
      "Train loss: 0.5787019756918984\n",
      "Train loss: 0.5786673501148417\n",
      "Train loss: 0.5786331625069313\n",
      "Train loss: 0.5786426309902573\n",
      "Train loss: 0.578630486780186\n",
      "Train loss: 0.5785638791467751\n",
      "Train loss: 0.5785241832963427\n",
      "Train loss: 0.5785103847509465\n",
      "Train loss: 0.578453747030234\n",
      "Train loss: 0.5784190981697195\n",
      "Train loss: 0.5784165359491422\n",
      "Train loss: 0.5783132055397895\n",
      "Train loss: 0.5784542627144625\n",
      "Train loss: 0.578470108726916\n",
      "Train loss: 0.5784563886653408\n",
      "Train loss: 0.5784388062401349\n",
      "Train loss: 0.5784277412126029\n",
      "Train loss: 0.5783669797464438\n",
      "Train loss: 0.5783052659258832\n",
      "Train loss: 0.5782685516411271\n",
      "Train loss: 0.5782255917749752\n",
      "Train loss: 0.5782678728285282\n",
      "Train loss: 0.57821792200071\n",
      "Train loss: 0.5781866165659594\n",
      "Train loss: 0.5782096163090434\n",
      "Train loss: 0.5782239031836239\n",
      "Train loss: 0.5782338352961455\n",
      "Train loss: 0.5782685024427807\n",
      "Train loss: 0.5782194355672294\n",
      "Train loss: 0.5781273074914832\n",
      "Train loss: 0.5781255043465934\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6928014829754829\n",
      "Val loss: 0.6186161736647288\n",
      "Val loss: 0.591282599738666\n",
      "Val loss: 0.5920453777438716\n",
      "Val loss: 0.5917003167172273\n",
      "Val loss: 0.5946902930736542\n",
      "Val loss: 0.5889682287678999\n",
      "Val loss: 0.5838862313674047\n",
      "Val loss: 0.5784713253378868\n",
      "Val loss: 0.5788584491428064\n",
      "Val loss: 0.5794153219019925\n",
      "Val loss: 0.5787213278018822\n",
      "Val loss: 0.5781740234233439\n",
      "Val loss: 0.5778489652751149\n",
      "Val loss: 0.576728064063433\n",
      "Val loss: 0.5749027415921416\n",
      "Val loss: 0.5735612309404782\n",
      "Val loss: 0.5721770213561111\n",
      "Val loss: 0.5731044097783717\n",
      "Val loss: 0.5705475873417325\n",
      "Val loss: 0.5713224388085879\n",
      "Val loss: 0.5699686069007314\n",
      "Val loss: 0.5687493219187385\n",
      "Val loss: 0.5683357650492372\n",
      "Val loss: 0.5681305311379894\n",
      "Val loss: 0.5668748679549195\n",
      "Val loss: 0.566558276984229\n",
      "Val loss: 0.5655643918531404\n",
      "Val loss: 0.5647156797349453\n",
      "Val loss: 0.5648052324384651\n",
      "Val loss: 0.5645242999126385\n",
      "Val loss: 0.5647017865810754\n",
      "Val loss: 0.564478007395093\n",
      "Val loss: 0.5638577483109468\n",
      "Val loss: 0.5636433861378966\n",
      "Val loss: 0.5636650835002601\n",
      "Val loss: 0.5619148493784926\n",
      "Val loss: 0.5618568140047567\n",
      "Val loss: 0.5626631869176\n",
      "Val loss: 0.561861228553494\n",
      "Val loss: 0.5622299630560127\n",
      "Val loss: 0.5617463657160124\n",
      "Val loss: 0.5611500564579651\n",
      "Val loss: 0.5612842666504045\n",
      "Val loss: 0.5607700732403568\n",
      "Val loss: 0.5609074290923156\n",
      "Val loss: 0.560568938525314\n",
      "Val loss: 0.560687893479439\n",
      "Val loss: 0.5607534081476634\n",
      "Val loss: 0.5608642633899628\n",
      "Val loss: 0.5602150295428404\n",
      "Val loss: 0.559509575251907\n",
      "Val loss: 0.559381471551729\n",
      "Val loss: 0.5588920516373943\n",
      "Val loss: 0.5589984123506685\n",
      "Val loss: 0.5583537756114878\n",
      "Val loss: 0.5584566116752759\n",
      "Val loss: 0.5586803224466251\n",
      "Val loss: 0.5587953789299037\n",
      "Val loss: 0.5587514012952314\n",
      "Val loss: 0.5589238087597647\n",
      "Val loss: 0.5589732927990577\n",
      "Val loss: 0.5585431411957286\n",
      "Val loss: 0.5582884257676833\n",
      "Val loss: 0.5580750765439905\n",
      "Val loss: 0.5581184987058031\n",
      "Val loss: 0.5580203810673274\n",
      "Val loss: 0.557602745945123\n",
      "Val loss: 0.5586827040411705\n",
      "Val loss: 0.5585134692041785\n",
      "Val loss: 0.5578209536392137\n",
      "Val loss: 0.557739482948707\n",
      "Val loss: 0.5574219124494019\n",
      "Val loss: 0.5576782734574988\n",
      "Val loss: 0.5577043923624059\n",
      "Val loss: 0.5572954815736232\n",
      "Val loss: 0.5574704197545847\n",
      "Val loss: 0.5572049636920497\n",
      "Val loss: 0.5567967353133381\n",
      "Val loss: 0.5569491640368202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5572018022879516\n",
      "Val loss: 0.5567119545633461\n",
      "Val loss: 0.5569055653424655\n",
      "Val loss: 0.5565353599254727\n",
      "Val loss: 0.556304470126359\n",
      "Val loss: 0.5564670691957007\n",
      "Val loss: 0.5562449875652515\n",
      "Val loss: 0.5562598942216816\n",
      "Val loss: 0.555806840123894\n",
      "Val loss: 0.5556523166679858\n",
      "Val loss: 0.5557776417763748\n",
      "Val loss: 0.5557932999139258\n",
      "Val loss: 0.5558159100855219\n",
      "Val loss: 0.5557597154366182\n",
      "Val loss: 0.5555506473729379\n",
      "Val loss: 0.5558009930966045\n",
      "Val loss: 0.5558931655750787\n",
      "Val loss: 0.5559199811368876\n",
      "Val loss: 0.5558711018277566\n",
      "Val loss: 0.5564036422478174\n",
      "Val loss: 0.5563469559309029\n",
      "Val loss: 0.5560840293911445\n",
      "Val loss: 0.5564159858783395\n",
      "Val loss: 0.5562046277614917\n",
      "Val loss: 0.5561331861460482\n",
      "Val loss: 0.556079233188485\n",
      "Val loss: 0.555762321482437\n",
      "Val loss: 0.555742177151572\n",
      "Val loss: 0.5555804362520576\n",
      "Val loss: 0.5556819199538622\n",
      "Val loss: 0.555698908946144\n",
      "Val loss: 0.5558004090312555\n",
      "Val loss: 0.5557343491108705\n",
      "Val loss: 0.5559611796599491\n",
      "Val loss: 0.5557614891475086\n",
      "Val loss: 0.5559111346131985\n",
      "Val loss: 0.5560287103975472\n",
      "Val loss: 0.5560528781195447\n",
      "Val loss: 0.5561509410841297\n",
      "Val loss: 0.5564163502846815\n",
      "Val loss: 0.5566602662600428\n",
      "Val loss: 0.556959211581642\n",
      "Val loss: 0.557048211122957\n",
      "Val loss: 0.5570929701859808\n",
      "Val loss: 0.557266969138231\n",
      "Val loss: 0.5573691588327502\n",
      "Val loss: 0.5572640296976649\n",
      "\n",
      "starting Epoch 24\n",
      "Training...\n",
      "Train loss: 0.6041510999202728\n",
      "Train loss: 0.595554184455138\n",
      "Train loss: 0.5850787551726325\n",
      "Train loss: 0.5828188653988174\n",
      "Train loss: 0.580453464780191\n",
      "Train loss: 0.576742128664706\n",
      "Train loss: 0.5800580468109185\n",
      "Train loss: 0.579148493847757\n",
      "Train loss: 0.5814565679880493\n",
      "Train loss: 0.5819295677707423\n",
      "Train loss: 0.5813171663512923\n",
      "Train loss: 0.5812926079189428\n",
      "Train loss: 0.5803171581513172\n",
      "Train loss: 0.5781444335710191\n",
      "Train loss: 0.5793738659209631\n",
      "Train loss: 0.5795104523250676\n",
      "Train loss: 0.5786014302993594\n",
      "Train loss: 0.578609320114582\n",
      "Train loss: 0.5797571626376351\n",
      "Train loss: 0.5789950891096789\n",
      "Train loss: 0.5789106612700551\n",
      "Train loss: 0.5794765794467275\n",
      "Train loss: 0.5788337798679576\n",
      "Train loss: 0.5785031090947432\n",
      "Train loss: 0.5786013499290528\n",
      "Train loss: 0.578911804394915\n",
      "Train loss: 0.578352118799107\n",
      "Train loss: 0.577671518319408\n",
      "Train loss: 0.5775560812320116\n",
      "Train loss: 0.5771590295637987\n",
      "Train loss: 0.5773419667522819\n",
      "Train loss: 0.5767062768130236\n",
      "Train loss: 0.5766292756171798\n",
      "Train loss: 0.5761787252766746\n",
      "Train loss: 0.5762541213090158\n",
      "Train loss: 0.5763373670060711\n",
      "Train loss: 0.5763127223483926\n",
      "Train loss: 0.5764515656727576\n",
      "Train loss: 0.5763741574024205\n",
      "Train loss: 0.5766647152220352\n",
      "Train loss: 0.5767339507302085\n",
      "Train loss: 0.5767787448413608\n",
      "Train loss: 0.5770188404531224\n",
      "Train loss: 0.5774206592700316\n",
      "Train loss: 0.5773707041817326\n",
      "Train loss: 0.577196294776244\n",
      "Train loss: 0.5768102022786491\n",
      "Train loss: 0.5768403582453604\n",
      "Train loss: 0.5766277109578145\n",
      "Train loss: 0.5766455820492199\n",
      "Train loss: 0.5763278066000597\n",
      "Train loss: 0.5764682484831916\n",
      "Train loss: 0.5762789190603496\n",
      "Train loss: 0.5762789919765268\n",
      "Train loss: 0.5763815788109808\n",
      "Train loss: 0.5766862645225934\n",
      "Train loss: 0.5767041617992994\n",
      "Train loss: 0.5768312082607444\n",
      "Train loss: 0.5768875364684978\n",
      "Train loss: 0.5769796416697848\n",
      "Train loss: 0.5768918791591778\n",
      "Train loss: 0.5768582296765938\n",
      "Train loss: 0.5765232819475002\n",
      "Train loss: 0.5765580248096382\n",
      "Train loss: 0.5766567065451492\n",
      "Train loss: 0.5766116485982523\n",
      "Train loss: 0.5764613766737888\n",
      "Train loss: 0.5763258454640005\n",
      "Train loss: 0.5766953466334491\n",
      "Train loss: 0.5767928704438335\n",
      "Train loss: 0.5768950571889521\n",
      "Train loss: 0.5767342341575795\n",
      "Train loss: 0.5765777468027686\n",
      "Train loss: 0.5767260809215522\n",
      "Train loss: 0.5766517652560267\n",
      "Train loss: 0.5766421695184049\n",
      "Train loss: 0.5762965532771328\n",
      "Train loss: 0.576447470590348\n",
      "Train loss: 0.5764243282646677\n",
      "Train loss: 0.5764730647252901\n",
      "Train loss: 0.576361586993826\n",
      "Train loss: 0.5763132296152563\n",
      "Train loss: 0.5763614138901557\n",
      "Train loss: 0.5762229517253117\n",
      "Train loss: 0.576279441930744\n",
      "Train loss: 0.5765596119694546\n",
      "Train loss: 0.5768612467776502\n",
      "Train loss: 0.5772089943620141\n",
      "Train loss: 0.5772242448918266\n",
      "Train loss: 0.5770532039054703\n",
      "Train loss: 0.577162971284902\n",
      "Train loss: 0.5772087138233267\n",
      "Train loss: 0.577183319475139\n",
      "Train loss: 0.5770271399859864\n",
      "Train loss: 0.5767437059073275\n",
      "Train loss: 0.5766937144104548\n",
      "Train loss: 0.5768027614385213\n",
      "Train loss: 0.5769476881284018\n",
      "Train loss: 0.5769022749101591\n",
      "Train loss: 0.5768334244178974\n",
      "Train loss: 0.5767632923012856\n",
      "Train loss: 0.5769835770159389\n",
      "Train loss: 0.5770619699044851\n",
      "Train loss: 0.5771422511886782\n",
      "Train loss: 0.5773276864573637\n",
      "Train loss: 0.5772354030114066\n",
      "Train loss: 0.5772416136598074\n",
      "Train loss: 0.5772520572347407\n",
      "Train loss: 0.5774053456368606\n",
      "Train loss: 0.5773006017476985\n",
      "Train loss: 0.5773475078405051\n",
      "Train loss: 0.5773165531726009\n",
      "Train loss: 0.5773844083572395\n",
      "Train loss: 0.5774394332473348\n",
      "Train loss: 0.5774324558749827\n",
      "Train loss: 0.5774658384149369\n",
      "Train loss: 0.5774861985850201\n",
      "Train loss: 0.5775947339987543\n",
      "Train loss: 0.5776561175438976\n",
      "Train loss: 0.5776579987262378\n",
      "Train loss: 0.5775887411874698\n",
      "Train loss: 0.5777775227730271\n",
      "Train loss: 0.5777432233664794\n",
      "Train loss: 0.57758173909145\n",
      "Train loss: 0.577579450576293\n",
      "Train loss: 0.5776040621956647\n",
      "Train loss: 0.5773500101145631\n",
      "Train loss: 0.5772520979082458\n",
      "Train loss: 0.5771594377232412\n",
      "Train loss: 0.5772025177069653\n",
      "Train loss: 0.5773418636870957\n",
      "Train loss: 0.5773766970331807\n",
      "Train loss: 0.5772301378152388\n",
      "Train loss: 0.5773214655120055\n",
      "Train loss: 0.5773381455247603\n",
      "Train loss: 0.5774160364547518\n",
      "Train loss: 0.5775587629797512\n",
      "Train loss: 0.5775180461506949\n",
      "Train loss: 0.5775347316316582\n",
      "Train loss: 0.577541227456117\n",
      "Train loss: 0.5775729285757268\n",
      "Train loss: 0.5775696095151354\n",
      "Train loss: 0.5776773439645351\n",
      "Train loss: 0.5777031527020196\n",
      "Train loss: 0.5777994036345533\n",
      "Train loss: 0.5776467765998906\n",
      "Train loss: 0.5775771293359453\n",
      "Train loss: 0.5776737058795195\n",
      "Train loss: 0.5775991347601206\n",
      "Train loss: 0.5775503613683294\n",
      "Train loss: 0.5775471906979458\n",
      "Train loss: 0.5775231787663379\n",
      "Train loss: 0.5776068768340725\n",
      "Train loss: 0.5775498285933182\n",
      "Train loss: 0.5775421646265262\n",
      "Train loss: 0.5774559944954084\n",
      "Train loss: 0.5773253019598875\n",
      "Train loss: 0.5773070895358632\n",
      "Train loss: 0.5773212891323981\n",
      "Train loss: 0.5773665033567321\n",
      "Train loss: 0.5771518302430422\n",
      "Train loss: 0.577227259959362\n",
      "Train loss: 0.5773008930313693\n",
      "Train loss: 0.5774892247388479\n",
      "Train loss: 0.5776130505567899\n",
      "Train loss: 0.577610066709981\n",
      "Train loss: 0.5777250196484613\n",
      "Train loss: 0.5777607476757409\n",
      "Train loss: 0.5777342039987017\n",
      "Train loss: 0.5777666382787507\n",
      "Train loss: 0.5777725865614975\n",
      "Train loss: 0.5777889488827666\n",
      "Train loss: 0.5777532466318126\n",
      "Train loss: 0.5777166063979495\n",
      "Train loss: 0.5776315170139815\n",
      "Train loss: 0.5775455098644716\n",
      "Train loss: 0.5775344399080063\n",
      "Train loss: 0.5775954345030274\n",
      "Train loss: 0.5776363782923992\n",
      "Train loss: 0.5776100592518488\n",
      "Train loss: 0.5776044308200466\n",
      "Train loss: 0.5776136783690006\n",
      "Train loss: 0.5777105094061161\n",
      "Train loss: 0.5777147609227117\n",
      "Train loss: 0.5776927287786772\n",
      "Train loss: 0.5775934180831294\n",
      "Train loss: 0.5775047809276137\n",
      "Train loss: 0.5775162181929725\n",
      "Train loss: 0.577518262467898\n",
      "Train loss: 0.5775337006252481\n",
      "Train loss: 0.5774515535732189\n",
      "Train loss: 0.5774077587712708\n",
      "Train loss: 0.5773785208352151\n",
      "Train loss: 0.5774447513966562\n",
      "Train loss: 0.5775382332417806\n",
      "Train loss: 0.5774310096988085\n",
      "Train loss: 0.5774112324343625\n",
      "Train loss: 0.577376426711592\n",
      "Train loss: 0.5773841285187266\n",
      "Train loss: 0.5774466087592187\n",
      "Train loss: 0.5774912783891356\n",
      "Train loss: 0.5775439036854662\n",
      "Train loss: 0.5775091227535897\n",
      "Train loss: 0.5774523128361994\n",
      "Train loss: 0.5774657348360367\n",
      "Train loss: 0.5775361267737203\n",
      "Train loss: 0.5775522684008351\n",
      "Train loss: 0.5774748622009633\n",
      "Train loss: 0.5775822430143062\n",
      "Train loss: 0.5775591029769382\n",
      "Train loss: 0.5775846937632895\n",
      "Train loss: 0.5776365409133063\n",
      "Train loss: 0.5776440851717495\n",
      "Train loss: 0.5776442255224962\n",
      "Train loss: 0.577630044034981\n",
      "Train loss: 0.5776823456018549\n",
      "Train loss: 0.5776618649866703\n",
      "Train loss: 0.5775247811860901\n",
      "Train loss: 0.5774677665463919\n",
      "Train loss: 0.5774107439785173\n",
      "Train loss: 0.5773706402436563\n",
      "Train loss: 0.5772635125246476\n",
      "Train loss: 0.5772498137522075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5773344856536556\n",
      "Train loss: 0.5773562346201098\n",
      "Train loss: 0.5774043840653972\n",
      "Train loss: 0.5773034723195296\n",
      "Train loss: 0.5772301848192544\n",
      "Train loss: 0.5772451441161469\n",
      "Train loss: 0.5771237625314195\n",
      "Train loss: 0.577281689348353\n",
      "Train loss: 0.5773264479536927\n",
      "Train loss: 0.5772201469602645\n",
      "Train loss: 0.5772727069105188\n",
      "Train loss: 0.5773085409350841\n",
      "Train loss: 0.577398693221786\n",
      "Train loss: 0.5774207274404148\n",
      "Train loss: 0.5774561218364721\n",
      "Train loss: 0.5774805143453706\n",
      "Train loss: 0.5775568683610953\n",
      "Train loss: 0.5775935131871718\n",
      "Train loss: 0.5775609298736799\n",
      "Train loss: 0.5775586484783248\n",
      "Train loss: 0.5775753594209582\n",
      "Train loss: 0.5775391227081129\n",
      "Train loss: 0.5776135172512405\n",
      "Train loss: 0.5775765617153475\n",
      "Train loss: 0.5776453807448206\n",
      "Train loss: 0.5775467106127121\n",
      "Train loss: 0.5776128843799307\n",
      "Train loss: 0.577615004453699\n",
      "Train loss: 0.5775573359853007\n",
      "Train loss: 0.57763616672074\n",
      "Train loss: 0.5776917818588261\n",
      "Train loss: 0.5777737943886261\n",
      "Train loss: 0.5778092176193697\n",
      "Train loss: 0.5777982189848303\n",
      "Train loss: 0.5778329871807202\n",
      "Train loss: 0.5779279215809092\n",
      "Train loss: 0.5778780485384968\n",
      "Train loss: 0.5777980776828101\n",
      "Train loss: 0.5778347846432755\n",
      "Train loss: 0.5778283722943782\n",
      "Train loss: 0.5778528480880706\n",
      "Train loss: 0.5779083244338038\n",
      "Train loss: 0.5779411021278953\n",
      "Train loss: 0.5779186065365791\n",
      "Train loss: 0.5779743525080548\n",
      "Train loss: 0.5779310057798844\n",
      "Train loss: 0.5778309034128767\n",
      "Train loss: 0.5779453430267735\n",
      "Train loss: 0.5779455964903684\n",
      "Train loss: 0.5779159658283601\n",
      "Train loss: 0.5779233375623031\n",
      "Train loss: 0.5779222511805108\n",
      "Train loss: 0.5778741663263466\n",
      "Train loss: 0.5778357002317185\n",
      "Train loss: 0.5778084253156157\n",
      "Train loss: 0.5778280744887655\n",
      "Train loss: 0.577807928402736\n",
      "Train loss: 0.5777645847098983\n",
      "Train loss: 0.5777966250160269\n",
      "Train loss: 0.577781677103902\n",
      "Train loss: 0.5777815807440853\n",
      "Train loss: 0.5777587597360525\n",
      "Train loss: 0.5777966937290911\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.695177748799324\n",
      "Val loss: 0.6202444301711189\n",
      "Val loss: 0.5932452763829913\n",
      "Val loss: 0.5935645981838829\n",
      "Val loss: 0.593176948527495\n",
      "Val loss: 0.5954459021831381\n",
      "Val loss: 0.589850061080035\n",
      "Val loss: 0.5853704122396616\n",
      "Val loss: 0.5802172733978792\n",
      "Val loss: 0.5802557456250094\n",
      "Val loss: 0.5807597880010251\n",
      "Val loss: 0.579663746437784\n",
      "Val loss: 0.5791728598996997\n",
      "Val loss: 0.5787671454574751\n",
      "Val loss: 0.5776994586796373\n",
      "Val loss: 0.5761064611658265\n",
      "Val loss: 0.5747933256484213\n",
      "Val loss: 0.5738828965787137\n",
      "Val loss: 0.5747589881115771\n",
      "Val loss: 0.5726083556208947\n",
      "Val loss: 0.5731505473645834\n",
      "Val loss: 0.571930342584575\n",
      "Val loss: 0.5708285028997221\n",
      "Val loss: 0.5702532765244236\n",
      "Val loss: 0.5700037936049123\n",
      "Val loss: 0.5686993270881416\n",
      "Val loss: 0.5684243000265378\n",
      "Val loss: 0.5673820835223301\n",
      "Val loss: 0.5666998939381706\n",
      "Val loss: 0.5668006411334813\n",
      "Val loss: 0.5665864979291891\n",
      "Val loss: 0.5665819397512472\n",
      "Val loss: 0.5663127859191197\n",
      "Val loss: 0.5657535579782971\n",
      "Val loss: 0.5653659336525818\n",
      "Val loss: 0.5653865115935576\n",
      "Val loss: 0.5638686067060285\n",
      "Val loss: 0.5637932973248618\n",
      "Val loss: 0.5644507474198783\n",
      "Val loss: 0.5637607944371114\n",
      "Val loss: 0.5641308371342865\n",
      "Val loss: 0.5636756728140361\n",
      "Val loss: 0.5632549816759947\n",
      "Val loss: 0.5633425176415814\n",
      "Val loss: 0.5628926666187388\n",
      "Val loss: 0.5631026966603042\n",
      "Val loss: 0.5628828987097129\n",
      "Val loss: 0.5629295685550658\n",
      "Val loss: 0.5629384340565713\n",
      "Val loss: 0.5631087344573683\n",
      "Val loss: 0.5624924447592787\n",
      "Val loss: 0.5618950266635556\n",
      "Val loss: 0.561739924053351\n",
      "Val loss: 0.5613500471451912\n",
      "Val loss: 0.5613689347557778\n",
      "Val loss: 0.5608218043081222\n",
      "Val loss: 0.5608197914462694\n",
      "Val loss: 0.5609609305239879\n",
      "Val loss: 0.561028432481143\n",
      "Val loss: 0.5609389241523168\n",
      "Val loss: 0.561058827626862\n",
      "Val loss: 0.5611355008623746\n",
      "Val loss: 0.5606761324177881\n",
      "Val loss: 0.5603551825385856\n",
      "Val loss: 0.5602183063273076\n",
      "Val loss: 0.5602611923833748\n",
      "Val loss: 0.5601313234624749\n",
      "Val loss: 0.5597211594602703\n",
      "Val loss: 0.560496790551169\n",
      "Val loss: 0.5603389511135725\n",
      "Val loss: 0.5597039994883672\n",
      "Val loss: 0.5596469987733782\n",
      "Val loss: 0.5593785961250682\n",
      "Val loss: 0.5596446729287868\n",
      "Val loss: 0.5596983944349748\n",
      "Val loss: 0.5593942568924937\n",
      "Val loss: 0.5595348205727836\n",
      "Val loss: 0.5592970417520381\n",
      "Val loss: 0.5589316736925677\n",
      "Val loss: 0.5590467121368065\n",
      "Val loss: 0.5593054238522407\n",
      "Val loss: 0.5588989719056267\n",
      "Val loss: 0.5590078778888868\n",
      "Val loss: 0.5587290489161498\n",
      "Val loss: 0.5585091274301961\n",
      "Val loss: 0.5586430919476044\n",
      "Val loss: 0.5584853625242612\n",
      "Val loss: 0.5585082839996234\n",
      "Val loss: 0.5580508018936123\n",
      "Val loss: 0.5579166432265981\n",
      "Val loss: 0.5580594006614013\n",
      "Val loss: 0.5581434618413837\n",
      "Val loss: 0.5581812369155472\n",
      "Val loss: 0.5581030100901753\n",
      "Val loss: 0.5578607059229276\n",
      "Val loss: 0.5581081156193091\n",
      "Val loss: 0.5582099755194562\n",
      "Val loss: 0.5582394374416406\n",
      "Val loss: 0.5582545410524978\n",
      "Val loss: 0.5587184803041523\n",
      "Val loss: 0.5586044615696347\n",
      "Val loss: 0.5584275930944267\n",
      "Val loss: 0.5587682624271407\n",
      "Val loss: 0.5585484980158724\n",
      "Val loss: 0.5584611599226944\n",
      "Val loss: 0.5583748675474373\n",
      "Val loss: 0.5580645695830999\n",
      "Val loss: 0.5580471618056076\n",
      "Val loss: 0.5578877224093851\n",
      "Val loss: 0.5580042708658347\n",
      "Val loss: 0.5580222086463164\n",
      "Val loss: 0.5581078508565592\n",
      "Val loss: 0.5580479019182794\n",
      "Val loss: 0.5582170419931831\n",
      "Val loss: 0.5580711546467572\n",
      "Val loss: 0.5582029646542406\n",
      "Val loss: 0.5583230170280966\n",
      "Val loss: 0.5583383070592767\n",
      "Val loss: 0.5584561328594934\n",
      "Val loss: 0.5587014449956222\n",
      "Val loss: 0.5589062125970986\n",
      "Val loss: 0.5591468469947821\n",
      "Val loss: 0.5591934817912912\n",
      "Val loss: 0.5592027962785545\n",
      "Val loss: 0.5593248484895016\n",
      "Val loss: 0.5594626585042344\n",
      "Val loss: 0.5593402754715189\n",
      "\n",
      "starting Epoch 25\n",
      "Training...\n",
      "Train loss: 0.6016165143565128\n",
      "Train loss: 0.5921818698063875\n",
      "Train loss: 0.5872524360478935\n",
      "Train loss: 0.5867697200443172\n",
      "Train loss: 0.5877242861974119\n",
      "Train loss: 0.5864892709655922\n",
      "Train loss: 0.5855091735184621\n",
      "Train loss: 0.5819878328896169\n",
      "Train loss: 0.583611637686884\n",
      "Train loss: 0.5807845963904607\n",
      "Train loss: 0.5798932058081779\n",
      "Train loss: 0.5778107765329433\n",
      "Train loss: 0.5782718042839448\n",
      "Train loss: 0.5779101834715908\n",
      "Train loss: 0.578060191909605\n",
      "Train loss: 0.5787285174882524\n",
      "Train loss: 0.5777149972310812\n",
      "Train loss: 0.5778716500423081\n",
      "Train loss: 0.5772748072103334\n",
      "Train loss: 0.5773574566482601\n",
      "Train loss: 0.5771677422210538\n",
      "Train loss: 0.5769958177025606\n",
      "Train loss: 0.5761643304674194\n",
      "Train loss: 0.5760161312363093\n",
      "Train loss: 0.5762748469212251\n",
      "Train loss: 0.5750505603232595\n",
      "Train loss: 0.5756619123116495\n",
      "Train loss: 0.5754220960178614\n",
      "Train loss: 0.5757836871402457\n",
      "Train loss: 0.5756132563883952\n",
      "Train loss: 0.5752707020724147\n",
      "Train loss: 0.5761513988923206\n",
      "Train loss: 0.5762181676392852\n",
      "Train loss: 0.5762392541445759\n",
      "Train loss: 0.5768803520946203\n",
      "Train loss: 0.5768137726730697\n",
      "Train loss: 0.5766477247294296\n",
      "Train loss: 0.5770154648775797\n",
      "Train loss: 0.5767909283050371\n",
      "Train loss: 0.5771434382070438\n",
      "Train loss: 0.5768686691264966\n",
      "Train loss: 0.5772150366326196\n",
      "Train loss: 0.5770450190698448\n",
      "Train loss: 0.5771045361362627\n",
      "Train loss: 0.5771969257127721\n",
      "Train loss: 0.5769150205548362\n",
      "Train loss: 0.5768442256128191\n",
      "Train loss: 0.5770497828628771\n",
      "Train loss: 0.5773468338432546\n",
      "Train loss: 0.5780618256396121\n",
      "Train loss: 0.5783642439191778\n",
      "Train loss: 0.5782345994030545\n",
      "Train loss: 0.5783156038906576\n",
      "Train loss: 0.5782362395022289\n",
      "Train loss: 0.5784321742181456\n",
      "Train loss: 0.5784297326081986\n",
      "Train loss: 0.5785518847504031\n",
      "Train loss: 0.5785499986797905\n",
      "Train loss: 0.5782034224226275\n",
      "Train loss: 0.5782418006454735\n",
      "Train loss: 0.5781846318555914\n",
      "Train loss: 0.5780484163299311\n",
      "Train loss: 0.5778347014244631\n",
      "Train loss: 0.5776672872413966\n",
      "Train loss: 0.5774445048747749\n",
      "Train loss: 0.577521303709571\n",
      "Train loss: 0.5774844040213636\n",
      "Train loss: 0.5773730463758824\n",
      "Train loss: 0.5773900372554981\n",
      "Train loss: 0.5774148926298647\n",
      "Train loss: 0.5774460119878515\n",
      "Train loss: 0.5770416159319993\n",
      "Train loss: 0.5769812131761443\n",
      "Train loss: 0.5770683592441035\n",
      "Train loss: 0.5771491005072361\n",
      "Train loss: 0.5771227949265197\n",
      "Train loss: 0.5773828857281209\n",
      "Train loss: 0.577533592774059\n",
      "Train loss: 0.5775565290239659\n",
      "Train loss: 0.5775800745251628\n",
      "Train loss: 0.5774349254763664\n",
      "Train loss: 0.5773153675264379\n",
      "Train loss: 0.5772284148614618\n",
      "Train loss: 0.5770929455402141\n",
      "Train loss: 0.5771350769031182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5771125820294725\n",
      "Train loss: 0.5773242064904317\n",
      "Train loss: 0.5775210441277067\n",
      "Train loss: 0.5774278507231326\n",
      "Train loss: 0.5777050196097386\n",
      "Train loss: 0.5777235773318806\n",
      "Train loss: 0.5779577503391036\n",
      "Train loss: 0.5781649973778522\n",
      "Train loss: 0.5781911555092788\n",
      "Train loss: 0.578135731723699\n",
      "Train loss: 0.5779789855481436\n",
      "Train loss: 0.5780425712489294\n",
      "Train loss: 0.5777968093866959\n",
      "Train loss: 0.5777944944132573\n",
      "Train loss: 0.5778010947934504\n",
      "Train loss: 0.5778000199924666\n",
      "Train loss: 0.5777536421327277\n",
      "Train loss: 0.5775659733920957\n",
      "Train loss: 0.5775796120530312\n",
      "Train loss: 0.5774468198567699\n",
      "Train loss: 0.5774277187338861\n",
      "Train loss: 0.5773816057988114\n",
      "Train loss: 0.5773146304475759\n",
      "Train loss: 0.5771922198059691\n",
      "Train loss: 0.5772211218406093\n",
      "Train loss: 0.577262507910232\n",
      "Train loss: 0.5770479856643788\n",
      "Train loss: 0.5771279860355517\n",
      "Train loss: 0.5770869452919861\n",
      "Train loss: 0.5770574389410413\n",
      "Train loss: 0.577068878227701\n",
      "Train loss: 0.5770288642755478\n",
      "Train loss: 0.5770251853782375\n",
      "Train loss: 0.5768717278097296\n",
      "Train loss: 0.5770167033092933\n",
      "Train loss: 0.5770122638390349\n",
      "Train loss: 0.5770262594274831\n",
      "Train loss: 0.5770211589559398\n",
      "Train loss: 0.5769895481914221\n",
      "Train loss: 0.5769525425178426\n",
      "Train loss: 0.5769685127045153\n",
      "Train loss: 0.5769841757380338\n",
      "Train loss: 0.5769580326167901\n",
      "Train loss: 0.5770078292878481\n",
      "Train loss: 0.5770262168228008\n",
      "Train loss: 0.57684508273282\n",
      "Train loss: 0.5768598848759744\n",
      "Train loss: 0.5768523549015875\n",
      "Train loss: 0.5767790880853804\n",
      "Train loss: 0.5767392909782999\n",
      "Train loss: 0.5767612847807478\n",
      "Train loss: 0.5768850877821206\n",
      "Train loss: 0.5768410071336346\n",
      "Train loss: 0.5767913826209429\n",
      "Train loss: 0.5768640355665201\n",
      "Train loss: 0.5768105976439486\n",
      "Train loss: 0.576859794082084\n",
      "Train loss: 0.5768763900178427\n",
      "Train loss: 0.5769530194382901\n",
      "Train loss: 0.5768464207957638\n",
      "Train loss: 0.57679853629437\n",
      "Train loss: 0.5766219949491092\n",
      "Train loss: 0.5766071157807553\n",
      "Train loss: 0.5765795437690995\n",
      "Train loss: 0.576504931534557\n",
      "Train loss: 0.5764798315975516\n",
      "Train loss: 0.5764864445536809\n",
      "Train loss: 0.5764524301659558\n",
      "Train loss: 0.5765043643241813\n",
      "Train loss: 0.576485873316903\n",
      "Train loss: 0.5763587291212409\n",
      "Train loss: 0.5764057591258435\n",
      "Train loss: 0.5764916291075366\n",
      "Train loss: 0.5765952458731144\n",
      "Train loss: 0.5765608391582164\n",
      "Train loss: 0.5766348168045587\n",
      "Train loss: 0.5767083676757913\n",
      "Train loss: 0.5766856226591144\n",
      "Train loss: 0.576617405950096\n",
      "Train loss: 0.5766565461580087\n",
      "Train loss: 0.576648808676782\n",
      "Train loss: 0.5767314140669562\n",
      "Train loss: 0.5766607054059226\n",
      "Train loss: 0.5766650670712147\n",
      "Train loss: 0.5766892735222993\n",
      "Train loss: 0.5767693742477744\n",
      "Train loss: 0.5768821950862143\n",
      "Train loss: 0.5768565006271956\n",
      "Train loss: 0.5767709538404816\n",
      "Train loss: 0.5768498785071933\n",
      "Train loss: 0.5767738253479486\n",
      "Train loss: 0.5768134005159333\n",
      "Train loss: 0.5769019553955435\n",
      "Train loss: 0.57678204978868\n",
      "Train loss: 0.5766193850499519\n",
      "Train loss: 0.5765709345447454\n",
      "Train loss: 0.57656661429455\n",
      "Train loss: 0.5766186781938234\n",
      "Train loss: 0.576634448097594\n",
      "Train loss: 0.57666960427103\n",
      "Train loss: 0.5767375664802032\n",
      "Train loss: 0.5767462294460076\n",
      "Train loss: 0.5767591653446721\n",
      "Train loss: 0.5768725300204284\n",
      "Train loss: 0.5768532634456084\n",
      "Train loss: 0.5769038318448367\n",
      "Train loss: 0.5768859945460824\n",
      "Train loss: 0.576889999791976\n",
      "Train loss: 0.5768486850192234\n",
      "Train loss: 0.5770367153426506\n",
      "Train loss: 0.5770602908843815\n",
      "Train loss: 0.5771247191488456\n",
      "Train loss: 0.5771431206572866\n",
      "Train loss: 0.5771790248989969\n",
      "Train loss: 0.5772615230122218\n",
      "Train loss: 0.5772019542177153\n",
      "Train loss: 0.5772821752778313\n",
      "Train loss: 0.5773090960195894\n",
      "Train loss: 0.5773068606984998\n",
      "Train loss: 0.5772631310869875\n",
      "Train loss: 0.5771868080732805\n",
      "Train loss: 0.5771963288744281\n",
      "Train loss: 0.5770676704331871\n",
      "Train loss: 0.5770654323412545\n",
      "Train loss: 0.5770415877495303\n",
      "Train loss: 0.5769584999742257\n",
      "Train loss: 0.5769776286424176\n",
      "Train loss: 0.5770211130919437\n",
      "Train loss: 0.5770674073047019\n",
      "Train loss: 0.5771107168274165\n",
      "Train loss: 0.5770920103320537\n",
      "Train loss: 0.577124272961033\n",
      "Train loss: 0.5771068101302285\n",
      "Train loss: 0.5771727904755657\n",
      "Train loss: 0.5771900209263526\n",
      "Train loss: 0.5772697176766574\n",
      "Train loss: 0.5773943960398107\n",
      "Train loss: 0.5774519270606361\n",
      "Train loss: 0.5774331904862922\n",
      "Train loss: 0.5774932184148349\n",
      "Train loss: 0.5775418730535505\n",
      "Train loss: 0.57753885540833\n",
      "Train loss: 0.5775267091020206\n",
      "Train loss: 0.5775783049224583\n",
      "Train loss: 0.5776008268780594\n",
      "Train loss: 0.5775924898582404\n",
      "Train loss: 0.5775958644107746\n",
      "Train loss: 0.5776618932879363\n",
      "Train loss: 0.5776476368967298\n",
      "Train loss: 0.5776082877213206\n",
      "Train loss: 0.5776542786080141\n",
      "Train loss: 0.577626823957622\n",
      "Train loss: 0.5776374662754089\n",
      "Train loss: 0.5776280428568024\n",
      "Train loss: 0.577645193179167\n",
      "Train loss: 0.5776115455897574\n",
      "Train loss: 0.5775767076505711\n",
      "Train loss: 0.5777338153542155\n",
      "Train loss: 0.5777044608398011\n",
      "Train loss: 0.5776600062628137\n",
      "Train loss: 0.5776537302536896\n",
      "Train loss: 0.5777183671510758\n",
      "Train loss: 0.5777008628491842\n",
      "Train loss: 0.5776774064020642\n",
      "Train loss: 0.5776899177554989\n",
      "Train loss: 0.577741183811128\n",
      "Train loss: 0.5777003162967991\n",
      "Train loss: 0.5777718733777452\n",
      "Train loss: 0.5777588524401106\n",
      "Train loss: 0.5778258835002239\n",
      "Train loss: 0.5778668541697745\n",
      "Train loss: 0.5779082102113532\n",
      "Train loss: 0.5779528229140477\n",
      "Train loss: 0.5779387820386178\n",
      "Train loss: 0.5778966016507099\n",
      "Train loss: 0.5778663991556974\n",
      "Train loss: 0.5778666228641874\n",
      "Train loss: 0.5778807305659511\n",
      "Train loss: 0.5778422376991651\n",
      "Train loss: 0.5778080108008715\n",
      "Train loss: 0.5778285935871312\n",
      "Train loss: 0.5779009181186264\n",
      "Train loss: 0.5778392268041328\n",
      "Train loss: 0.577814162076683\n",
      "Train loss: 0.577804176006787\n",
      "Train loss: 0.5778471335732952\n",
      "Train loss: 0.5778698747397828\n",
      "Train loss: 0.5778630658326338\n",
      "Train loss: 0.5779134805053663\n",
      "Train loss: 0.5778858407527062\n",
      "Train loss: 0.5779212468461857\n",
      "Train loss: 0.5778948805041209\n",
      "Train loss: 0.5779457424619383\n",
      "Train loss: 0.5779085273511052\n",
      "Train loss: 0.5778797001822928\n",
      "Train loss: 0.5778541597347918\n",
      "Train loss: 0.5778193426461988\n",
      "Train loss: 0.5778067376392851\n",
      "Train loss: 0.5778168958034036\n",
      "Train loss: 0.5777970041950913\n",
      "Train loss: 0.577696146472473\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6976048797369003\n",
      "Val loss: 0.6142314804924859\n",
      "Val loss: 0.5864790763173785\n",
      "Val loss: 0.5903406990201849\n",
      "Val loss: 0.5908606126904488\n",
      "Val loss: 0.5948757632025357\n",
      "Val loss: 0.5886253609376795\n",
      "Val loss: 0.5828207379732376\n",
      "Val loss: 0.5774441916834224\n",
      "Val loss: 0.577939763361094\n",
      "Val loss: 0.5782629538465429\n",
      "Val loss: 0.5773305317102853\n",
      "Val loss: 0.5765219070017338\n",
      "Val loss: 0.5765645737233369\n",
      "Val loss: 0.5754482552811906\n",
      "Val loss: 0.5738618815247016\n",
      "Val loss: 0.5724868022260212\n",
      "Val loss: 0.5714917695254422\n",
      "Val loss: 0.5722989969431086\n",
      "Val loss: 0.5698683927155505\n",
      "Val loss: 0.5705988954466122\n",
      "Val loss: 0.5691295260683112\n",
      "Val loss: 0.5679285913182978\n",
      "Val loss: 0.5675449511584114\n",
      "Val loss: 0.5674939552141774\n",
      "Val loss: 0.5658469509708789\n",
      "Val loss: 0.565521029171659\n",
      "Val loss: 0.5642521049050119\n",
      "Val loss: 0.56339799053967\n",
      "Val loss: 0.5635966196556219\n",
      "Val loss: 0.5633372174842017\n",
      "Val loss: 0.5635408146201439\n",
      "Val loss: 0.5632058660068163\n",
      "Val loss: 0.5625477780604503\n",
      "Val loss: 0.5622116748628945\n",
      "Val loss: 0.5621902742865366\n",
      "Val loss: 0.5602282403927782\n",
      "Val loss: 0.560234640483503\n",
      "Val loss: 0.5611738149345535\n",
      "Val loss: 0.5602174786167529\n",
      "Val loss: 0.560713959821299\n",
      "Val loss: 0.5603189268751008\n",
      "Val loss: 0.5597886814970836\n",
      "Val loss: 0.5599685891033852\n",
      "Val loss: 0.5596370772857752\n",
      "Val loss: 0.5598541733739678\n",
      "Val loss: 0.5595433560446796\n",
      "Val loss: 0.559647191767912\n",
      "Val loss: 0.559598152632596\n",
      "Val loss: 0.5597746151996904\n",
      "Val loss: 0.5591134549829904\n",
      "Val loss: 0.5583264833481616\n",
      "Val loss: 0.5581380288031969\n",
      "Val loss: 0.5576495821812782\n",
      "Val loss: 0.5578800538813111\n",
      "Val loss: 0.557177620335719\n",
      "Val loss: 0.5572050037518353\n",
      "Val loss: 0.5574390058080217\n",
      "Val loss: 0.5575153351235552\n",
      "Val loss: 0.5574682261234143\n",
      "Val loss: 0.5576418026497489\n",
      "Val loss: 0.5577866942365578\n",
      "Val loss: 0.5571665701213157\n",
      "Val loss: 0.5568247588824329\n",
      "Val loss: 0.5565680878949754\n",
      "Val loss: 0.5566307504669874\n",
      "Val loss: 0.5563661450754382\n",
      "Val loss: 0.5559931980297629\n",
      "Val loss: 0.5570172590398511\n",
      "Val loss: 0.5569333658689756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5562262296003136\n",
      "Val loss: 0.556067746495802\n",
      "Val loss: 0.5557638419853462\n",
      "Val loss: 0.5561417531837939\n",
      "Val loss: 0.5562193956285875\n",
      "Val loss: 0.5557502781810106\n",
      "Val loss: 0.5559172881767154\n",
      "Val loss: 0.5556013927361653\n",
      "Val loss: 0.5551600205868029\n",
      "Val loss: 0.5552786517710913\n",
      "Val loss: 0.5556158382851298\n",
      "Val loss: 0.5551197337929369\n",
      "Val loss: 0.5553198825334005\n",
      "Val loss: 0.554984188236314\n",
      "Val loss: 0.5546457702821156\n",
      "Val loss: 0.5548018340742116\n",
      "Val loss: 0.5546102388525889\n",
      "Val loss: 0.5546581348688564\n",
      "Val loss: 0.5541052054446023\n",
      "Val loss: 0.5539184395879307\n",
      "Val loss: 0.5541188484509085\n",
      "Val loss: 0.5541348265796445\n",
      "Val loss: 0.5541497136883694\n",
      "Val loss: 0.5540464311393339\n",
      "Val loss: 0.5537625491493362\n",
      "Val loss: 0.5539616685446219\n",
      "Val loss: 0.5540314398644385\n",
      "Val loss: 0.5540344624309695\n",
      "Val loss: 0.5540062172210168\n",
      "Val loss: 0.5546012253226164\n",
      "Val loss: 0.5544974436126058\n",
      "Val loss: 0.5542962890711187\n",
      "Val loss: 0.5546665298335747\n",
      "Val loss: 0.5543897885807677\n",
      "Val loss: 0.5543472514352725\n",
      "Val loss: 0.5542705097360737\n",
      "Val loss: 0.5538765666413397\n",
      "Val loss: 0.5538042626708248\n",
      "Val loss: 0.5535978257436963\n",
      "Val loss: 0.5537049621615037\n",
      "Val loss: 0.5537404564851458\n",
      "Val loss: 0.5538330507299768\n",
      "Val loss: 0.5537710609905263\n",
      "Val loss: 0.5540595979074602\n",
      "Val loss: 0.5538494841873853\n",
      "Val loss: 0.5540155645376248\n",
      "Val loss: 0.5541549741915645\n",
      "Val loss: 0.554172067721145\n",
      "Val loss: 0.554272975333612\n",
      "Val loss: 0.5545886963954155\n",
      "Val loss: 0.5548477808371285\n",
      "Val loss: 0.5552046404683532\n",
      "Val loss: 0.5552983579592907\n",
      "Val loss: 0.5553414431161372\n",
      "Val loss: 0.5555477521549432\n",
      "Val loss: 0.5556914742876122\n",
      "Val loss: 0.5555514156442335\n",
      "\n",
      "starting Epoch 26\n",
      "Training...\n",
      "Train loss: 0.6227118874851026\n",
      "Train loss: 0.5966658821472755\n",
      "Train loss: 0.5887910774198629\n",
      "Train loss: 0.5863157797463333\n",
      "Train loss: 0.5871294511087013\n",
      "Train loss: 0.5874185414374375\n",
      "Train loss: 0.5856282678868273\n",
      "Train loss: 0.5861282328014854\n",
      "Train loss: 0.586587117204453\n",
      "Train loss: 0.5862876838775136\n",
      "Train loss: 0.5848768021418079\n",
      "Train loss: 0.5850276155202458\n",
      "Train loss: 0.585445830835799\n",
      "Train loss: 0.5856947049658786\n",
      "Train loss: 0.5846271957442114\n",
      "Train loss: 0.5846108014120204\n",
      "Train loss: 0.5840321907603283\n",
      "Train loss: 0.584316658442399\n",
      "Train loss: 0.5840413874718005\n",
      "Train loss: 0.5843207418619839\n",
      "Train loss: 0.5842395734246421\n",
      "Train loss: 0.5838540613243954\n",
      "Train loss: 0.5835331966674405\n",
      "Train loss: 0.5838648931343022\n",
      "Train loss: 0.5845043308032538\n",
      "Train loss: 0.5837508391782728\n",
      "Train loss: 0.5838083643639022\n",
      "Train loss: 0.583814722054333\n",
      "Train loss: 0.5839966629762106\n",
      "Train loss: 0.5832234557363545\n",
      "Train loss: 0.5834612834338032\n",
      "Train loss: 0.5827501917109243\n",
      "Train loss: 0.5826574770199511\n",
      "Train loss: 0.5821067783899265\n",
      "Train loss: 0.5817140569332161\n",
      "Train loss: 0.5814699070510015\n",
      "Train loss: 0.5809831412783818\n",
      "Train loss: 0.5814914346055394\n",
      "Train loss: 0.5811810895551919\n",
      "Train loss: 0.5809422711630786\n",
      "Train loss: 0.581155567742675\n",
      "Train loss: 0.580844266266874\n",
      "Train loss: 0.5805340557423126\n",
      "Train loss: 0.5802171877558864\n",
      "Train loss: 0.5802612482705821\n",
      "Train loss: 0.5806982293806087\n",
      "Train loss: 0.5798962387604455\n",
      "Train loss: 0.5796363348446251\n",
      "Train loss: 0.5797720181016561\n",
      "Train loss: 0.5793644079753945\n",
      "Train loss: 0.5793458967084857\n",
      "Train loss: 0.5796087379347716\n",
      "Train loss: 0.5799191349570316\n",
      "Train loss: 0.5802792479474861\n",
      "Train loss: 0.5802411974778058\n",
      "Train loss: 0.5800415639157163\n",
      "Train loss: 0.5803453278342708\n",
      "Train loss: 0.580257278099669\n",
      "Train loss: 0.5801927790589207\n",
      "Train loss: 0.5802283949957379\n",
      "Train loss: 0.5801053851280572\n",
      "Train loss: 0.5801908571312175\n",
      "Train loss: 0.5798414203976714\n",
      "Train loss: 0.5796418549774027\n",
      "Train loss: 0.5794372582912812\n",
      "Train loss: 0.5791417906644039\n",
      "Train loss: 0.5793621571007017\n",
      "Train loss: 0.5792459956725965\n",
      "Train loss: 0.5789939543533532\n",
      "Train loss: 0.5789242248138076\n",
      "Train loss: 0.5787882980240155\n",
      "Train loss: 0.5787526719722589\n",
      "Train loss: 0.5787923157296825\n",
      "Train loss: 0.5788794296911393\n",
      "Train loss: 0.5786870588573955\n",
      "Train loss: 0.5789173143211362\n",
      "Train loss: 0.5785606081722773\n",
      "Train loss: 0.5786907468146434\n",
      "Train loss: 0.5787433457382123\n",
      "Train loss: 0.5786370432063741\n",
      "Train loss: 0.5785840097918342\n",
      "Train loss: 0.5785843493590782\n",
      "Train loss: 0.5786991625611073\n",
      "Train loss: 0.5788085533260519\n",
      "Train loss: 0.5789014251706458\n",
      "Train loss: 0.5788280251866652\n",
      "Train loss: 0.578736530168088\n",
      "Train loss: 0.57874999172041\n",
      "Train loss: 0.5788643233993203\n",
      "Train loss: 0.5786761991211677\n",
      "Train loss: 0.5782597653973293\n",
      "Train loss: 0.5784286044906701\n",
      "Train loss: 0.5785296410722973\n",
      "Train loss: 0.5786025949048513\n",
      "Train loss: 0.5786296571004134\n",
      "Train loss: 0.5785071254242454\n",
      "Train loss: 0.5784616107759923\n",
      "Train loss: 0.57804869180008\n",
      "Train loss: 0.5779359894548178\n",
      "Train loss: 0.5781576317987065\n",
      "Train loss: 0.5779343170952006\n",
      "Train loss: 0.577776368267107\n",
      "Train loss: 0.5777690741348869\n",
      "Train loss: 0.5779005039506603\n",
      "Train loss: 0.5780032134981823\n",
      "Train loss: 0.5780757353827884\n",
      "Train loss: 0.5779865266648114\n",
      "Train loss: 0.5779187488936892\n",
      "Train loss: 0.5778423487155357\n",
      "Train loss: 0.5778770313690119\n",
      "Train loss: 0.5777224681704041\n",
      "Train loss: 0.5777584854887766\n",
      "Train loss: 0.5776071567723264\n",
      "Train loss: 0.5774046007401591\n",
      "Train loss: 0.5775748855681874\n",
      "Train loss: 0.5777184932383035\n",
      "Train loss: 0.5777601656902545\n",
      "Train loss: 0.5777195476156617\n",
      "Train loss: 0.5777113886002785\n",
      "Train loss: 0.5777461362063959\n",
      "Train loss: 0.5776643440614017\n",
      "Train loss: 0.5775207530259597\n",
      "Train loss: 0.5776588386057256\n",
      "Train loss: 0.5776549979244908\n",
      "Train loss: 0.5777772563774617\n",
      "Train loss: 0.5779776592465704\n",
      "Train loss: 0.5782381237351364\n",
      "Train loss: 0.5781669062811586\n",
      "Train loss: 0.5783226512438939\n",
      "Train loss: 0.5782376839225684\n",
      "Train loss: 0.5782780642726122\n",
      "Train loss: 0.5782542359585019\n",
      "Train loss: 0.5781624458615338\n",
      "Train loss: 0.5781637946252904\n",
      "Train loss: 0.5783553022108682\n",
      "Train loss: 0.5782948142248464\n",
      "Train loss: 0.5782545225143259\n",
      "Train loss: 0.5783049954414022\n",
      "Train loss: 0.5783468529812592\n",
      "Train loss: 0.578415835998279\n",
      "Train loss: 0.5784666357669648\n",
      "Train loss: 0.5784267350039293\n",
      "Train loss: 0.5783513691843118\n",
      "Train loss: 0.5784247701948159\n",
      "Train loss: 0.5783160906612729\n",
      "Train loss: 0.5785766406882581\n",
      "Train loss: 0.5785801541383431\n",
      "Train loss: 0.5784846810160074\n",
      "Train loss: 0.5787060523481328\n",
      "Train loss: 0.5786292538041868\n",
      "Train loss: 0.5784990442395408\n",
      "Train loss: 0.578384265308515\n",
      "Train loss: 0.5782960156827006\n",
      "Train loss: 0.5783087434649429\n",
      "Train loss: 0.5782733847918762\n",
      "Train loss: 0.5782461400290413\n",
      "Train loss: 0.5780526476420579\n",
      "Train loss: 0.5781352503573679\n",
      "Train loss: 0.5780321270386803\n",
      "Train loss: 0.5779851001197377\n",
      "Train loss: 0.5781144220272033\n",
      "Train loss: 0.5780607835769506\n",
      "Train loss: 0.578028000381396\n",
      "Train loss: 0.5780637747414703\n",
      "Train loss: 0.5780147667471731\n",
      "Train loss: 0.5780344968569067\n",
      "Train loss: 0.5779970517484111\n",
      "Train loss: 0.5779760898969541\n",
      "Train loss: 0.5780190676142034\n",
      "Train loss: 0.5781300470263652\n",
      "Train loss: 0.5780195515296653\n",
      "Train loss: 0.5779327771264338\n",
      "Train loss: 0.5779277156305024\n",
      "Train loss: 0.577820729176249\n",
      "Train loss: 0.5777402729451844\n",
      "Train loss: 0.5777831081302033\n",
      "Train loss: 0.5778462078551783\n",
      "Train loss: 0.5778970614046353\n",
      "Train loss: 0.5779531014618443\n",
      "Train loss: 0.57807143916518\n",
      "Train loss: 0.5780719193792304\n",
      "Train loss: 0.5780342881268085\n",
      "Train loss: 0.5780943492277296\n",
      "Train loss: 0.5780867737624009\n",
      "Train loss: 0.5779912884926983\n",
      "Train loss: 0.5780424295766087\n",
      "Train loss: 0.5779954265722396\n",
      "Train loss: 0.5780412022358371\n",
      "Train loss: 0.5780367511988759\n",
      "Train loss: 0.5780365278796793\n",
      "Train loss: 0.5779713826143296\n",
      "Train loss: 0.5780857529265088\n",
      "Train loss: 0.5782243797710505\n",
      "Train loss: 0.578265599704766\n",
      "Train loss: 0.5782552774700332\n",
      "Train loss: 0.5781546156377201\n",
      "Train loss: 0.5781873005330487\n",
      "Train loss: 0.5781345308790546\n",
      "Train loss: 0.5782276188781975\n",
      "Train loss: 0.5782465711850827\n",
      "Train loss: 0.5782332088141099\n",
      "Train loss: 0.5781431108986159\n",
      "Train loss: 0.5781922322110784\n",
      "Train loss: 0.5781878322530944\n",
      "Train loss: 0.5782369743849947\n",
      "Train loss: 0.5783077418355578\n",
      "Train loss: 0.5783818147556308\n",
      "Train loss: 0.578427399658016\n",
      "Train loss: 0.5785141326032435\n",
      "Train loss: 0.5785172327937725\n",
      "Train loss: 0.5784635231460432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5785252577602146\n",
      "Train loss: 0.578564366003414\n",
      "Train loss: 0.5785492514165436\n",
      "Train loss: 0.5785819094079571\n",
      "Train loss: 0.5785458097029517\n",
      "Train loss: 0.5784841670943834\n",
      "Train loss: 0.5785359749897381\n",
      "Train loss: 0.578598350408987\n",
      "Train loss: 0.5786589773561933\n",
      "Train loss: 0.5786736152659824\n",
      "Train loss: 0.5786197654874402\n",
      "Train loss: 0.5786394060567025\n",
      "Train loss: 0.5784767399837087\n",
      "Train loss: 0.578439855680224\n",
      "Train loss: 0.5784297947481054\n",
      "Train loss: 0.5784281536355683\n",
      "Train loss: 0.57837185023071\n",
      "Train loss: 0.5783192419503344\n",
      "Train loss: 0.5781755973383457\n",
      "Train loss: 0.5782246005744174\n",
      "Train loss: 0.5782010972140087\n",
      "Train loss: 0.5782278902695961\n",
      "Train loss: 0.5782473191131662\n",
      "Train loss: 0.5782412802647925\n",
      "Train loss: 0.578305246915118\n",
      "Train loss: 0.578273856550562\n",
      "Train loss: 0.5782201443932893\n",
      "Train loss: 0.5782715933458604\n",
      "Train loss: 0.5783341298701491\n",
      "Train loss: 0.5783405682622063\n",
      "Train loss: 0.5783073067246304\n",
      "Train loss: 0.5783165417830869\n",
      "Train loss: 0.5784192274073202\n",
      "Train loss: 0.5784027290833339\n",
      "Train loss: 0.5784007767929151\n",
      "Train loss: 0.5784442804740204\n",
      "Train loss: 0.5784140657359348\n",
      "Train loss: 0.5784286500627579\n",
      "Train loss: 0.5784976733138643\n",
      "Train loss: 0.5785859354014035\n",
      "Train loss: 0.5785537896990942\n",
      "Train loss: 0.5785412288013714\n",
      "Train loss: 0.578562006093217\n",
      "Train loss: 0.5785772032213108\n",
      "Train loss: 0.5785017436685728\n",
      "Train loss: 0.5785637541860197\n",
      "Train loss: 0.5785496107631701\n",
      "Train loss: 0.5786180082297873\n",
      "Train loss: 0.5785582535231106\n",
      "Train loss: 0.5786019558706921\n",
      "Train loss: 0.5786016404594623\n",
      "Train loss: 0.5786112864608206\n",
      "Train loss: 0.5785913824368932\n",
      "Train loss: 0.5786618510660844\n",
      "Train loss: 0.5787117209990774\n",
      "Train loss: 0.5786510947257933\n",
      "Train loss: 0.5787251279422609\n",
      "Train loss: 0.5787172405432538\n",
      "Train loss: 0.5787499437654078\n",
      "Train loss: 0.578728153704937\n",
      "Train loss: 0.5786977903186278\n",
      "Train loss: 0.5786074710582328\n",
      "Train loss: 0.5786051582922398\n",
      "Train loss: 0.5786256294973244\n",
      "Train loss: 0.5786204861058859\n",
      "Train loss: 0.578637539904607\n",
      "Train loss: 0.5786238847758449\n",
      "Train loss: 0.5786641423987282\n",
      "Train loss: 0.5786806391400724\n",
      "Train loss: 0.5786768455674246\n",
      "Train loss: 0.5786618941228511\n",
      "Train loss: 0.5785917479371746\n",
      "Train loss: 0.5785457271781475\n",
      "Train loss: 0.5785667095232018\n",
      "Train loss: 0.5786003017471395\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6949619948863983\n",
      "Val loss: 0.6193216575516595\n",
      "Val loss: 0.5914178277765002\n",
      "Val loss: 0.5925752865640741\n",
      "Val loss: 0.5930965716640154\n",
      "Val loss: 0.5951302647590637\n",
      "Val loss: 0.5888528438175425\n",
      "Val loss: 0.5838551506018027\n",
      "Val loss: 0.57888332686641\n",
      "Val loss: 0.5785879796865036\n",
      "Val loss: 0.579031205839581\n",
      "Val loss: 0.5776574379306728\n",
      "Val loss: 0.5773741081357002\n",
      "Val loss: 0.5770828775737596\n",
      "Val loss: 0.5760380504904566\n",
      "Val loss: 0.5746464970745618\n",
      "Val loss: 0.5731538874762399\n",
      "Val loss: 0.5726291714759355\n",
      "Val loss: 0.5736655919475758\n",
      "Val loss: 0.5714334219393103\n",
      "Val loss: 0.5719638535609612\n",
      "Val loss: 0.5707814108341112\n",
      "Val loss: 0.569697064788718\n",
      "Val loss: 0.5689271851247099\n",
      "Val loss: 0.5687857676417597\n",
      "Val loss: 0.5674000731272291\n",
      "Val loss: 0.5671711913240489\n",
      "Val loss: 0.5660413145590172\n",
      "Val loss: 0.565349179216557\n",
      "Val loss: 0.5654780134658686\n",
      "Val loss: 0.5653556668526166\n",
      "Val loss: 0.5652685917023593\n",
      "Val loss: 0.565003847385325\n",
      "Val loss: 0.564439685563364\n",
      "Val loss: 0.5641611328070191\n",
      "Val loss: 0.5641306115928308\n",
      "Val loss: 0.562533603090307\n",
      "Val loss: 0.5625301195830895\n",
      "Val loss: 0.5633204434950327\n",
      "Val loss: 0.5625595386004328\n",
      "Val loss: 0.5629425092655069\n",
      "Val loss: 0.5624583862614974\n",
      "Val loss: 0.5620431987760223\n",
      "Val loss: 0.5621530327894916\n",
      "Val loss: 0.561590981669724\n",
      "Val loss: 0.5618111836077344\n",
      "Val loss: 0.5616037294141247\n",
      "Val loss: 0.5617340249496524\n",
      "Val loss: 0.5618421640552458\n",
      "Val loss: 0.5620714990968206\n",
      "Val loss: 0.5615344198904638\n",
      "Val loss: 0.5610327621684571\n",
      "Val loss: 0.5607931532643058\n",
      "Val loss: 0.5604182919383492\n",
      "Val loss: 0.5604521955234291\n",
      "Val loss: 0.5598502759437834\n",
      "Val loss: 0.5599398330693514\n",
      "Val loss: 0.5600182644017427\n",
      "Val loss: 0.5601110274897141\n",
      "Val loss: 0.560022850399432\n",
      "Val loss: 0.5601859044675764\n",
      "Val loss: 0.5603435327898723\n",
      "Val loss: 0.5598073529589708\n",
      "Val loss: 0.5594980749403795\n",
      "Val loss: 0.5593494964234623\n",
      "Val loss: 0.5594598555818517\n",
      "Val loss: 0.559329277384067\n",
      "Val loss: 0.5588961236596459\n",
      "Val loss: 0.55970145432755\n",
      "Val loss: 0.5595177172931354\n",
      "Val loss: 0.5588663101701413\n",
      "Val loss: 0.558797628268558\n",
      "Val loss: 0.5585490280946532\n",
      "Val loss: 0.5588954448861482\n",
      "Val loss: 0.5589863684247521\n",
      "Val loss: 0.5587475380828324\n",
      "Val loss: 0.5589771107770503\n",
      "Val loss: 0.5587679682476662\n",
      "Val loss: 0.5583509187105343\n",
      "Val loss: 0.5585529977516422\n",
      "Val loss: 0.5588432383714336\n",
      "Val loss: 0.5584223746961953\n",
      "Val loss: 0.5585283881800186\n",
      "Val loss: 0.5582134062179802\n",
      "Val loss: 0.5579482373904507\n",
      "Val loss: 0.5581133344095626\n",
      "Val loss: 0.5579502642292031\n",
      "Val loss: 0.5579817669130946\n",
      "Val loss: 0.5575054575865334\n",
      "Val loss: 0.5573297336797141\n",
      "Val loss: 0.5575387056441033\n",
      "Val loss: 0.5575674138557417\n",
      "Val loss: 0.5576241803837234\n",
      "Val loss: 0.5575889176143004\n",
      "Val loss: 0.5573227731235922\n",
      "Val loss: 0.5575638267839627\n",
      "Val loss: 0.55768750103052\n",
      "Val loss: 0.5576889521017641\n",
      "Val loss: 0.5577265258260101\n",
      "Val loss: 0.5582110619019411\n",
      "Val loss: 0.558072899187368\n",
      "Val loss: 0.5579310170793814\n",
      "Val loss: 0.5583052731441617\n",
      "Val loss: 0.5580951829291952\n",
      "Val loss: 0.5579962271429201\n",
      "Val loss: 0.5578949614260733\n",
      "Val loss: 0.5575777195700992\n",
      "Val loss: 0.5575089094700751\n",
      "Val loss: 0.5573226190446054\n",
      "Val loss: 0.557457789534428\n",
      "Val loss: 0.5575027707896938\n",
      "Val loss: 0.5575821352772721\n",
      "Val loss: 0.5575435540988936\n",
      "Val loss: 0.5577399922380967\n",
      "Val loss: 0.5575977258046745\n",
      "Val loss: 0.5577282646674154\n",
      "Val loss: 0.557846343017196\n",
      "Val loss: 0.5578574376822732\n",
      "Val loss: 0.5579724444283379\n",
      "Val loss: 0.5582400087721161\n",
      "Val loss: 0.5584100287481649\n",
      "Val loss: 0.5586863183622877\n",
      "Val loss: 0.5587144819261197\n",
      "Val loss: 0.5587355981735883\n",
      "Val loss: 0.5588491295870298\n",
      "Val loss: 0.5589895744486719\n",
      "Val loss: 0.5588786361345357\n",
      "\n",
      "starting Epoch 27\n",
      "Training...\n",
      "Train loss: 0.5962421423510501\n",
      "Train loss: 0.5799875763746408\n",
      "Train loss: 0.5812022398083897\n",
      "Train loss: 0.5805725783486909\n",
      "Train loss: 0.5770119543027397\n",
      "Train loss: 0.5752714183651099\n",
      "Train loss: 0.5772973091053448\n",
      "Train loss: 0.5810449908739366\n",
      "Train loss: 0.5813453062619577\n",
      "Train loss: 0.5826438544084079\n",
      "Train loss: 0.5805799362321967\n",
      "Train loss: 0.5814278604595232\n",
      "Train loss: 0.580420884271386\n",
      "Train loss: 0.5802181782901928\n",
      "Train loss: 0.5804565644583176\n",
      "Train loss: 0.5806938533125252\n",
      "Train loss: 0.5797643819741444\n",
      "Train loss: 0.5800249186398924\n",
      "Train loss: 0.5801490895980895\n",
      "Train loss: 0.5805040757757679\n",
      "Train loss: 0.5810922276859921\n",
      "Train loss: 0.5814826863909094\n",
      "Train loss: 0.5820792679043897\n",
      "Train loss: 0.5824475056318749\n",
      "Train loss: 0.5823908053443044\n",
      "Train loss: 0.5812996360607919\n",
      "Train loss: 0.5808118462894313\n",
      "Train loss: 0.5804342157299062\n",
      "Train loss: 0.5794003479736045\n",
      "Train loss: 0.5799596341006545\n",
      "Train loss: 0.5800334534371612\n",
      "Train loss: 0.5799623478279801\n",
      "Train loss: 0.5797775529313702\n",
      "Train loss: 0.5800636479622837\n",
      "Train loss: 0.5801272600761299\n",
      "Train loss: 0.5804224132413824\n",
      "Train loss: 0.580392343507568\n",
      "Train loss: 0.5803171855658882\n",
      "Train loss: 0.5802273948094658\n",
      "Train loss: 0.5803795043980523\n",
      "Train loss: 0.5804094199514215\n",
      "Train loss: 0.5799606860891713\n",
      "Train loss: 0.5798345292128007\n",
      "Train loss: 0.5797103740724144\n",
      "Train loss: 0.5797072194839876\n",
      "Train loss: 0.579926524834223\n",
      "Train loss: 0.5798650939370623\n",
      "Train loss: 0.579760679953787\n",
      "Train loss: 0.5796143486563105\n",
      "Train loss: 0.5796837636956701\n",
      "Train loss: 0.5794114950419175\n",
      "Train loss: 0.5791389021997387\n",
      "Train loss: 0.5788494791198835\n",
      "Train loss: 0.5784683141639876\n",
      "Train loss: 0.5784440236649153\n",
      "Train loss: 0.5787759109519229\n",
      "Train loss: 0.5787830217246741\n",
      "Train loss: 0.5788443398640215\n",
      "Train loss: 0.579206995027767\n",
      "Train loss: 0.5793036123033958\n",
      "Train loss: 0.5792340288355861\n",
      "Train loss: 0.5792751247551866\n",
      "Train loss: 0.5793571604727942\n",
      "Train loss: 0.5795252897759916\n",
      "Train loss: 0.5798710648420685\n",
      "Train loss: 0.579700959483451\n",
      "Train loss: 0.5796425524845864\n",
      "Train loss: 0.5796024318211564\n",
      "Train loss: 0.5796279484504716\n",
      "Train loss: 0.5797628054796073\n",
      "Train loss: 0.5798456274639813\n",
      "Train loss: 0.5797773559446381\n",
      "Train loss: 0.5797739480357206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5798235406659599\n",
      "Train loss: 0.5797337224437364\n",
      "Train loss: 0.5796508198117174\n",
      "Train loss: 0.5795414265175002\n",
      "Train loss: 0.5791137506749249\n",
      "Train loss: 0.5793839443364123\n",
      "Train loss: 0.5794039555569601\n",
      "Train loss: 0.5794019644793498\n",
      "Train loss: 0.5792477369090273\n",
      "Train loss: 0.5795711848375091\n",
      "Train loss: 0.5797819281135591\n",
      "Train loss: 0.5797954914919273\n",
      "Train loss: 0.5800217761419206\n",
      "Train loss: 0.5801739315317581\n",
      "Train loss: 0.5800964394504879\n",
      "Train loss: 0.5801284000939234\n",
      "Train loss: 0.5800056392414958\n",
      "Train loss: 0.5801197002555329\n",
      "Train loss: 0.5801795946637724\n",
      "Train loss: 0.5801575190280958\n",
      "Train loss: 0.5802058684280288\n",
      "Train loss: 0.5801194096126325\n",
      "Train loss: 0.5801279433479279\n",
      "Train loss: 0.5802839756964899\n",
      "Train loss: 0.5803955411990118\n",
      "Train loss: 0.5805111598372158\n",
      "Train loss: 0.580567306506747\n",
      "Train loss: 0.5805360107447854\n",
      "Train loss: 0.5802723906126719\n",
      "Train loss: 0.5802342603801812\n",
      "Train loss: 0.5801612685339104\n",
      "Train loss: 0.5799577821005294\n",
      "Train loss: 0.5798090693803258\n",
      "Train loss: 0.5797030225904705\n",
      "Train loss: 0.5794790473289101\n",
      "Train loss: 0.57957623950272\n",
      "Train loss: 0.5795008384975426\n",
      "Train loss: 0.5794258339950876\n",
      "Train loss: 0.579374847759781\n",
      "Train loss: 0.5791656306065411\n",
      "Train loss: 0.5790039134187728\n",
      "Train loss: 0.5789028345475564\n",
      "Train loss: 0.5787699531511584\n",
      "Train loss: 0.5789396747048465\n",
      "Train loss: 0.579016483906418\n",
      "Train loss: 0.5789780041509037\n",
      "Train loss: 0.5789809671974818\n",
      "Train loss: 0.5791083936998794\n",
      "Train loss: 0.5792418890260389\n",
      "Train loss: 0.5791628354082461\n",
      "Train loss: 0.5791620359323639\n",
      "Train loss: 0.5791143944331196\n",
      "Train loss: 0.5789864922667742\n",
      "Train loss: 0.5790490896601337\n",
      "Train loss: 0.5789896663672852\n",
      "Train loss: 0.5789503321577385\n",
      "Train loss: 0.579098220030222\n",
      "Train loss: 0.5790850289703644\n",
      "Train loss: 0.5790893512948438\n",
      "Train loss: 0.5791485616702611\n",
      "Train loss: 0.5791644735116486\n",
      "Train loss: 0.5792119998033509\n",
      "Train loss: 0.5791090227699139\n",
      "Train loss: 0.5790229787265962\n",
      "Train loss: 0.5790594325803595\n",
      "Train loss: 0.5789859854797529\n",
      "Train loss: 0.5789481530171626\n",
      "Train loss: 0.5789831326907727\n",
      "Train loss: 0.5790842934555888\n",
      "Train loss: 0.5790862088521989\n",
      "Train loss: 0.5790265502926375\n",
      "Train loss: 0.5791797006323487\n",
      "Train loss: 0.5792165132326882\n",
      "Train loss: 0.5793120127910252\n",
      "Train loss: 0.5792487667859023\n",
      "Train loss: 0.5793572307813484\n",
      "Train loss: 0.5791667229177635\n",
      "Train loss: 0.5791951471525694\n",
      "Train loss: 0.5790872281713288\n",
      "Train loss: 0.5789960579530754\n",
      "Train loss: 0.5789324836360824\n",
      "Train loss: 0.5789329714258089\n",
      "Train loss: 0.5789068537707817\n",
      "Train loss: 0.5787819568690846\n",
      "Train loss: 0.5786852807646199\n",
      "Train loss: 0.5786671245405606\n",
      "Train loss: 0.5786022163622452\n",
      "Train loss: 0.5785243369456069\n",
      "Train loss: 0.5785322941243924\n",
      "Train loss: 0.5784722288059435\n",
      "Train loss: 0.578494676150502\n",
      "Train loss: 0.5785261568119036\n",
      "Train loss: 0.5784100134191689\n",
      "Train loss: 0.5784421560560097\n",
      "Train loss: 0.578424434615374\n",
      "Train loss: 0.5784166544790486\n",
      "Train loss: 0.5783986749719893\n",
      "Train loss: 0.5783826321337299\n",
      "Train loss: 0.5783559534656478\n",
      "Train loss: 0.578381127508502\n",
      "Train loss: 0.5784601875936618\n",
      "Train loss: 0.5784386141567988\n",
      "Train loss: 0.5783473506980608\n",
      "Train loss: 0.5784423124844087\n",
      "Train loss: 0.5784881427553337\n",
      "Train loss: 0.5785736048244904\n",
      "Train loss: 0.5786032327912721\n",
      "Train loss: 0.5784034603585456\n",
      "Train loss: 0.5784482374381941\n",
      "Train loss: 0.5785116624086077\n",
      "Train loss: 0.5785751690534704\n",
      "Train loss: 0.5785456502089535\n",
      "Train loss: 0.5784356915668699\n",
      "Train loss: 0.5784562333011474\n",
      "Train loss: 0.578607090033569\n",
      "Train loss: 0.5787107407983632\n",
      "Train loss: 0.5787638435319211\n",
      "Train loss: 0.5787059662073\n",
      "Train loss: 0.5786938745005424\n",
      "Train loss: 0.5786332725227481\n",
      "Train loss: 0.578651836504485\n",
      "Train loss: 0.5785072321355511\n",
      "Train loss: 0.5784834499458295\n",
      "Train loss: 0.5785218292488365\n",
      "Train loss: 0.5786479534329595\n",
      "Train loss: 0.5785799843937226\n",
      "Train loss: 0.5784366276971279\n",
      "Train loss: 0.5783870161528847\n",
      "Train loss: 0.5784736893191199\n",
      "Train loss: 0.5785698120113546\n",
      "Train loss: 0.5785854508126299\n",
      "Train loss: 0.5785211174407799\n",
      "Train loss: 0.5785551620834978\n",
      "Train loss: 0.578518358873094\n",
      "Train loss: 0.5785762890653846\n",
      "Train loss: 0.5787008812365334\n",
      "Train loss: 0.578711636316552\n",
      "Train loss: 0.5786801376354641\n",
      "Train loss: 0.5786884390160452\n",
      "Train loss: 0.5787827799747908\n",
      "Train loss: 0.5786884520955943\n",
      "Train loss: 0.5786933532757215\n",
      "Train loss: 0.578709431960368\n",
      "Train loss: 0.5787106842647419\n",
      "Train loss: 0.5786941081702531\n",
      "Train loss: 0.5786372916792103\n",
      "Train loss: 0.5787176752827552\n",
      "Train loss: 0.5787247663966992\n",
      "Train loss: 0.578780836237527\n",
      "Train loss: 0.5788048194258917\n",
      "Train loss: 0.5788486204354405\n",
      "Train loss: 0.578745300609765\n",
      "Train loss: 0.5787293020010152\n",
      "Train loss: 0.5787583056276763\n",
      "Train loss: 0.5787231811608471\n",
      "Train loss: 0.5786659396171153\n",
      "Train loss: 0.5786378017765824\n",
      "Train loss: 0.5786199576670433\n",
      "Train loss: 0.5785787149117049\n",
      "Train loss: 0.5785219761774657\n",
      "Train loss: 0.5785297112578025\n",
      "Train loss: 0.5785882011147909\n",
      "Train loss: 0.5786808638103839\n",
      "Train loss: 0.5787225002149762\n",
      "Train loss: 0.5787131127802156\n",
      "Train loss: 0.5787411462402563\n",
      "Train loss: 0.5787487902363581\n",
      "Train loss: 0.5786774018711756\n",
      "Train loss: 0.5786860750575873\n",
      "Train loss: 0.5786956937366072\n",
      "Train loss: 0.5787180469316595\n",
      "Train loss: 0.5787542347495326\n",
      "Train loss: 0.5787314716200199\n",
      "Train loss: 0.5787096520586761\n",
      "Train loss: 0.5787497598946059\n",
      "Train loss: 0.578640601689568\n",
      "Train loss: 0.5786349482239664\n",
      "Train loss: 0.5787298199331934\n",
      "Train loss: 0.5787077140100751\n",
      "Train loss: 0.5786321902432963\n",
      "Train loss: 0.5786577669518931\n",
      "Train loss: 0.5786599113289388\n",
      "Train loss: 0.5786558417434174\n",
      "Train loss: 0.578678628498999\n",
      "Train loss: 0.5785646201924636\n",
      "Train loss: 0.5785945598672637\n",
      "Train loss: 0.5784977074826445\n",
      "Train loss: 0.578496849052658\n",
      "Train loss: 0.5783980863595832\n",
      "Train loss: 0.5783781942747283\n",
      "Train loss: 0.5782621585469193\n",
      "Train loss: 0.5782995307967447\n",
      "Train loss: 0.5782685229307191\n",
      "Train loss: 0.5782411758831365\n",
      "Train loss: 0.5782523969590475\n",
      "Train loss: 0.5781949694647757\n",
      "Train loss: 0.5782169667377055\n",
      "Train loss: 0.5782298929171272\n",
      "Train loss: 0.578246222428483\n",
      "Train loss: 0.5781853732542582\n",
      "Train loss: 0.5782501057217343\n",
      "Train loss: 0.5782438107600405\n",
      "Train loss: 0.5781988127278254\n",
      "Train loss: 0.5782355748266278\n",
      "Train loss: 0.5782393066547268\n",
      "Train loss: 0.5782365547241031\n",
      "Train loss: 0.5782373669765362\n",
      "Train loss: 0.5781665209837922\n",
      "Train loss: 0.5781280577373031\n",
      "Train loss: 0.5780874734517647\n",
      "Train loss: 0.5780330368447711\n",
      "Train loss: 0.5780457658900736\n",
      "Train loss: 0.5780484278128267\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6948955655097961\n",
      "Val loss: 0.6197459962632921\n",
      "Val loss: 0.5938663844551358\n",
      "Val loss: 0.5922115084372068\n",
      "Val loss: 0.5918123212953409\n",
      "Val loss: 0.5957357030490349\n",
      "Val loss: 0.5894573760383269\n",
      "Val loss: 0.5836947705501165\n",
      "Val loss: 0.579094682904807\n",
      "Val loss: 0.5795243570999223\n",
      "Val loss: 0.5801281581322352\n",
      "Val loss: 0.5793058998503927\n",
      "Val loss: 0.5787438503466547\n",
      "Val loss: 0.5782945592334305\n",
      "Val loss: 0.5772998135637593\n",
      "Val loss: 0.5754596217523648\n",
      "Val loss: 0.5743436490496\n",
      "Val loss: 0.5728578095355731\n",
      "Val loss: 0.5737200350837505\n",
      "Val loss: 0.571360732569839\n",
      "Val loss: 0.5722624287009239\n",
      "Val loss: 0.5708892812969488\n",
      "Val loss: 0.5699021680313244\n",
      "Val loss: 0.5696556207512607\n",
      "Val loss: 0.5697034236884886\n",
      "Val loss: 0.5686685722465663\n",
      "Val loss: 0.5683959564611093\n",
      "Val loss: 0.5673763453960419\n",
      "Val loss: 0.566412944967548\n",
      "Val loss: 0.5665630104157748\n",
      "Val loss: 0.5662474692254872\n",
      "Val loss: 0.5664334685172675\n",
      "Val loss: 0.5663689453063941\n",
      "Val loss: 0.5656338131286689\n",
      "Val loss: 0.565311498340519\n",
      "Val loss: 0.5652992069388235\n",
      "Val loss: 0.5636280995348225\n",
      "Val loss: 0.5636087542488462\n",
      "Val loss: 0.5643027741269967\n",
      "Val loss: 0.5635508864968266\n",
      "Val loss: 0.5638991439459371\n",
      "Val loss: 0.5634687283392729\n",
      "Val loss: 0.56280520154494\n",
      "Val loss: 0.5630035448019908\n",
      "Val loss: 0.5624800315126777\n",
      "Val loss: 0.5626381876427013\n",
      "Val loss: 0.5623331764060208\n",
      "Val loss: 0.5622908911944433\n",
      "Val loss: 0.5622940830519942\n",
      "Val loss: 0.5623254505506002\n",
      "Val loss: 0.5616802001797309\n",
      "Val loss: 0.5609142254448305\n",
      "Val loss: 0.5608501277412429\n",
      "Val loss: 0.5603091666911171\n",
      "Val loss: 0.5604306800957144\n",
      "Val loss: 0.5597738616141794\n",
      "Val loss: 0.5598839040793163\n",
      "Val loss: 0.5601416700439057\n",
      "Val loss: 0.5602026683132665\n",
      "Val loss: 0.5600649906839416\n",
      "Val loss: 0.5602542673678774\n",
      "Val loss: 0.5602068314660329\n",
      "Val loss: 0.5597722254171493\n",
      "Val loss: 0.5595453921530315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5593202918951894\n",
      "Val loss: 0.5592987902439837\n",
      "Val loss: 0.5591971969711567\n",
      "Val loss: 0.5588007660742003\n",
      "Val loss: 0.5598917010911676\n",
      "Val loss: 0.5597017476586011\n",
      "Val loss: 0.5590884165238525\n",
      "Val loss: 0.5590574785361383\n",
      "Val loss: 0.5586394307377575\n",
      "Val loss: 0.5588967939056355\n",
      "Val loss: 0.5588960794204059\n",
      "Val loss: 0.5584413926645445\n",
      "Val loss: 0.5585427104185025\n",
      "Val loss: 0.558294539892888\n",
      "Val loss: 0.5578768083135489\n",
      "Val loss: 0.5579596083415183\n",
      "Val loss: 0.5582278480771745\n",
      "Val loss: 0.557734364329457\n",
      "Val loss: 0.5580147027537443\n",
      "Val loss: 0.5576796229386387\n",
      "Val loss: 0.5574965223248275\n",
      "Val loss: 0.5576951962945622\n",
      "Val loss: 0.5574106041187515\n",
      "Val loss: 0.5573957641195327\n",
      "Val loss: 0.5569695390022553\n",
      "Val loss: 0.5568099930583766\n",
      "Val loss: 0.5569539761359471\n",
      "Val loss: 0.5569612519543675\n",
      "Val loss: 0.5570150875197403\n",
      "Val loss: 0.5569957900148973\n",
      "Val loss: 0.5567655435091332\n",
      "Val loss: 0.5569442301553077\n",
      "Val loss: 0.5570108857898672\n",
      "Val loss: 0.556988778472678\n",
      "Val loss: 0.5569449697549527\n",
      "Val loss: 0.5574548862620681\n",
      "Val loss: 0.5574329212189667\n",
      "Val loss: 0.5571973531677119\n",
      "Val loss: 0.5575064640448715\n",
      "Val loss: 0.557270120621187\n",
      "Val loss: 0.5571714161347797\n",
      "Val loss: 0.5571218426488974\n",
      "Val loss: 0.5568449821476633\n",
      "Val loss: 0.5568199941200759\n",
      "Val loss: 0.5566384642220595\n",
      "Val loss: 0.556755325616166\n",
      "Val loss: 0.5567453626798808\n",
      "Val loss: 0.5568374151727168\n",
      "Val loss: 0.5567691227328693\n",
      "Val loss: 0.5569999606098148\n",
      "Val loss: 0.5568181339664326\n",
      "Val loss: 0.5569453962108631\n",
      "Val loss: 0.5570468292252658\n",
      "Val loss: 0.5570631332834629\n",
      "Val loss: 0.5571787507345379\n",
      "Val loss: 0.5573977941961241\n",
      "Val loss: 0.55763262860625\n",
      "Val loss: 0.5578897697682843\n",
      "Val loss: 0.5580262031636719\n",
      "Val loss: 0.5580514559164186\n",
      "Val loss: 0.5582421210427315\n",
      "Val loss: 0.5583628556781429\n",
      "Val loss: 0.5582601607597965\n",
      "\n",
      "starting Epoch 28\n",
      "Training...\n",
      "Train loss: 0.5926471368262642\n",
      "Train loss: 0.589047351708779\n",
      "Train loss: 0.5802072816986149\n",
      "Train loss: 0.5796531668946713\n",
      "Train loss: 0.5780219381505792\n",
      "Train loss: 0.5804644061737702\n",
      "Train loss: 0.5795674977971496\n",
      "Train loss: 0.5780200772690323\n",
      "Train loss: 0.578919978614626\n",
      "Train loss: 0.576132195828548\n",
      "Train loss: 0.5746626384454231\n",
      "Train loss: 0.5761119108569173\n",
      "Train loss: 0.5767961341211695\n",
      "Train loss: 0.5775794476591131\n",
      "Train loss: 0.5776233661134905\n",
      "Train loss: 0.5793289678597525\n",
      "Train loss: 0.5792921254065184\n",
      "Train loss: 0.5788543876001098\n",
      "Train loss: 0.5785247907632259\n",
      "Train loss: 0.577884465754779\n",
      "Train loss: 0.5778635251635867\n",
      "Train loss: 0.5778797665326634\n",
      "Train loss: 0.5775149622384239\n",
      "Train loss: 0.577469805399411\n",
      "Train loss: 0.5786961217323142\n",
      "Train loss: 0.5791801682786445\n",
      "Train loss: 0.5788477722258205\n",
      "Train loss: 0.5796736945832991\n",
      "Train loss: 0.5799749697943014\n",
      "Train loss: 0.5797352279864488\n",
      "Train loss: 0.5797134258924278\n",
      "Train loss: 0.5794527371631168\n",
      "Train loss: 0.579388878640347\n",
      "Train loss: 0.5792470580523776\n",
      "Train loss: 0.578694862059087\n",
      "Train loss: 0.5788336095292644\n",
      "Train loss: 0.5790449572028907\n",
      "Train loss: 0.5793262157829697\n",
      "Train loss: 0.5796187626382047\n",
      "Train loss: 0.5793982127282736\n",
      "Train loss: 0.5795294451407897\n",
      "Train loss: 0.5792303861466295\n",
      "Train loss: 0.578989041594049\n",
      "Train loss: 0.5792434642068084\n",
      "Train loss: 0.5795601726373921\n",
      "Train loss: 0.579413181549836\n",
      "Train loss: 0.5793396560465311\n",
      "Train loss: 0.5791949121062027\n",
      "Train loss: 0.5790520471742375\n",
      "Train loss: 0.5790176507052001\n",
      "Train loss: 0.5793569301190157\n",
      "Train loss: 0.5792774558354158\n",
      "Train loss: 0.5793227017826355\n",
      "Train loss: 0.5795801123664598\n",
      "Train loss: 0.579026358549545\n",
      "Train loss: 0.5793831767888874\n",
      "Train loss: 0.5796877165914524\n",
      "Train loss: 0.5800214195117753\n",
      "Train loss: 0.5798903152384527\n",
      "Train loss: 0.5798320435950715\n",
      "Train loss: 0.5800641330418966\n",
      "Train loss: 0.5800171334287445\n",
      "Train loss: 0.5801157544206493\n",
      "Train loss: 0.5801641409801636\n",
      "Train loss: 0.5803170291344141\n",
      "Train loss: 0.5804944082450649\n",
      "Train loss: 0.580663994700273\n",
      "Train loss: 0.5804492357331571\n",
      "Train loss: 0.5804722252274181\n",
      "Train loss: 0.5801216488953059\n",
      "Train loss: 0.5798938696110559\n",
      "Train loss: 0.5799326305591538\n",
      "Train loss: 0.5801675984351254\n",
      "Train loss: 0.5799478944126539\n",
      "Train loss: 0.5802331355271775\n",
      "Train loss: 0.5801864117482055\n",
      "Train loss: 0.5800598474041528\n",
      "Train loss: 0.5801261344194565\n",
      "Train loss: 0.5802475055461447\n",
      "Train loss: 0.5800847057404557\n",
      "Train loss: 0.5800353458002808\n",
      "Train loss: 0.5800782288080893\n",
      "Train loss: 0.5801834025750899\n",
      "Train loss: 0.5805532165834633\n",
      "Train loss: 0.5805656921168928\n",
      "Train loss: 0.5803891612458465\n",
      "Train loss: 0.580561900786657\n",
      "Train loss: 0.5805527196308643\n",
      "Train loss: 0.5803781302709671\n",
      "Train loss: 0.5802922328688689\n",
      "Train loss: 0.5802762354875148\n",
      "Train loss: 0.5803512765085522\n",
      "Train loss: 0.5804527866994263\n",
      "Train loss: 0.580368631630898\n",
      "Train loss: 0.580250785217717\n",
      "Train loss: 0.5802321542110711\n",
      "Train loss: 0.5803611353846664\n",
      "Train loss: 0.5802096751437253\n",
      "Train loss: 0.5801181304539136\n",
      "Train loss: 0.5799846404519542\n",
      "Train loss: 0.5801086943447147\n",
      "Train loss: 0.5799995844214497\n",
      "Train loss: 0.5799988622425712\n",
      "Train loss: 0.5798118545809522\n",
      "Train loss: 0.5797910684770263\n",
      "Train loss: 0.5798023630139286\n",
      "Train loss: 0.5797519509677521\n",
      "Train loss: 0.5800550191862929\n",
      "Train loss: 0.5799702819113668\n",
      "Train loss: 0.5798831120600967\n",
      "Train loss: 0.5798019877877951\n",
      "Train loss: 0.5796246415219599\n",
      "Train loss: 0.5795899578669051\n",
      "Train loss: 0.5794158524695903\n",
      "Train loss: 0.5792841810721737\n",
      "Train loss: 0.5794467606283354\n",
      "Train loss: 0.5794464885908195\n",
      "Train loss: 0.579339903800252\n",
      "Train loss: 0.5793094293141174\n",
      "Train loss: 0.579408232484474\n",
      "Train loss: 0.5792249055650915\n",
      "Train loss: 0.5791308356020577\n",
      "Train loss: 0.5791173282194158\n",
      "Train loss: 0.5793349954835731\n",
      "Train loss: 0.5793627968379239\n",
      "Train loss: 0.5792756576465207\n",
      "Train loss: 0.5793748910077776\n",
      "Train loss: 0.5793107574388542\n",
      "Train loss: 0.5791782021591857\n",
      "Train loss: 0.5792124271851496\n",
      "Train loss: 0.5791427490478165\n",
      "Train loss: 0.5789906897184387\n",
      "Train loss: 0.5788759236252367\n",
      "Train loss: 0.5787841802592952\n",
      "Train loss: 0.5787765812259906\n",
      "Train loss: 0.5788151739208345\n",
      "Train loss: 0.5787520853978694\n",
      "Train loss: 0.5786633803473732\n",
      "Train loss: 0.5787162740224555\n",
      "Train loss: 0.5786464925364283\n",
      "Train loss: 0.578869405241882\n",
      "Train loss: 0.5788564769135852\n",
      "Train loss: 0.5789979356658671\n",
      "Train loss: 0.5789557674582032\n",
      "Train loss: 0.5788269135696225\n",
      "Train loss: 0.5788041413054967\n",
      "Train loss: 0.5788669902761602\n",
      "Train loss: 0.5788815624913721\n",
      "Train loss: 0.5788531836526192\n",
      "Train loss: 0.5788015369257556\n",
      "Train loss: 0.5787641909905087\n",
      "Train loss: 0.5788390453844802\n",
      "Train loss: 0.5787423091333807\n",
      "Train loss: 0.5787864063575152\n",
      "Train loss: 0.5787811429222078\n",
      "Train loss: 0.5787986135991119\n",
      "Train loss: 0.5786876498741419\n",
      "Train loss: 0.5786575546706919\n",
      "Train loss: 0.5787057026859799\n",
      "Train loss: 0.5785918076659188\n",
      "Train loss: 0.5785393058587098\n",
      "Train loss: 0.5784441882278052\n",
      "Train loss: 0.5784449910927345\n",
      "Train loss: 0.5782886988489901\n",
      "Train loss: 0.5783271920540073\n",
      "Train loss: 0.5783578104985769\n",
      "Train loss: 0.5783675417960065\n",
      "Train loss: 0.578305536817319\n",
      "Train loss: 0.5782035056321922\n",
      "Train loss: 0.5782630468123308\n",
      "Train loss: 0.5783023386714262\n",
      "Train loss: 0.5783532272767591\n",
      "Train loss: 0.5784255776680354\n",
      "Train loss: 0.5783693757626681\n",
      "Train loss: 0.5783903619482641\n",
      "Train loss: 0.5783107280223063\n",
      "Train loss: 0.5782925713261025\n",
      "Train loss: 0.5782315852133603\n",
      "Train loss: 0.5782853038442904\n",
      "Train loss: 0.5782635743981038\n",
      "Train loss: 0.5782795695268373\n",
      "Train loss: 0.5782599977147747\n",
      "Train loss: 0.5782365685409782\n",
      "Train loss: 0.5782122592719945\n",
      "Train loss: 0.5782126262723578\n",
      "Train loss: 0.5782828257589707\n",
      "Train loss: 0.5781833210406567\n",
      "Train loss: 0.5782046051981856\n",
      "Train loss: 0.5781520880082512\n",
      "Train loss: 0.5780817916767319\n",
      "Train loss: 0.5780823251813393\n",
      "Train loss: 0.5780245022759458\n",
      "Train loss: 0.577984483509553\n",
      "Train loss: 0.577932027980328\n",
      "Train loss: 0.5778412144209795\n",
      "Train loss: 0.5778957927415613\n",
      "Train loss: 0.5779188457298594\n",
      "Train loss: 0.5778542623115809\n",
      "Train loss: 0.5778838806369251\n",
      "Train loss: 0.5779025457626166\n",
      "Train loss: 0.5778734721660496\n",
      "Train loss: 0.5778701802562092\n",
      "Train loss: 0.5777349185476635\n",
      "Train loss: 0.5778027186720475\n",
      "Train loss: 0.5778081834127218\n",
      "Train loss: 0.5777285491112049\n",
      "Train loss: 0.577719562221881\n",
      "Train loss: 0.5777280362975457\n",
      "Train loss: 0.5776994424763123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5777772352308794\n",
      "Train loss: 0.5777604627959619\n",
      "Train loss: 0.5777159544847349\n",
      "Train loss: 0.5777382058530773\n",
      "Train loss: 0.5778638756311394\n",
      "Train loss: 0.5777910946205234\n",
      "Train loss: 0.5777944653336167\n",
      "Train loss: 0.5777245112266594\n",
      "Train loss: 0.5777972618711462\n",
      "Train loss: 0.5778359161729436\n",
      "Train loss: 0.5777861605546344\n",
      "Train loss: 0.5778913435811343\n",
      "Train loss: 0.5778712730508152\n",
      "Train loss: 0.5778281051940409\n",
      "Train loss: 0.5777201008639663\n",
      "Train loss: 0.5777366321652326\n",
      "Train loss: 0.5777669006162682\n",
      "Train loss: 0.5778038688143541\n",
      "Train loss: 0.5777515944916315\n",
      "Train loss: 0.5777398090704634\n",
      "Train loss: 0.5777348539678395\n",
      "Train loss: 0.5776953856958974\n",
      "Train loss: 0.5776359297947062\n",
      "Train loss: 0.5775896619384088\n",
      "Train loss: 0.5775375298313362\n",
      "Train loss: 0.577569854807463\n",
      "Train loss: 0.5775186070245968\n",
      "Train loss: 0.5775666993098009\n",
      "Train loss: 0.5775070906336705\n",
      "Train loss: 0.5775040700286471\n",
      "Train loss: 0.577521919216755\n",
      "Train loss: 0.5775759193839857\n",
      "Train loss: 0.5775466071571489\n",
      "Train loss: 0.5775270706698541\n",
      "Train loss: 0.5775587130983141\n",
      "Train loss: 0.5775581306706985\n",
      "Train loss: 0.5776036627216536\n",
      "Train loss: 0.577606079754112\n",
      "Train loss: 0.5775918508638511\n",
      "Train loss: 0.5776797700692047\n",
      "Train loss: 0.5776724806843483\n",
      "Train loss: 0.5777647233348986\n",
      "Train loss: 0.5777545388692616\n",
      "Train loss: 0.5777764359960728\n",
      "Train loss: 0.5777718088046658\n",
      "Train loss: 0.5777440859093622\n",
      "Train loss: 0.5777767651608317\n",
      "Train loss: 0.5777271645823953\n",
      "Train loss: 0.5777037858523979\n",
      "Train loss: 0.5776773027458529\n",
      "Train loss: 0.5776679926100637\n",
      "Train loss: 0.5776723596437107\n",
      "Train loss: 0.5776306280575083\n",
      "Train loss: 0.5776257894183774\n",
      "Train loss: 0.5776216603122577\n",
      "Train loss: 0.5776655954512858\n",
      "Train loss: 0.5777108079285791\n",
      "Train loss: 0.5777591826719701\n",
      "Train loss: 0.5776874053765197\n",
      "Train loss: 0.5777471024632388\n",
      "Train loss: 0.5777258007288818\n",
      "Train loss: 0.5777743282595735\n",
      "Train loss: 0.5777813049937723\n",
      "Train loss: 0.5777069860258957\n",
      "Train loss: 0.5776661602080924\n",
      "Train loss: 0.5776626088604491\n",
      "Train loss: 0.5777329263976139\n",
      "Train loss: 0.5777118645768493\n",
      "Train loss: 0.5776049852006662\n",
      "Train loss: 0.5776827183434732\n",
      "Train loss: 0.5776119727304012\n",
      "Train loss: 0.5776679422650012\n",
      "Train loss: 0.5777262708495581\n",
      "Train loss: 0.5777349022246224\n",
      "Train loss: 0.5776795753187477\n",
      "Train loss: 0.5776848402498563\n",
      "Train loss: 0.5776707632419925\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6938949674367905\n",
      "Val loss: 0.618327968650394\n",
      "Val loss: 0.5904263449566705\n",
      "Val loss: 0.5926126476965452\n",
      "Val loss: 0.5922548087934653\n",
      "Val loss: 0.5949838377278427\n",
      "Val loss: 0.5890805922886905\n",
      "Val loss: 0.5842265310959939\n",
      "Val loss: 0.578968917104331\n",
      "Val loss: 0.5788323946145116\n",
      "Val loss: 0.5790502291034769\n",
      "Val loss: 0.5778897308697135\n",
      "Val loss: 0.5776679809205234\n",
      "Val loss: 0.5773004859253981\n",
      "Val loss: 0.5761402382238491\n",
      "Val loss: 0.5746575710894186\n",
      "Val loss: 0.5732411163903418\n",
      "Val loss: 0.5726347568999516\n",
      "Val loss: 0.5736843249265183\n",
      "Val loss: 0.5716222961141606\n",
      "Val loss: 0.5719802525754158\n",
      "Val loss: 0.5708137176452427\n",
      "Val loss: 0.5695652355227554\n",
      "Val loss: 0.5689997217234444\n",
      "Val loss: 0.5686483032280399\n",
      "Val loss: 0.5670503844586454\n",
      "Val loss: 0.5666506930963316\n",
      "Val loss: 0.565688932971131\n",
      "Val loss: 0.565080487065845\n",
      "Val loss: 0.5653430403478994\n",
      "Val loss: 0.5651580811320961\n",
      "Val loss: 0.5650832289419834\n",
      "Val loss: 0.5648134688778621\n",
      "Val loss: 0.5643994991595929\n",
      "Val loss: 0.5641299797885719\n",
      "Val loss: 0.5641961157654917\n",
      "Val loss: 0.5626180769632692\n",
      "Val loss: 0.5625388920307159\n",
      "Val loss: 0.5633700801846907\n",
      "Val loss: 0.5627014979944757\n",
      "Val loss: 0.5631448227693053\n",
      "Val loss: 0.5627889684512855\n",
      "Val loss: 0.5623834783507284\n",
      "Val loss: 0.5624328003898603\n",
      "Val loss: 0.561987181992403\n",
      "Val loss: 0.5621912532758504\n",
      "Val loss: 0.5619600123574591\n",
      "Val loss: 0.5622107685859233\n",
      "Val loss: 0.5623319469025878\n",
      "Val loss: 0.5625408939329017\n",
      "Val loss: 0.5619345949156078\n",
      "Val loss: 0.561374287232469\n",
      "Val loss: 0.5611094781620936\n",
      "Val loss: 0.5607993843165472\n",
      "Val loss: 0.5609038850686846\n",
      "Val loss: 0.5604102466268779\n",
      "Val loss: 0.56044713963925\n",
      "Val loss: 0.5605990583508897\n",
      "Val loss: 0.5606581092286272\n",
      "Val loss: 0.5606157890330987\n",
      "Val loss: 0.5607827359712437\n",
      "Val loss: 0.5609289215799288\n",
      "Val loss: 0.560376073429539\n",
      "Val loss: 0.5600403734135404\n",
      "Val loss: 0.5598527997916127\n",
      "Val loss: 0.5599159744191677\n",
      "Val loss: 0.5597309315811374\n",
      "Val loss: 0.559301068561267\n",
      "Val loss: 0.5599695436309936\n",
      "Val loss: 0.5598050892182271\n",
      "Val loss: 0.5591681401776729\n",
      "Val loss: 0.5591182579237106\n",
      "Val loss: 0.558928788445153\n",
      "Val loss: 0.5592631816379423\n",
      "Val loss: 0.5593012548226086\n",
      "Val loss: 0.5590592393145397\n",
      "Val loss: 0.5592503554653376\n",
      "Val loss: 0.559017618088612\n",
      "Val loss: 0.558600942192949\n",
      "Val loss: 0.558762158218183\n",
      "Val loss: 0.5590360371193083\n",
      "Val loss: 0.5585870403240829\n",
      "Val loss: 0.5586703818489388\n",
      "Val loss: 0.558413832952982\n",
      "Val loss: 0.5581159791997019\n",
      "Val loss: 0.5582525978138396\n",
      "Val loss: 0.5580969152774679\n",
      "Val loss: 0.55814853826525\n",
      "Val loss: 0.5576718829505078\n",
      "Val loss: 0.5574944997019651\n",
      "Val loss: 0.5576647753101088\n",
      "Val loss: 0.5577752979102997\n",
      "Val loss: 0.5578267315466856\n",
      "Val loss: 0.5576952488056378\n",
      "Val loss: 0.5574320181889876\n",
      "Val loss: 0.5577195511357023\n",
      "Val loss: 0.5578619758329115\n",
      "Val loss: 0.5579158047591251\n",
      "Val loss: 0.557928350471292\n",
      "Val loss: 0.5584225503500095\n",
      "Val loss: 0.5583206357818747\n",
      "Val loss: 0.5581702363397377\n",
      "Val loss: 0.5585206938972733\n",
      "Val loss: 0.5583038474783043\n",
      "Val loss: 0.5582373973749976\n",
      "Val loss: 0.5581579967798042\n",
      "Val loss: 0.5578225791342696\n",
      "Val loss: 0.5577966568089593\n",
      "Val loss: 0.557596086853129\n",
      "Val loss: 0.55771824125818\n",
      "Val loss: 0.5577742249741882\n",
      "Val loss: 0.5578148047271483\n",
      "Val loss: 0.5577517282349843\n",
      "Val loss: 0.5579353872103934\n",
      "Val loss: 0.5577768165786923\n",
      "Val loss: 0.5579326902749419\n",
      "Val loss: 0.5580684431917864\n",
      "Val loss: 0.5581056159400778\n",
      "Val loss: 0.5582336899307039\n",
      "Val loss: 0.5585204057880554\n",
      "Val loss: 0.5586775730283845\n",
      "Val loss: 0.558934053225666\n",
      "Val loss: 0.5589145299180711\n",
      "Val loss: 0.5589024688873999\n",
      "Val loss: 0.55902853460075\n",
      "Val loss: 0.5591340309959527\n",
      "Val loss: 0.5590337646966477\n",
      "\n",
      "starting Epoch 29\n",
      "Training...\n",
      "Train loss: 0.6070814320915624\n",
      "Train loss: 0.5948239213381058\n",
      "Train loss: 0.5930371208716247\n",
      "Train loss: 0.5929759721967238\n",
      "Train loss: 0.5890840057170752\n",
      "Train loss: 0.5886754716644768\n",
      "Train loss: 0.587561697839833\n",
      "Train loss: 0.5857847004941424\n",
      "Train loss: 0.5833561548640608\n",
      "Train loss: 0.5803469494359577\n",
      "Train loss: 0.5807872451331517\n",
      "Train loss: 0.5813463718073139\n",
      "Train loss: 0.5809869854836851\n",
      "Train loss: 0.5801145678993621\n",
      "Train loss: 0.5802460627811011\n",
      "Train loss: 0.5799520596815128\n",
      "Train loss: 0.5800372110707218\n",
      "Train loss: 0.5798209072985689\n",
      "Train loss: 0.5795947751300631\n",
      "Train loss: 0.5792861094600276\n",
      "Train loss: 0.579456018732954\n",
      "Train loss: 0.58006756520217\n",
      "Train loss: 0.5794393470214603\n",
      "Train loss: 0.5791453963655022\n",
      "Train loss: 0.579082144764\n",
      "Train loss: 0.5794940549514197\n",
      "Train loss: 0.5795279193897637\n",
      "Train loss: 0.579487044397535\n",
      "Train loss: 0.5793531478377821\n",
      "Train loss: 0.5798750748618418\n",
      "Train loss: 0.5795789888197848\n",
      "Train loss: 0.580547141403846\n",
      "Train loss: 0.5804955934447114\n",
      "Train loss: 0.5805108554672247\n",
      "Train loss: 0.5805602051072537\n",
      "Train loss: 0.5802325308322906\n",
      "Train loss: 0.5802258298264789\n",
      "Train loss: 0.5795745366254186\n",
      "Train loss: 0.5795608806365261\n",
      "Train loss: 0.5796233210456238\n",
      "Train loss: 0.5797389070702414\n",
      "Train loss: 0.5799902424269552\n",
      "Train loss: 0.5799432645993128\n",
      "Train loss: 0.5795721128938954\n",
      "Train loss: 0.579598038526743\n",
      "Train loss: 0.5798420436169559\n",
      "Train loss: 0.5797717459222125\n",
      "Train loss: 0.5802833250528581\n",
      "Train loss: 0.5804093330033101\n",
      "Train loss: 0.5802446090661967\n",
      "Train loss: 0.5805226463798919\n",
      "Train loss: 0.5803245782450601\n",
      "Train loss: 0.5802060781614864\n",
      "Train loss: 0.5804785417118373\n",
      "Train loss: 0.5804954080065344\n",
      "Train loss: 0.580099919765948\n",
      "Train loss: 0.5802968065397482\n",
      "Train loss: 0.580297641269702\n",
      "Train loss: 0.5802174286128506\n",
      "Train loss: 0.5802597879021242\n",
      "Train loss: 0.5801953894374212\n",
      "Train loss: 0.5799489078548669\n",
      "Train loss: 0.5798287182407212\n",
      "Train loss: 0.5796688973810451\n",
      "Train loss: 0.5793688566387388\n",
      "Train loss: 0.5796378340750051\n",
      "Train loss: 0.5797120251256731\n",
      "Train loss: 0.5799264022758377\n",
      "Train loss: 0.5798640042608246\n",
      "Train loss: 0.5798955794272037\n",
      "Train loss: 0.5800960326043546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5799624448521755\n",
      "Train loss: 0.5803112674037262\n",
      "Train loss: 0.580338969554669\n",
      "Train loss: 0.5804321116968503\n",
      "Train loss: 0.5802162462076598\n",
      "Train loss: 0.5802443046280128\n",
      "Train loss: 0.5803153086274461\n",
      "Train loss: 0.5802343199908394\n",
      "Train loss: 0.5802854161809727\n",
      "Train loss: 0.5800883345687883\n",
      "Train loss: 0.5799260571498999\n",
      "Train loss: 0.5800181453202129\n",
      "Train loss: 0.5798750989596144\n",
      "Train loss: 0.579990779392294\n",
      "Train loss: 0.5801425716639813\n",
      "Train loss: 0.5803634587754595\n",
      "Train loss: 0.5802646430125624\n",
      "Train loss: 0.5802298717246824\n",
      "Train loss: 0.5803337086234376\n",
      "Train loss: 0.5802130049863577\n",
      "Train loss: 0.5803576107417974\n",
      "Train loss: 0.5804123038595373\n",
      "Train loss: 0.5803792916789469\n",
      "Train loss: 0.5803845920739769\n",
      "Train loss: 0.5804717252812328\n",
      "Train loss: 0.5804076271612906\n",
      "Train loss: 0.5805867422959706\n",
      "Train loss: 0.5805558967674664\n",
      "Train loss: 0.5804675799002941\n",
      "Train loss: 0.5803466082032569\n",
      "Train loss: 0.580327415624163\n",
      "Train loss: 0.5803798695048294\n",
      "Train loss: 0.5803856050354635\n",
      "Train loss: 0.5802914151951153\n",
      "Train loss: 0.5801785392910875\n",
      "Train loss: 0.580056462571018\n",
      "Train loss: 0.5800721186794248\n",
      "Train loss: 0.579951852915647\n",
      "Train loss: 0.5798490795697772\n",
      "Train loss: 0.5797200447780262\n",
      "Train loss: 0.5797874933244289\n",
      "Train loss: 0.5798096741100707\n",
      "Train loss: 0.5799357427113304\n",
      "Train loss: 0.5798033469135008\n",
      "Train loss: 0.5796600454914617\n",
      "Train loss: 0.5797391083949954\n",
      "Train loss: 0.5798416003149007\n",
      "Train loss: 0.5797705357191992\n",
      "Train loss: 0.5796571860168317\n",
      "Train loss: 0.5798428660830766\n",
      "Train loss: 0.5798157835749633\n",
      "Train loss: 0.5797563377751137\n",
      "Train loss: 0.5797336250964361\n",
      "Train loss: 0.5796490701235214\n",
      "Train loss: 0.5797266809062004\n",
      "Train loss: 0.5797342886108731\n",
      "Train loss: 0.5797675407668863\n",
      "Train loss: 0.5795471766005195\n",
      "Train loss: 0.5795615474367747\n",
      "Train loss: 0.5795380930014811\n",
      "Train loss: 0.579546070552857\n",
      "Train loss: 0.5795115870432371\n",
      "Train loss: 0.579603798211889\n",
      "Train loss: 0.579713295757351\n",
      "Train loss: 0.5797574155884835\n",
      "Train loss: 0.579883286773658\n",
      "Train loss: 0.5798038413728567\n",
      "Train loss: 0.579835448152687\n",
      "Train loss: 0.5799844010935038\n",
      "Train loss: 0.5799755194416364\n",
      "Train loss: 0.5798220117250015\n",
      "Train loss: 0.5800094244445942\n",
      "Train loss: 0.5799370107861102\n",
      "Train loss: 0.5799922112371643\n",
      "Train loss: 0.5798447762972364\n",
      "Train loss: 0.5798164559854623\n",
      "Train loss: 0.579871531212076\n",
      "Train loss: 0.5798528684930299\n",
      "Train loss: 0.5797767661320603\n",
      "Train loss: 0.5796543919179328\n",
      "Train loss: 0.579537777945414\n",
      "Train loss: 0.5795581829278988\n",
      "Train loss: 0.5795350666749242\n",
      "Train loss: 0.5794734610169501\n",
      "Train loss: 0.5794377215170792\n",
      "Train loss: 0.5795399722930693\n",
      "Train loss: 0.579624955631125\n",
      "Train loss: 0.5796902150000068\n",
      "Train loss: 0.5797296331036125\n",
      "Train loss: 0.5797059039645182\n",
      "Train loss: 0.5796076033854418\n",
      "Train loss: 0.579690934946728\n",
      "Train loss: 0.5796532263821965\n",
      "Train loss: 0.5797920444221993\n",
      "Train loss: 0.5796958660192251\n",
      "Train loss: 0.579647368248011\n",
      "Train loss: 0.5796649731245379\n",
      "Train loss: 0.5796828590825169\n",
      "Train loss: 0.5797307769334747\n",
      "Train loss: 0.5796476030653072\n",
      "Train loss: 0.579600391605629\n",
      "Train loss: 0.5796460661801275\n",
      "Train loss: 0.5795089861259751\n",
      "Train loss: 0.5795246159716926\n",
      "Train loss: 0.5796191416517785\n",
      "Train loss: 0.5796545757500254\n",
      "Train loss: 0.5796651543024116\n",
      "Train loss: 0.5795865218693999\n",
      "Train loss: 0.579632571577264\n",
      "Train loss: 0.5795357692106615\n",
      "Train loss: 0.5795650409453188\n",
      "Train loss: 0.5796342961251915\n",
      "Train loss: 0.5795650860478483\n",
      "Train loss: 0.5795163487173087\n",
      "Train loss: 0.5795612574408984\n",
      "Train loss: 0.5794019649200307\n",
      "Train loss: 0.57960086184063\n",
      "Train loss: 0.5795226928086217\n",
      "Train loss: 0.5795577839742556\n",
      "Train loss: 0.5796166602292901\n",
      "Train loss: 0.5796300163024352\n",
      "Train loss: 0.579612550177269\n",
      "Train loss: 0.5796241563031526\n",
      "Train loss: 0.5796920753543454\n",
      "Train loss: 0.5797075475697981\n",
      "Train loss: 0.5797610579710619\n",
      "Train loss: 0.5798173963602037\n",
      "Train loss: 0.5797148383975957\n",
      "Train loss: 0.5796772690453091\n",
      "Train loss: 0.5797053787224208\n",
      "Train loss: 0.5797398854125458\n",
      "Train loss: 0.5797461973942393\n",
      "Train loss: 0.5795906786899352\n",
      "Train loss: 0.5795334599226909\n",
      "Train loss: 0.5794094864619996\n",
      "Train loss: 0.579461999073729\n",
      "Train loss: 0.5794117516208996\n",
      "Train loss: 0.579373539207255\n",
      "Train loss: 0.5793595481455454\n",
      "Train loss: 0.5793609817281213\n",
      "Train loss: 0.5792931774563035\n",
      "Train loss: 0.5792281564117794\n",
      "Train loss: 0.579270962480352\n",
      "Train loss: 0.5792302705501784\n",
      "Train loss: 0.5791382194186394\n",
      "Train loss: 0.5791388971653367\n",
      "Train loss: 0.5790802467769098\n",
      "Train loss: 0.5791459325637195\n",
      "Train loss: 0.579155869803989\n",
      "Train loss: 0.5791537142779175\n",
      "Train loss: 0.5790622725589342\n",
      "Train loss: 0.5791224680839299\n",
      "Train loss: 0.5792107296977412\n",
      "Train loss: 0.5792670795045235\n",
      "Train loss: 0.5791364019556524\n",
      "Train loss: 0.5791664574673638\n",
      "Train loss: 0.5790579661540646\n",
      "Train loss: 0.5790600692387569\n",
      "Train loss: 0.5790635420851511\n",
      "Train loss: 0.5790573846573798\n",
      "Train loss: 0.5790450550610969\n",
      "Train loss: 0.5790517307077515\n",
      "Train loss: 0.5789640249350458\n",
      "Train loss: 0.5789142747170419\n",
      "Train loss: 0.5787800553871125\n",
      "Train loss: 0.5788261579307581\n",
      "Train loss: 0.5789236203477923\n",
      "Train loss: 0.5789199706075779\n",
      "Train loss: 0.5790091981910671\n",
      "Train loss: 0.5790285952149598\n",
      "Train loss: 0.5790641734141362\n",
      "Train loss: 0.5791326184948915\n",
      "Train loss: 0.579214400054246\n",
      "Train loss: 0.5791333212818898\n",
      "Train loss: 0.5790937053621675\n",
      "Train loss: 0.5791291751934464\n",
      "Train loss: 0.5791147801695296\n",
      "Train loss: 0.5791104746214427\n",
      "Train loss: 0.5791079024358567\n",
      "Train loss: 0.5790358177514445\n",
      "Train loss: 0.5790773799824417\n",
      "Train loss: 0.5790331203797562\n",
      "Train loss: 0.5790731533819072\n",
      "Train loss: 0.5789927757938929\n",
      "Train loss: 0.5789901737962809\n",
      "Train loss: 0.5789727605277635\n",
      "Train loss: 0.5790341045302368\n",
      "Train loss: 0.5790481073190218\n",
      "Train loss: 0.5789615560446961\n",
      "Train loss: 0.5788916303442685\n",
      "Train loss: 0.5788742656026503\n",
      "Train loss: 0.578944993491725\n",
      "Train loss: 0.5789952722264637\n",
      "Train loss: 0.5790009360298568\n",
      "Train loss: 0.5789988633328574\n",
      "Train loss: 0.5789983433090942\n",
      "Train loss: 0.5789959346220372\n",
      "Train loss: 0.578927099654878\n",
      "Train loss: 0.5789335272360299\n",
      "Train loss: 0.5789733876885469\n",
      "Train loss: 0.5789895639259119\n",
      "Train loss: 0.5789970965487199\n",
      "Train loss: 0.5789475086059943\n",
      "Train loss: 0.5789391894972656\n",
      "Train loss: 0.5788726500409478\n",
      "Train loss: 0.5788658193236418\n",
      "Train loss: 0.578890847380287\n",
      "Train loss: 0.5788580615004666\n",
      "Train loss: 0.578795892944462\n",
      "Train loss: 0.5788279224659841\n",
      "Train loss: 0.5788315512605922\n",
      "Train loss: 0.5787341084438576\n",
      "Train loss: 0.5786943025919812\n",
      "Train loss: 0.5787078514237596\n",
      "Train loss: 0.5787463271288548\n",
      "\n",
      "Validating...\n",
      "Val loss: 0.6931923553347588\n",
      "Val loss: 0.6207670668760935\n",
      "Val loss: 0.5936029659850257\n",
      "Val loss: 0.593705709043302\n",
      "Val loss: 0.5936914719641209\n",
      "Val loss: 0.5957216671828566\n",
      "Val loss: 0.5896341967232087\n",
      "Val loss: 0.5846716066201528\n",
      "Val loss: 0.5795116959647699\n",
      "Val loss: 0.579556403719649\n",
      "Val loss: 0.5805047689764588\n",
      "Val loss: 0.5795250469345158\n",
      "Val loss: 0.5795620041899383\n",
      "Val loss: 0.5791241345198258\n",
      "Val loss: 0.5781563858728151\n",
      "Val loss: 0.5765272846704796\n",
      "Val loss: 0.5752831803900855\n",
      "Val loss: 0.5745389361060067\n",
      "Val loss: 0.5758333206176758\n",
      "Val loss: 0.5735584762361314\n",
      "Val loss: 0.5737904797379787\n",
      "Val loss: 0.5726273046174181\n",
      "Val loss: 0.5715442464539879\n",
      "Val loss: 0.5709839460729551\n",
      "Val loss: 0.5707864811824214\n",
      "Val loss: 0.5695865955925727\n",
      "Val loss: 0.5693404810197318\n",
      "Val loss: 0.5682056411135968\n",
      "Val loss: 0.567578187212348\n",
      "Val loss: 0.5677306942091692\n",
      "Val loss: 0.5676117916772891\n",
      "Val loss: 0.5675233038341474\n",
      "Val loss: 0.5672212637415747\n",
      "Val loss: 0.5667682439970547\n",
      "Val loss: 0.5665591360851266\n",
      "Val loss: 0.5664383953843037\n",
      "Val loss: 0.5648693764663261\n",
      "Val loss: 0.5646999164548501\n",
      "Val loss: 0.5652948201624388\n",
      "Val loss: 0.5647174402996523\n",
      "Val loss: 0.565016951777187\n",
      "Val loss: 0.5646057967363932\n",
      "Val loss: 0.5641101414912215\n",
      "Val loss: 0.5641214452922072\n",
      "Val loss: 0.5634674299508333\n",
      "Val loss: 0.5637887216551335\n",
      "Val loss: 0.5635567439926995\n",
      "Val loss: 0.5636736986277991\n",
      "Val loss: 0.5638719348878157\n",
      "Val loss: 0.564051329730505\n",
      "Val loss: 0.5634653747785748\n",
      "Val loss: 0.5629212401778542\n",
      "Val loss: 0.5626882972816626\n",
      "Val loss: 0.5623512796531379\n",
      "Val loss: 0.5623877560787828\n",
      "Val loss: 0.5619033554762495\n",
      "Val loss: 0.5620289416590207\n",
      "Val loss: 0.5620788157398725\n",
      "Val loss: 0.5620909937384988\n",
      "Val loss: 0.5620025765138327\n",
      "Val loss: 0.5622053691431096\n",
      "Val loss: 0.562226467148\n",
      "Val loss: 0.5617542325691053\n",
      "Val loss: 0.561440338722217\n",
      "Val loss: 0.5613234670441828\n",
      "Val loss: 0.5614218823272045\n",
      "Val loss: 0.5613237789826478\n",
      "Val loss: 0.5608514664271588\n",
      "Val loss: 0.5616127282900866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5614437058696091\n",
      "Val loss: 0.5608530876326696\n",
      "Val loss: 0.5608575074287511\n",
      "Val loss: 0.560724217403721\n",
      "Val loss: 0.5609442485864893\n",
      "Val loss: 0.5609267050410337\n",
      "Val loss: 0.5607450973232692\n",
      "Val loss: 0.5609590615301082\n",
      "Val loss: 0.5607667659579328\n",
      "Val loss: 0.5603105242784858\n",
      "Val loss: 0.560513997287081\n",
      "Val loss: 0.5607975427762116\n",
      "Val loss: 0.5603391452348612\n",
      "Val loss: 0.5604583538960719\n",
      "Val loss: 0.5601413946305368\n",
      "Val loss: 0.5598589666890648\n",
      "Val loss: 0.5599639679724242\n",
      "Val loss: 0.5597963584458223\n",
      "Val loss: 0.5598473538706135\n",
      "Val loss: 0.5593677246355796\n",
      "Val loss: 0.5591486845754569\n",
      "Val loss: 0.5593217642165491\n",
      "Val loss: 0.5593727918621761\n",
      "Val loss: 0.5594536584384483\n",
      "Val loss: 0.5594092937293591\n",
      "Val loss: 0.5591894845293544\n",
      "Val loss: 0.5594750161832956\n",
      "Val loss: 0.5595747430093032\n",
      "Val loss: 0.559587315730522\n",
      "Val loss: 0.5596460751917681\n",
      "Val loss: 0.5600938313231917\n",
      "Val loss: 0.5599400990066075\n",
      "Val loss: 0.5597940819202566\n",
      "Val loss: 0.5601249316323128\n",
      "Val loss: 0.5599031751555514\n",
      "Val loss: 0.5597502415189306\n",
      "Val loss: 0.5596481972921548\n",
      "Val loss: 0.5593573878543654\n",
      "Val loss: 0.5593199932287708\n",
      "Val loss: 0.5591126965249286\n",
      "Val loss: 0.5592710402928198\n",
      "Val loss: 0.5593602518527516\n",
      "Val loss: 0.5593870716146152\n",
      "Val loss: 0.5593651170109181\n",
      "Val loss: 0.5594946274334182\n",
      "Val loss: 0.5593587059384855\n",
      "Val loss: 0.5594905027041988\n",
      "Val loss: 0.559592992475588\n",
      "Val loss: 0.5596165434411507\n",
      "Val loss: 0.5597217140675393\n",
      "Val loss: 0.5599610605005032\n",
      "Val loss: 0.560138380142632\n",
      "Val loss: 0.5603744632798463\n",
      "Val loss: 0.5603705199028848\n",
      "Val loss: 0.5603634009261124\n",
      "Val loss: 0.5604889240020361\n",
      "Val loss: 0.5606486059144874\n",
      "Val loss: 0.5605159192243209\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_validate = []\n",
    "\n",
    "for i in range(30):\n",
    "    print(f'\\nstarting Epoch {i}')\n",
    "    epoch_loss_on_train = train(model_1, train_iterator, optimizer, criterion)\n",
    "    losses_train.append(epoch_loss_on_train)\n",
    "\n",
    "    epoch_loss_on_validation = validate(model_1, test_iterator, criterion)\n",
    "    losses_validate.append(epoch_loss_on_validation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dec41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6c407903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses_train)\n",
    "plt.plot(losses_validate)\n",
    "plt.title('BCE loss value')\n",
    "plt.ylabel('BCE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c27cf3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m y_score \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     model_1\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_1.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1] # Get the probabilities for class 1\n",
    "        y_true += labels.cpu().numpy().tolist()\n",
    "        y_score += probs.cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "# Calculate the ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Calculate other quality metrics\n",
    "y_pred = np.array(y_score) >= 0.5\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "967cbb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the integer class labels\n",
    "class_labels = torch.LongTensor([0, 1, 2, 1, 0])\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = class_labels.max() + 1\n",
    "\n",
    "# One-hot encode the class labels\n",
    "one_hot = torch.nn.functional.one_hot(class_labels, num_classes=num_classes)\n",
    "\n",
    "# Print the one-hot encoded tensor\n",
    "print(one_hot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86379c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a special DataLoader for CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c6592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e311f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OneHotEncoder, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, inp):\n",
    "        one_hot = torch.nn.functional.one_hot(inp, num_classes=self.num_classes)\n",
    "        return one_hot.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4dfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_One_Hot(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "        self.one_hot_encoder = OneHotEncoder(vocab_size)\n",
    "        conv_layer_1 = \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d16910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        self.fc1 = nn.Linear(hidden_size, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (h_n, c_n) = self.lstm1(x)\n",
    "        x = self.fc1(h_n[-1])\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be709470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "text = list('KNEQFNCYGPINAYGFQRTGGEDW')\n",
    "encoder = OneHotEncoder()\n",
    "text_arr = np.array(text)\n",
    "text_arr = text_arr.reshape(-1, 1)\n",
    "encoder.fit(text_arr)\n",
    "onehot = encoder.transform(text_arr).toarray()\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37be9f49",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:160\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:160\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, string_classes):\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ace2_dataloader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      4\u001b[0m     j \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:163\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\u001b[38;5;241m*\u001b[39m(default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:163\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(elem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# namedtuple\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elem_type(\u001b[38;5;241m*\u001b[39m(default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n",
      "File \u001b[0;32m/home/paul-valery/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(batch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, string_classes):\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "j = 0 \n",
    "for i in ace2_dataloader:\n",
    "    print(i)\n",
    "    j += 1\n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c21e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "indices = torch.LongTensor([0, 1, 2, 3, 1, 1])\n",
    "num_classes = 4\n",
    "one_hot = torch.nn.functional.one_hot(indices, num_classes)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a042a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        self.fc1 = nn.Linear(hidden_size, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (h_n, c_n) = self.lstm1(x)\n",
    "        x = self.fc1(h_n[-1])\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def fit_RNN(X_train, y_train, RNN_name):\n",
    "    # define the model\n",
    "    input_size = X_train.shape[2]\n",
    "    hidden_size = 80\n",
    "    output_size = 1\n",
    "    model = RNN(input_size, hidden_size, output_size)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(50):\n",
    "        running_loss = 0.0\n",
    "        for i in range(len(X_train)):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = torch.tensor(X_train[i], dtype=torch.float32).unsqueeze(0)\n",
    "            labels = torch.tensor(y_train[i], dtype=torch.float32).unsqueeze(0)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(X_train)\n",
    "        print('Epoch [%d], train loss: %.4f' % (epoch+1, train_loss))\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), RNN_name+\"_RNN_All_3LSTM.pt\")\n",
    "    print(RNN_name+' RNN Model has been trained!')\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
