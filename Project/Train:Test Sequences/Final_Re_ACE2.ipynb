{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2008c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a59cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading data.\n",
    "\"\"\"\n",
    "\n",
    "ACE2_train = pd.read_csv(\"ACE2_train_data.csv\")\n",
    "ACE2_test = pd.read_csv(\"ACE2_test_data.csv\")\n",
    "LY16_test = pd.read_csv(\"LY16_test_data.csv\")\n",
    "LY16_train = pd.read_csv(\"LY16_train_data.csv\")\n",
    "LY555_test = pd.read_csv(\"LY555_test_data.csv\")\n",
    "LY555_train = pd.read_csv(\"LY555_train_data.csv\")\n",
    "REGN33_train = pd.read_csv(\"REGN33_train_data.csv\")\n",
    "REGN87_train = pd.read_csv(\"REGN87_train_data.csv\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function creates a vocabulary of letters in the dataset\n",
    "\"\"\"\n",
    "def vocabulary(series):\n",
    "    un_val = series.apply(lambda x: set(list(x))).tolist()\n",
    "    un_val = set().union(*un_val)\n",
    "    return un_val\n",
    "\n",
    "vocab = vocabulary(ACE2_train['junction_aa'])\n",
    "\n",
    "\"\"\"\n",
    "One-hot encoding. No padding is needed, because all strings have a length of 24\n",
    "\"\"\"\n",
    "def one_hot_encode(series, aa_vocab):\n",
    "    length = 24\n",
    "    encoding = lambda x: np.array([[letter == aa_vocab[i] for i in range(len(aa_vocab))] for letter in x])\n",
    "    \n",
    "    return series.apply(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d3b73fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.996422182468693"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(LY555_train)/len(LY555_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a445c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre-process test and train data\n",
    "\"\"\"\n",
    "ACE2_train['junction_aa_encoded'] = one_hot_encode(ACE2_train['junction_aa'], list(vocab))\n",
    "ACE2_test['junction_aa_encoded'] = one_hot_encode(ACE2_test['junction_aa'], list(vocab))\n",
    "\n",
    "X_train = torch.tensor(np.stack(ACE2_train['junction_aa_encoded'].values), dtype=torch.float32)\n",
    "y_train = torch.tensor(ACE2_train['Label'].values, dtype=torch.long)\n",
    "X_test = torch.tensor(np.stack(ACE2_test['junction_aa_encoded'].values), dtype=torch.float32)\n",
    "y_test = torch.tensor(ACE2_test['Label'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e882d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Datasets\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OneHotDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_data = self.X[idx]\n",
    "        X_data = torch.unsqueeze(X_data, 0)  # Add an extra dimension at position 0\n",
    "                                             # for each batch to have the shape [64, 1, 24, 20]\n",
    "        y_data = self.y[idx]\n",
    "        \n",
    "        return X_data, y_data\n",
    "\n",
    "    \n",
    "train_dataset = OneHotDataset(X_train, y_train)\n",
    "test_dataset = OneHotDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f121618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data loaders:\n",
    "\"\"\"\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1be8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN, 1x24x20 input. Kernels of size (3, 3), 2 layers, with BatchNorm, 6 and 9 channels. \n",
    "All the sizes are hardcoded for now.\n",
    "\"\"\"\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(3, 3)) # 22x18\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2)) # 11x9xx6\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=9, kernel_size=(3, 3)) # 9x7xx9\n",
    "        self.bn2 = nn.BatchNorm2d(9)\n",
    "        self.flatten = nn.Flatten() # 9*7*9\n",
    "        \n",
    "        self.fc1 = nn.Linear(9*7*9, 64) # HARDCODE!!!! <<<<<\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.bn4 = nn.BatchNorm1d(10)\n",
    "        self.out = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1316edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model setup\n",
    "\"\"\"\n",
    "model = ConvNet()\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acda062e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 21307), started 0:21:14 ago. (Use '!kill 21307' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-82ffa2721856eaab\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-82ffa2721856eaab\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tracking the training process (copypasted from MSU lectures on DL)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# этот код создает папку на диске с названием 'logs'\n",
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "    \n",
    "%load_ext tensorboard\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "%tensorboard --logdir=./logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70c97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Another minutely changed copypaste\n",
    "\"\"\"\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    num_correct = 0\n",
    "    num_elements = len(dataloader)\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        # так получаем текущий батч\n",
    "        X_batch, y_batch = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(X_batch.to(device))\n",
    "            \n",
    "            loss = criterion(logits.flatten(), y_batch.float().to(device))\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            y_pred = torch.argmax(logits, dim=1).cpu()\n",
    "            \n",
    "            num_correct += torch.sum(y_pred == y_batch)\n",
    "    \n",
    "    accuracy = num_correct / num_elements\n",
    "            \n",
    "    return accuracy, np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, n_epoch=3):\n",
    "\n",
    "    num_iter = 0\n",
    "    \n",
    "    # цикл обучения сети\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        print(\"Epoch:\", epoch)\n",
    "\n",
    "        model.train(True)\n",
    "        \n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            # так получаем текущий батч\n",
    "            X_batch, y_batch = batch \n",
    "            \n",
    "            # forward pass (получение ответов на батч картинок)\n",
    "            logits = model(X_batch.to(device)) \n",
    "            \n",
    "            # вычисление лосса от выданных сетью ответов и правильных ответов на батч\n",
    "            loss = loss_fn(logits.flatten(), y_batch.float().to(device)) \n",
    "            \n",
    "            \n",
    "            loss.backward() # backpropagation (вычисление градиентов)\n",
    "            optimizer.step() # обновление весов сети\n",
    "            optimizer.zero_grad() # обнуляем веса\n",
    "\n",
    "            #########################\n",
    "            # Логирование результатов\n",
    "            num_iter += 1\n",
    "            writer.add_scalar('Loss/train', loss.item(), num_iter)\n",
    "\n",
    "            # вычислим accuracy на текущем train батче\n",
    "            model_answers = torch.argmax(logits, dim=1).cpu()\n",
    "            train_accuracy = torch.sum(y_batch == model_answers) / len(y_batch)\n",
    "            writer.add_scalar('Accuracy/train', train_accuracy, num_iter)\n",
    "            #########################\n",
    "\n",
    "        # после каждой эпохи получаем метрику качества на валидационной выборке\n",
    "        model.train(False)\n",
    "\n",
    "        val_accuracy, val_loss = evaluate(model, test_dataloader, criterion=criterion)\n",
    "\n",
    "        writer.add_scalar('Loss/val', val_loss.item(), num_iter)\n",
    "        writer.add_scalar('Accuracy/val', val_accuracy, num_iter)\n",
    "        \n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "model = train(model, criterion, optimizer, n_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate quality metrics (copypaste continues)\n",
    "\"\"\"\n",
    "train_accuracy, _ = evaluate(model, train_dataloader, criterion)\n",
    "print('Train accuracy is', train_accuracy)\n",
    "\n",
    "test_accuracy, _ = evaluate(model, test_dataloader, criterion)\n",
    "print('Test accuracy is', test_accuracy)\n",
    "\n",
    "!tensorboard dev upload --logdir=./logs \\\n",
    "--name \"My latest experiment\" \\\n",
    "--description \"Simple comparison of several hyperparameters\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
